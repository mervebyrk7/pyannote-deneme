#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Basit Ses KaydÄ± ve PyAnnote Analizi

Bu program, mikrofon giriÅŸinden ses kaydÄ± alÄ±r, dosyaya kaydeder ve
PyAnnote.audio kullanarak konuÅŸmacÄ± diyarizasyonu yapar.
"""

import os
import time
import wave
import threading
import numpy as np
import sounddevice as sd
import torch
from pyannote.audio import Pipeline
from pyannote.audio import Model
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext, filedialog
from datetime import datetime
# macOS uyumluluÄŸu iÃ§in matplotlib backend ayarÄ±
import matplotlib
matplotlib.use('TkAgg')  # macOS'ta en uyumlu backend

import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.figure import Figure
import matplotlib.patches as patches

# macOS'ta font uyarÄ±larÄ±nÄ± bastÄ±r
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
import openai  # Whisper yerine OpenAI API kullanacaÄŸÄ±z
import base64  # Base64 kodlama iÃ§in
import librosa
import soundfile as sf
import noisereduce as nr
from scipy import signal
import seaborn as sns
from collections import defaultdict
import json
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings("ignore")

# HuggingFace eriÅŸim tokeni - kendi tokeninizle deÄŸiÅŸtirin
HF_TOKEN = "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"

# OpenAI API anahtarÄ± - kendi anahtarÄ±nÄ±zla deÄŸiÅŸtirin
OPENAI_API_KEY = "sk-proj-AZXoZV7vdTPzxP1GBiE7T3BlbkFJnUPhxjWfTcWBaod7INhj"  # Kendi API anahtarÄ±nÄ±zÄ± girin
openai.api_key = OPENAI_API_KEY

# Ses kaydÄ± parametreleri - Optimize edildi
SAMPLE_RATE = 44100  # Standart ses kalitesi
CHANNELS = 1         # Mono kayÄ±t
DTYPE = np.float32   # YÃ¼ksek hassasiyet
RECORDING_SECONDS = 30  # VarsayÄ±lan kayÄ±t sÃ¼resi

# Buffer parametreleri - Optimize edildi
BUFFER_SIZE = 1024      # KÃ¼Ã§Ã¼k ve gÃ¼venilir buffer boyutu  
BLOCK_DURATION = 0.1    # Her blok 100ms 
OVERLAP_SAMPLES = 0     # Overlap kullanmÄ±yoruz (sadece direkt kayÄ±t)

# GPT model parametresi
GPT_MODEL = "gpt-4o-mini"  # OpenAI'nin gpt-4o-mini modeli
GPT_LANGUAGE = "tr"  # TÃ¼rkÃ§e dil desteÄŸi

# Analiz modelleri
DIARIZATION_MODEL = "pyannote/speaker-diarization-3.1"
VAD_MODEL = "pyannote/voice-activity-detection"
OVERLAP_MODEL = "pyannote/overlapped-speech-detection"
SEGMENTATION_MODEL = "pyannote/segmentation"
SEPARATION_MODEL = "pyannote/speech-separation-ami-1.0"

# Renk paleti konuÅŸmacÄ±lar iÃ§in
SPEAKER_COLORS = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F']

# Duygu analizi iÃ§in renk paleti
EMOTION_COLORS = {
    'mutlu': '#2ECC71',
    'Ã¼zgÃ¼n': '#3498DB', 
    'kÄ±zgÄ±n': '#E74C3C',
    'sakin': '#95A5A6',
    'heyecanlÄ±': '#F39C12',
    'stresli': '#E67E22'
}

# Cinsiyet ve yaÅŸ tahmini modelleri
GENDER_AGE_MODELS = {
    'wav2vec2_gender': "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim",
    'speechbrain_gender': "speechbrain/spkrec-ecapa-voxceleb",
    'hubert_age_gender': "facebook/hubert-large-ls960-ft",
    'whisper_feature_extractor': "openai/whisper-base"
}

# Cinsiyet renkleri
GENDER_COLORS = {
    'erkek': '#3498DB',      # Mavi
    'kadÄ±n': '#E91E63',      # Pembe
    'belirsiz': '#95A5A6'    # Gri
}

# YaÅŸ grubu renkleri
AGE_COLORS = {
    'Ã§ocuk': '#FF9800',      # Turuncu (0-12)
    'genÃ§': '#4CAF50',       # YeÅŸil (13-25)
    'yetiÅŸkin': '#2196F3',   # Mavi (26-45)
    'orta_yaÅŸ': '#9C27B0',   # Mor (46-65)
    'yaÅŸlÄ±': '#795548'       # Kahverengi (65+)
}

class SesKayitAnaliz:
    def __init__(self, root):
        self.root = root
        self.root.title("GeliÅŸmiÅŸ Ses Analizi ve PyAnnote Sistemi")
        
        # macOS uyumluluÄŸu iÃ§in pencere ayarlarÄ±
        self.root.geometry("1200x800")
        
        # macOS'ta state('zoomed') Ã§alÄ±ÅŸmaz, alternatif yaklaÅŸÄ±m
        try:
            # Windows/Linux iÃ§in
            if self.root.tk.call('tk', 'windowingsystem') == 'win32':
                self.root.state('zoomed')
            # macOS iÃ§in
            elif self.root.tk.call('tk', 'windowingsystem') == 'aqua':
                self.root.attributes('-zoomed', True)
            # Linux iÃ§in
            else:
                self.root.attributes('-zoomed', True)
        except:
            # Hata durumunda normal boyut kullan
            self.root.geometry("1200x800")
        
        # Pencereyi ekranÄ±n ortasÄ±na yerleÅŸtir
        self.root.update_idletasks()
        width = self.root.winfo_width()
        height = self.root.winfo_height()
        x = (self.root.winfo_screenwidth() // 2) - (width // 2)
        y = (self.root.winfo_screenheight() // 2) - (height // 2)
        self.root.geometry(f'{width}x{height}+{x}+{y}')
        
        # Minimum pencere boyutu
        self.root.minsize(800, 600)
        
        # Pencereyi gÃ¶rÃ¼nÃ¼r yap
        self.root.deiconify()
        self.root.lift()
        self.root.focus_force()
        
        # Ana Ã§erÃ§eve
        main_frame = ttk.Frame(root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # BaÅŸlÄ±k
        header = ttk.Label(main_frame, text="ğŸ¤ GeliÅŸmiÅŸ Ses Analizi ve KonuÅŸmacÄ± Diyarizasyonu ğŸ¤", 
                          font=("Helvetica", 18, "bold"))
        header.pack(pady=10)
        
        # Kontrol Ã§erÃ§evesi - Ãœst kÄ±sÄ±m
        control_frame_top = ttk.Frame(main_frame)
        control_frame_top.pack(fill=tk.X, pady=5)
        
        # KayÄ±t kontrolleri
        record_frame = ttk.LabelFrame(control_frame_top, text="ğŸ™ï¸ KayÄ±t Kontrolleri")
        record_frame.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        
        # KayÄ±t sÃ¼resi giriÅŸi
        ttk.Label(record_frame, text="KayÄ±t SÃ¼resi (saniye):").pack(side=tk.LEFT, padx=5)
        self.duration_var = tk.StringVar(value=str(RECORDING_SECONDS))
        duration_entry = ttk.Entry(record_frame, textvariable=self.duration_var, width=5)
        duration_entry.pack(side=tk.LEFT, padx=5)
        
        # KayÄ±t dosya adÄ± giriÅŸi
        ttk.Label(record_frame, text="Dosya AdÄ±:").pack(side=tk.LEFT, padx=5)
        self.filename_var = tk.StringVar(value=f"kayit_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav")
        filename_entry = ttk.Entry(record_frame, textvariable=self.filename_var, width=20)
        filename_entry.pack(side=tk.LEFT, padx=5)
        
        # BaÅŸlat/Durdur dÃ¼ÄŸmesi
        self.record_button = ttk.Button(record_frame, text="ğŸ”´ KaydÄ± BaÅŸlat", 
                                       command=self.toggle_recording)
        self.record_button.pack(side=tk.LEFT, padx=10)
        
        # Dosya seÃ§ dÃ¼ÄŸmesi
        self.load_button = ttk.Button(record_frame, text="ğŸ“ Dosya SeÃ§", 
                                     command=self.load_audio_file)
        self.load_button.pack(side=tk.LEFT, padx=5)
        
        # Analiz seÃ§enekleri Ã§erÃ§evesi
        analysis_frame = ttk.LabelFrame(control_frame_top, text="ğŸ”¬ Analiz SeÃ§enekleri")
        analysis_frame.pack(side=tk.RIGHT, padx=5)
        
        # Analiz seÃ§enekleri
        self.enable_vad = tk.BooleanVar(value=True)
        self.enable_overlap = tk.BooleanVar(value=True)
        self.enable_emotion = tk.BooleanVar(value=True)
        self.enable_noise_reduction = tk.BooleanVar(value=True)
        self.enable_separation = tk.BooleanVar(value=False)
        self.enable_live_analysis = tk.BooleanVar(value=False)
        self.enable_gender_age = tk.BooleanVar(value=True)  # Yeni: Cinsiyet ve yaÅŸ analizi
        
        ttk.Checkbutton(analysis_frame, text="Ses Aktivitesi Tespiti", variable=self.enable_vad).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Ã–rtÃ¼ÅŸen KonuÅŸma", variable=self.enable_overlap).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Duygu Analizi", variable=self.enable_emotion).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Cinsiyet ve YaÅŸ Analizi", variable=self.enable_gender_age).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="GÃ¼rÃ¼ltÃ¼ Azaltma", variable=self.enable_noise_reduction).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Ses AyrÄ±ÅŸtÄ±rma", variable=self.enable_separation).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="CanlÄ± Analiz", variable=self.enable_live_analysis).pack(anchor=tk.W)
        
        # Kontrol Ã§erÃ§evesi - Alt kÄ±sÄ±m
        control_frame_bottom = ttk.Frame(main_frame)
        control_frame_bottom.pack(fill=tk.X, pady=5)
        
        # Ana analiz dÃ¼ÄŸmeleri
        button_frame = ttk.Frame(control_frame_bottom)
        button_frame.pack(side=tk.LEFT)
        
        # Analiz dÃ¼ÄŸmesi
        self.analyze_button = ttk.Button(button_frame, text="ğŸ” Tam Analiz Yap", 
                                        command=self.analyze_recording,
                                        state=tk.DISABLED)
        self.analyze_button.pack(side=tk.LEFT, padx=5)
        
        # HÄ±zlÄ± analiz dÃ¼ÄŸmesi
        self.quick_analyze_button = ttk.Button(button_frame, text="âš¡ HÄ±zlÄ± Analiz", 
                                              command=self.quick_analyze,
                                              state=tk.DISABLED)
        self.quick_analyze_button.pack(side=tk.LEFT, padx=5)
        
        # Rapor oluÅŸtur dÃ¼ÄŸmesi
        self.report_button = ttk.Button(button_frame, text="ğŸ“„ Rapor OluÅŸtur", 
                                       command=self.generate_report,
                                       state=tk.DISABLED)
        self.report_button.pack(side=tk.LEFT, padx=5)
        
        # Durum etiketi
        self.status_label = ttk.Label(control_frame_bottom, text="ğŸŸ¢ HazÄ±r", 
                                     font=("Helvetica", 12, "bold"))
        self.status_label.pack(side=tk.RIGHT, padx=20)
        
        # Ä°Ã§erik Ã§erÃ§evesi
        content_frame = ttk.Frame(main_frame)
        content_frame.pack(fill=tk.BOTH, expand=True, pady=10)
        
        # Sol panel (gÃ¶rselleÅŸtirme)
        left_panel = ttk.Frame(content_frame)
        left_panel.pack(fill=tk.BOTH, expand=True, side=tk.LEFT, padx=(0, 5))
        
        # Notebook widget for multiple visualizations
        self.viz_notebook = ttk.Notebook(left_panel)
        self.viz_notebook.pack(fill=tk.BOTH, expand=True)
        
        # Ses gÃ¶rselleÅŸtirme sekmesi
        viz_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(viz_frame, text="ğŸŒŠ Dalga Formu")
        
        self.fig = Figure(figsize=(8, 6), dpi=100)
        
        # Alt grafik alanlarÄ± oluÅŸtur
        self.ax_waveform = self.fig.add_subplot(3, 1, 1)
        self.ax_spectrogram = self.fig.add_subplot(3, 1, 2)
        self.ax_diarization = self.fig.add_subplot(3, 1, 3)
        
        self.canvas = FigureCanvasTkAgg(self.fig, master=viz_frame)
        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Spektogram sekmesi
        spectrogram_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(spectrogram_frame, text="ğŸ“Š Spektogram")
        
        self.fig2 = Figure(figsize=(8, 6), dpi=100)
        self.ax_spec_detail = self.fig2.add_subplot(111)
        self.canvas2 = FigureCanvasTkAgg(self.fig2, master=spectrogram_frame)
        self.canvas2.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Duygu analizi sekmesi
        emotion_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(emotion_frame, text="ğŸ˜Š Duygu Analizi")
        
        self.fig3 = Figure(figsize=(8, 6), dpi=100)
        self.ax_emotion = self.fig3.add_subplot(111)
        self.canvas3 = FigureCanvasTkAgg(self.fig3, master=emotion_frame)
        self.canvas3.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Ä°statistik sekmesi
        stats_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(stats_frame, text="ğŸ“ˆ Ä°statistikler")
        
        self.fig4 = Figure(figsize=(8, 6), dpi=100)
        self.ax_stats1 = self.fig4.add_subplot(2, 2, 1)
        self.ax_stats2 = self.fig4.add_subplot(2, 2, 2)
        self.ax_stats3 = self.fig4.add_subplot(2, 2, 3)
        self.ax_stats4 = self.fig4.add_subplot(2, 2, 4)
        self.canvas4 = FigureCanvasTkAgg(self.fig4, master=stats_frame)
        self.canvas4.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Cinsiyet ve yaÅŸ sekmesi
        gender_age_viz_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(gender_age_viz_frame, text="ğŸ‘¥ Cinsiyet & YaÅŸ")
        
        self.fig5 = Figure(figsize=(8, 6), dpi=100)
        self.ax_gender = self.fig5.add_subplot(2, 2, 1)
        self.ax_age = self.fig5.add_subplot(2, 2, 2)
        self.ax_speaker_gender = self.fig5.add_subplot(2, 2, 3)
        self.ax_speaker_age = self.fig5.add_subplot(2, 2, 4)
        self.canvas5 = FigureCanvasTkAgg(self.fig5, master=gender_age_viz_frame)
        self.canvas5.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # BoÅŸ grafikleri gÃ¶ster (thread-safe)
        self.root.after(500, self.update_empty_plots)
        
        # SaÄŸ panel (log ve sonuÃ§lar)
        right_panel = ttk.Frame(content_frame)
        right_panel.pack(fill=tk.BOTH, expand=True, side=tk.RIGHT, padx=(5, 0))
        
        # SaÄŸ panel iÃ§in notebook
        self.right_notebook = ttk.Notebook(right_panel)
        self.right_notebook.pack(fill=tk.BOTH, expand=True)
        
        # Log sekmesi
        log_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(log_frame, text="ğŸ“‹ Ä°ÅŸlem GÃ¼nlÃ¼ÄŸÃ¼")
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=25, 
                                                 font=("Consolas", 9))
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # SonuÃ§ sekmesi
        result_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(result_frame, text="ğŸ¯ Diyarizasyon SonuÃ§larÄ±")
        
        self.result_text = scrolledtext.ScrolledText(result_frame, height=25, 
                                                    font=("Consolas", 9))
        self.result_text.pack(fill=tk.BOTH, expand=True)
        
        # Transkript sekmesi
        transcript_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(transcript_frame, text="ğŸ’¬ KonuÅŸma Ä°Ã§eriÄŸi")
        
        self.transcript_text = scrolledtext.ScrolledText(transcript_frame, height=25, 
                                                       font=("Consolas", 9))
        self.transcript_text.pack(fill=tk.BOTH, expand=True)
        
        # DetaylÄ± analiz sekmesi
        analysis_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(analysis_frame, text="ğŸ”¬ DetaylÄ± Analiz")
        
        self.analysis_text = scrolledtext.ScrolledText(analysis_frame, height=25, 
                                                      font=("Consolas", 9))
        self.analysis_text.pack(fill=tk.BOTH, expand=True)
        
        # Duygu analizi sekmesi
        emotion_analysis_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(emotion_analysis_frame, text="ğŸ˜Š Duygu Raporu")
        
        self.emotion_text = scrolledtext.ScrolledText(emotion_analysis_frame, height=25, 
                                                     font=("Consolas", 9))
        self.emotion_text.pack(fill=tk.BOTH, expand=True)
        
        # Cinsiyet ve yaÅŸ analizi sekmesi
        gender_age_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(gender_age_frame, text="ğŸ‘¥ Cinsiyet & YaÅŸ")
        
        self.gender_age_text = scrolledtext.ScrolledText(gender_age_frame, height=25, 
                                                        font=("Consolas", 9))
        self.gender_age_text.pack(fill=tk.BOTH, expand=True)
        
        # Durum deÄŸiÅŸkenleri
        self.is_recording = False
        self.recorded_audio = []
        self.recording_thread = None
        self.current_audio_file = None
        self.sample_rate = SAMPLE_RATE
        
        # Analiz sonuÃ§larÄ±
        self.diarization_result = None
        self.vad_result = None
        self.overlap_result = None
        self.emotion_result = None
        self.gender_age_result = {}  # Yeni: Cinsiyet ve yaÅŸ sonuÃ§larÄ±
        self.speaker_embeddings = {}
        self.noise_reduced_audio = None
        
        # Pipeline'lar
        self.pipelines = {}
        self.models_loaded = False
        
        # GerÃ§ek zamanlÄ± analiz iÃ§in
        self.live_analysis_thread = None
        self.live_analysis_running = False
        
        # HoÅŸgeldin mesajÄ±
        self.add_log("ğŸš€ GeliÅŸmiÅŸ Ses Analizi Sistemi baÅŸlatÄ±ldÄ±!")
        self.add_log("ğŸ“ KullanÄ±m: Ses kaydÄ± yapÄ±n veya dosya yÃ¼kleyin, sonra analiz seÃ§eneklerini belirleyip analiz baÅŸlatÄ±n.")
        self.add_log("âš¡ Ä°pucu: HÄ±zlÄ± analiz iÃ§in sadece temel Ã¶zellikleri seÃ§in, tam analiz iÃ§in tÃ¼m seÃ§enekleri aktifleÅŸtirin.")
        self.add_log("ğŸ”§ Modeller ilk kullanÄ±mda otomatik olarak yÃ¼klenecektir.")
        
        # Pencereyi zorla gÃ¼ncelle ve gÃ¶rÃ¼nÃ¼r yap
        self.root.update_idletasks()
        self.root.update()
        
        # macOS uyumluluÄŸu iÃ§in son ayarlar
        self.root.after(100, self._final_setup)
    
    def _final_setup(self):
        """Son kurulum iÅŸlemleri - macOS uyumluluÄŸu"""
        try:
            self.root.lift()
            self.root.attributes('-topmost', True)
            self.root.after(100, lambda: self.root.attributes('-topmost', False))
            self.root.focus_force()
            self.add_log("âœ… ArayÃ¼z hazÄ±r!")
        except Exception as e:
            self.add_log(f"âš ï¸ ArayÃ¼z kurulum uyarÄ±sÄ±: {e}")
    
    def update_empty_plots(self):
        """BoÅŸ grafikleri gÃ¶ster"""
        # Ana dalga formu
        self.ax_waveform.clear()
        self.ax_waveform.text(0.5, 0.5, "ğŸ¤ Ses kaydÄ± bekleniyor...", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=12, color='gray', transform=self.ax_waveform.transAxes)
        self.ax_waveform.set_title("Ses Dalga Formu")
        self.ax_waveform.grid(True, alpha=0.3)
        
        # Spektogram
        self.ax_spectrogram.clear()
        self.ax_spectrogram.text(0.5, 0.5, "ğŸ“Š Spektogram gÃ¶rÃ¼nÃ¼mÃ¼", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=10, color='gray', transform=self.ax_spectrogram.transAxes)
        self.ax_spectrogram.set_title("Spektogram")
        
        # Diyarizasyon
        self.ax_diarization.clear()
        self.ax_diarization.text(0.5, 0.5, "ğŸ‘¥ KonuÅŸmacÄ± zaman Ã§izelgesi", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=10, color='gray', transform=self.ax_diarization.transAxes)
        self.ax_diarization.set_title("KonuÅŸmacÄ± Diyarizasyonu")
        self.ax_diarization.set_xlabel("Zaman (saniye)")
        
        self.fig.tight_layout()
        self.canvas.draw()
        
        # DetaylÄ± spektogram
        self.ax_spec_detail.clear()
        self.ax_spec_detail.text(0.5, 0.5, "ğŸ“ˆ DetaylÄ± frekans analizi iÃ§in ses yÃ¼kleyin", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=12, color='gray', transform=self.ax_spec_detail.transAxes)
        self.canvas2.draw()
        
        # Duygu analizi
        self.ax_emotion.clear()
        self.ax_emotion.text(0.5, 0.5, "ğŸ˜Š Duygu analizi sonuÃ§larÄ± burada gÃ¶rÃ¼necek", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=12, color='gray', transform=self.ax_emotion.transAxes)
        self.canvas3.draw()
        
        # Ä°statistikler
        for ax in [self.ax_stats1, self.ax_stats2, self.ax_stats3, self.ax_stats4]:
            ax.clear()
            ax.text(0.5, 0.5, "ğŸ“Š", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=20, color='lightgray', transform=ax.transAxes)
        self.canvas4.draw()
        
        # Cinsiyet ve yaÅŸ grafikleri
        for ax in [self.ax_gender, self.ax_age, self.ax_speaker_gender, self.ax_speaker_age]:
            ax.clear()
            ax.text(0.5, 0.5, "ğŸ‘¥", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=20, color='lightgray', transform=ax.transAxes)
        self.canvas5.draw()
    
    def load_audio_file(self):
        """Ses dosyasÄ± yÃ¼kle"""
        file_path = filedialog.askopenfilename(
            title="Ses DosyasÄ± SeÃ§in",
            filetypes=[
                ("Ses DosyalarÄ±", "*.wav *.mp3 *.flac *.m4a *.ogg"),
                ("WAV DosyalarÄ±", "*.wav"),
                ("MP3 DosyalarÄ±", "*.mp3"),
                ("FLAC DosyalarÄ±", "*.flac"),
                ("TÃ¼m Dosyalar", "*.*")
            ]
        )
        
        if file_path:
            try:
                self.add_log(f"ğŸ“ Dosya yÃ¼kleniyor: {os.path.basename(file_path)}")
                
                # Ses dosyasÄ±nÄ± yÃ¼kle
                audio_data, sample_rate = librosa.load(file_path, sr=None)
                self.sample_rate = sample_rate
                self.current_audio_file = file_path
                self.recorded_audio = audio_data
                
                # Dalga formunu gÃ¼ncelle
                self.update_comprehensive_plots(audio_data)
                
                # Analiz dÃ¼ÄŸmelerini etkinleÅŸtir
                self.analyze_button.config(state=tk.NORMAL)
                self.quick_analyze_button.config(state=tk.NORMAL)
                
                self.add_log(f"âœ… Dosya baÅŸarÄ±yla yÃ¼klendi! SÃ¼re: {len(audio_data)/sample_rate:.2f} saniye")
                self.status_label.config(text="ğŸŸ¢ Dosya yÃ¼klendi - Analiz iÃ§in hazÄ±r")
                
            except Exception as e:
                self.add_log(f"âŒ Dosya yÃ¼kleme hatasÄ±: {e}")
                messagebox.showerror("Hata", f"Dosya yÃ¼klenirken hata oluÅŸtu:\n{e}")
    
    def update_comprehensive_plots(self, audio_data):
        """KapsamlÄ± gÃ¶rselleÅŸtirme gÃ¼ncelle"""
        time_axis = np.arange(len(audio_data)) / self.sample_rate
        
        # Ana dalga formu
        self.ax_waveform.clear()
        self.ax_waveform.plot(time_axis, audio_data, color='#2E86AB', linewidth=0.8)
        self.ax_waveform.set_xlim(0, max(time_axis))
        self.ax_waveform.set_ylim(-1, 1)
        self.ax_waveform.set_ylabel("Genlik")
        self.ax_waveform.set_title("ğŸŒŠ Ses Dalga Formu")
        self.ax_waveform.grid(True, alpha=0.3)
        
        # Spektogram
        self.ax_spectrogram.clear()
        f, t, Sxx = signal.spectrogram(audio_data, self.sample_rate, nperseg=1024)
        self.ax_spectrogram.pcolormesh(t, f[:len(f)//4], 10 * np.log10(Sxx[:len(f)//4]), 
                                      shading='gouraud', cmap='viridis')
        self.ax_spectrogram.set_ylabel('Frekans (Hz)')
        self.ax_spectrogram.set_title("ğŸ“Š Spektogram")
        
        # Diyarizasyon placeholder
        self.ax_diarization.clear()
        self.ax_diarization.text(0.5, 0.5, "ğŸ‘¥ Diyarizasyon iÃ§in analiz yapÄ±n", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=10, color='gray', transform=self.ax_diarization.transAxes)
        self.ax_diarization.set_xlabel("Zaman (saniye)")
        self.ax_diarization.set_title("KonuÅŸmacÄ± Diyarizasyonu")
        
        self.fig.tight_layout()
        self.canvas.draw()
        
        # DetaylÄ± spektogram
        self.ax_spec_detail.clear()
        f_detail, t_detail, Sxx_detail = signal.spectrogram(audio_data, self.sample_rate, nperseg=2048)
        im = self.ax_spec_detail.pcolormesh(t_detail, f_detail, 10 * np.log10(Sxx_detail), 
                                           shading='gouraud', cmap='plasma')
        self.ax_spec_detail.set_ylabel('Frekans (Hz)')
        self.ax_spec_detail.set_xlabel('Zaman (saniye)')
        self.ax_spec_detail.set_title('ğŸµ DetaylÄ± Spektogram')
        plt.colorbar(im, ax=self.ax_spec_detail, label='GÃ¼Ã§ (dB)')
        self.canvas2.draw()
        
    def update_waveform_plot(self, audio_data):
        """BasitÃ§e dalga formunu gÃ¼ncelle"""
        self.update_comprehensive_plots(audio_data)
    
    def toggle_recording(self):
        """KaydÄ± baÅŸlat/durdur"""
        if not self.is_recording:
            # KayÄ±t sÃ¼resini al
            try:
                recording_seconds = int(self.duration_var.get())
                if recording_seconds <= 0:
                    messagebox.showerror("Hata", "KayÄ±t sÃ¼resi pozitif bir sayÄ± olmalÄ±dÄ±r!")
                    return
            except ValueError:
                messagebox.showerror("Hata", "GeÃ§erli bir kayÄ±t sÃ¼resi giriniz!")
                return
            
            # KaydÄ± baÅŸlat
            self.record_button.config(text="â¹ï¸ KaydÄ± Durdur")
            self.status_label.config(text="ğŸ”´ KayÄ±t yapÄ±lÄ±yor...")
            self.analyze_button.config(state=tk.DISABLED)
            self.quick_analyze_button.config(state=tk.DISABLED)
            self.recorded_audio = []
            self.is_recording = True
            
            # CanlÄ± analiz baÅŸlat
            if self.enable_live_analysis.get():
                self.start_live_analysis()
            
            # Ä°ÅŸ parÃ§acÄ±ÄŸÄ±nÄ± baÅŸlat
            self.recording_thread = threading.Thread(target=self.record_audio, 
                                                   args=(recording_seconds,))
            self.recording_thread.daemon = True
            self.recording_thread.start()
            
            self.add_log(f"ğŸ™ï¸ {recording_seconds} saniyelik kayÄ±t baÅŸlatÄ±ldÄ±.")
        else:
            # KaydÄ± durdur
            self.is_recording = False
            self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat")
            self.status_label.config(text="â¹ï¸ KayÄ±t durduruldu")
            self.add_log("â¹ï¸ KayÄ±t manuel olarak durduruldu.")
            
            # CanlÄ± analizi durdur
            if self.live_analysis_running:
                self.stop_live_analysis()
    
    def record_audio(self, duration):
        """Basit ve gÃ¼venilir ses kaydÄ±"""
        try:
            self.add_log("ğŸ™ï¸ Basit kayÄ±t sistemi baÅŸlatÄ±lÄ±yor...")
            
            # Ses cihazÄ±nÄ± optimize et
            self.optimize_audio_device()
            
            # Toplam sample sayÄ±sÄ±nÄ± hesapla
            total_samples = int(SAMPLE_RATE * duration)
            self.add_log(f"ğŸ“Š Hedef: {duration}s = {total_samples} sample")
            
            # KayÄ±t iÃ§in buffer
            self.recorded_audio = np.zeros(total_samples, dtype=DTYPE)
            current_frame = 0
            
            # Buffer boyutu - daha kÃ¼Ã§Ã¼k ve gÃ¼venilir
            block_size = 1024  # Sabit 1024 sample bloklar
            
            def audio_callback(indata, frames, time, status):
                nonlocal current_frame
                
                if status:
                    self.add_log(f"âš ï¸ Audio status: {status}")
                
                if self.is_recording and current_frame < total_samples:
                    # KaÃ§ sample alacaÄŸÄ±mÄ±zÄ± hesapla  
                    samples_to_take = min(frames, total_samples - current_frame)
                    
                    # Veriyi direkt kayÄ±t buffer'Ä±na kopyala
                    self.recorded_audio[current_frame:current_frame + samples_to_take] = indata[:samples_to_take, 0]
                    current_frame += samples_to_take
                    
                    # Progress gÃ¼ncelle (her 0.5 saniyede bir)
                    if current_frame % (SAMPLE_RATE // 2) < frames:
                        progress = (current_frame / total_samples) * 100
                        elapsed = current_frame / SAMPLE_RATE
                        self.root.after(1, lambda p=progress, e=elapsed: self.update_recording_progress(p, e, duration))
                    
                    # KayÄ±t tamamlandÄ± mÄ±?
                    if current_frame >= total_samples:
                        self.is_recording = False
            
            self.add_log(f"ğŸ¤ KayÄ±t baÅŸlÄ±yor: {block_size} sample bloklar")
            self.add_log(f"ğŸ”§ Sample rate: {SAMPLE_RATE} Hz")
            self.add_log(f"ğŸ“Š Dtype: {DTYPE}")
            self.add_log(f"ğŸ§ Channels: {CHANNELS}")
            
            # Debug - ses cihazÄ± bilgileri
            try:
                current_device = sd.query_devices(sd.default.device[0])
                self.add_log(f"ğŸ¤ Aktif cihaz: {current_device['name']}")
                self.add_log(f"âš¡ Desteklenen sample rate: {current_device['default_samplerate']}")
            except:
                pass
            
            # Stream baÅŸlat
            with sd.InputStream(
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=DTYPE,
                blocksize=block_size,
                callback=audio_callback,
                latency='low'
            ):
                start_time = time.time()
                
                # KayÄ±t dÃ¶ngÃ¼sÃ¼ - Ã§ok basit
                while self.is_recording and current_frame < total_samples:
                    time.sleep(0.1)  # 100ms bekle
                    
                    # Timeout kontrolÃ¼
                    if time.time() - start_time > duration + 2:  # 2 saniye extra
                        self.add_log("â° KayÄ±t timeout - zorla durduruluyor")
                        break
            
            # KayÄ±t iÅŸlemi bitti
            self.is_recording = False
            self.root.after(1, lambda: self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat"))
            
            # SonuÃ§larÄ± kontrol et
            actual_duration = current_frame / SAMPLE_RATE
            self.add_log(f"âœ… KayÄ±t tamamlandÄ±!")
            self.add_log(f"ğŸ“Š Hedef: {duration}s, GerÃ§ek: {actual_duration:.2f}s")
            self.add_log(f"ğŸ”¢ Sample sayÄ±sÄ±: {current_frame}/{total_samples}")
            
            # Kaydedilen veriyi kes (gereksiz sÄ±fÄ±rlarÄ± temizle)
            if current_frame > 0:
                self.recorded_audio = self.recorded_audio[:current_frame]
                self.sample_rate = SAMPLE_RATE
                
                # Ses seviyesi analizi
                max_amplitude = np.max(np.abs(self.recorded_audio))
                avg_amplitude = np.mean(np.abs(self.recorded_audio))
                
                self.add_log(f"ğŸ”Š Max seviye: {max_amplitude:.4f}")
                self.add_log(f"ğŸ“ˆ Ortalama seviye: {avg_amplitude:.4f}")
                
                # Normalize et (sadece gerekirse)
                if max_amplitude > 0.95:
                    self.recorded_audio = self.recorded_audio / max_amplitude * 0.9
                    self.add_log("ğŸ”§ Ses seviyesi normalize edildi")
                
                # Grafikleri gÃ¼ncelle
                self.root.after(1, lambda: self.update_comprehensive_plots(self.recorded_audio))
                self.root.after(1, lambda: self.status_label.config(text="âœ… KayÄ±t tamamlandÄ±"))
                
                # Dosyaya kaydet
                self.save_recording()
                
                # DÃ¼ÄŸmeleri etkinleÅŸtir
                self.root.after(1, lambda: self.analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.quick_analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.report_button.config(state=tk.NORMAL))
            else:
                self.add_log("âŒ HiÃ§ ses verisi alÄ±namadÄ±!")
                self.root.after(1, lambda: self.status_label.config(text="âŒ KayÄ±t baÅŸarÄ±sÄ±z"))
                
        except Exception as e:
            self.add_log(f"âŒ Callback kayÄ±t hatasÄ±: {e}")
            self.add_log("ğŸ”„ Fallback basit kayÄ±t yÃ¶ntemi deneniyor...")
            
            # Fallback - basit kayÄ±t yÃ¶ntemi
            self.is_recording = True  # Reset iÃ§in
            success = self.record_audio_simple(duration)
            
            if not success:
                self.is_recording = False
                self.root.after(1, lambda: self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat"))
                self.root.after(1, lambda: self.status_label.config(text="âŒ TÃ¼m kayÄ±t yÃ¶ntemleri baÅŸarÄ±sÄ±z!"))
        finally:
            # CanlÄ± analizi durdur
            if self.live_analysis_running:
                self.stop_live_analysis()
    
    def save_recording(self):
        """KaydÄ± yÃ¼ksek kaliteli WAV dosyasÄ±na kaydet"""
        try:
            filename = self.filename_var.get()
            if not filename.endswith(".wav"):
                filename += ".wav"
            
            # Soundfile kullanarak yÃ¼ksek kaliteli kayÄ±t
            # (wave modÃ¼lÃ¼nden daha iyi kalite)
            sf.write(filename, self.recorded_audio, SAMPLE_RATE, 
                    subtype='PCM_24')  # 24-bit yÃ¼ksek kalite
            
            self.current_audio_file = filename
            
            # Dosya bilgilerini gÃ¶ster
            file_size = os.path.getsize(filename) / (1024 * 1024)  # MB
            duration = len(self.recorded_audio) / SAMPLE_RATE
            
            self.add_log(f"ğŸ’¾ KayÄ±t '{filename}' dosyasÄ±na kaydedildi.")
            self.add_log(f"ğŸ“Š Dosya boyutu: {file_size:.2f} MB, SÃ¼re: {duration:.2f}s")
            self.add_log(f"ğŸµ Format: 24-bit PCM, {SAMPLE_RATE} Hz, {CHANNELS} kanal")
            
        except Exception as e:
            self.add_log(f"âŒ Dosya kaydetme hatasÄ±: {e}")
            # Fallback - basic wave format
            try:
                filename = self.filename_var.get()
                if not filename.endswith(".wav"):
                    filename += ".wav"
                    
                int_data = np.int16(self.recorded_audio * 32767)
                with wave.open(filename, 'wb') as wf:
                    wf.setnchannels(CHANNELS)
                    wf.setsampwidth(2)
                    wf.setframerate(SAMPLE_RATE)
                    wf.writeframes(int_data.tobytes())
                
                self.current_audio_file = filename
                self.add_log(f"ğŸ’¾ Fallback kayÄ±t baÅŸarÄ±lÄ±: {filename}")
            except Exception as e2:
                self.add_log(f"âŒ Fallback kayÄ±t da baÅŸarÄ±sÄ±z: {e2}")
    
    def load_models(self):
        """Analiz modellerini yÃ¼kle"""
        if self.models_loaded:
            return True
            
        try:
            self.add_log("ğŸ”„ Modeller yÃ¼kleniyor...")
            self.status_label.config(text="ğŸ”„ Modeller yÃ¼kleniyor...")
            
            # GPU kullanÄ±mÄ±nÄ± kontrol et
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.add_log(f"ğŸ’» Cihaz: {device}")
            
            # Diyarizasyon pipeline'Ä±
            if 'diarization' not in self.pipelines:
                self.pipelines['diarization'] = Pipeline.from_pretrained(
                    DIARIZATION_MODEL, use_auth_token=HF_TOKEN
                )
                self.pipelines['diarization'].to(device)
                self.add_log("âœ… Diyarizasyon modeli yÃ¼klendi")
            
            # VAD pipeline'Ä±
            if self.enable_vad.get() and 'vad' not in self.pipelines:
                try:
                    self.pipelines['vad'] = Pipeline.from_pretrained(
                        VAD_MODEL, use_auth_token=HF_TOKEN
                    )
                    self.pipelines['vad'].to(device)
                    self.add_log("âœ… Ses Aktivitesi Tespiti modeli yÃ¼klendi")
                except Exception as e:
                    self.add_log(f"âš ï¸ VAD modeli yÃ¼klenemedi: {e}")
            
            # Ã–rtÃ¼ÅŸen konuÅŸma tespiti pipeline'Ä±
            if self.enable_overlap.get() and 'overlap' not in self.pipelines:
                try:
                    self.pipelines['overlap'] = Pipeline.from_pretrained(
                        OVERLAP_MODEL, use_auth_token=HF_TOKEN
                    )
                    self.pipelines['overlap'].to(device)
                    self.add_log("âœ… Ã–rtÃ¼ÅŸen KonuÅŸma Tespiti modeli yÃ¼klendi")
                except Exception as e:
                    self.add_log(f"âš ï¸ Ã–rtÃ¼ÅŸme modeli yÃ¼klenemedi: {e}")
            
            self.models_loaded = True
            self.add_log("ğŸ‰ TÃ¼m modeller baÅŸarÄ±yla yÃ¼klendi!")
            return True
            
        except Exception as e:
            self.add_log(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            messagebox.showerror("Model HatasÄ±", f"Modeller yÃ¼klenirken hata oluÅŸtu:\n{e}")
            return False
    
    def quick_analyze(self):
        """HÄ±zlÄ± analiz yap (sadece diyarizasyon)"""
        if not self.current_audio_file and len(self.recorded_audio) == 0:
            messagebox.showwarning("UyarÄ±", "Ã–nce ses kaydÄ± yapÄ±n veya dosya yÃ¼kleyin!")
            return
            
        self.add_log("âš¡ HÄ±zlÄ± analiz baÅŸlatÄ±lÄ±yor...")
        self.status_label.config(text="âš¡ HÄ±zlÄ± analiz yapÄ±lÄ±yor...")
        self.quick_analyze_button.config(state=tk.DISABLED)
        
        # Sadece diyarizasyon iÃ§in diÄŸer seÃ§enekleri geÃ§ici olarak kapat
        original_states = {
            'vad': self.enable_vad.get(),
            'overlap': self.enable_overlap.get(),
            'emotion': self.enable_emotion.get(),
            'noise': self.enable_noise_reduction.get(),
            'separation': self.enable_separation.get()
        }
        
        # HÄ±zlÄ± analiz iÃ§in seÃ§enekleri kapat
        self.enable_vad.set(False)
        self.enable_overlap.set(False)
        self.enable_emotion.set(False)
        self.enable_noise_reduction.set(False)
        self.enable_separation.set(False)
        
        # Analizi baÅŸlat
        filename = self.current_audio_file or self.get_current_filename()
        threading.Thread(target=self.run_analysis, args=(filename, True, original_states)).start()
    
    def analyze_recording(self):
        """Tam analiz yap"""
        if not self.current_audio_file and len(self.recorded_audio) == 0:
            messagebox.showwarning("UyarÄ±", "Ã–nce ses kaydÄ± yapÄ±n veya dosya yÃ¼kleyin!")
            return
            
        filename = self.current_audio_file or self.get_current_filename()
        
        if not os.path.exists(filename):
            messagebox.showerror("Hata", f"'{filename}' dosyasÄ± bulunamadÄ±!")
            return
        
        self.add_log("ğŸ” Tam analiz baÅŸlatÄ±lÄ±yor...")
        self.status_label.config(text="ğŸ” Tam analiz yapÄ±lÄ±yor...")
        self.analyze_button.config(state=tk.DISABLED)
        
        # Ä°ÅŸ parÃ§acÄ±ÄŸÄ±nÄ± baÅŸlat
        threading.Thread(target=self.run_analysis, args=(filename,)).start()
    
    def get_current_filename(self):
        """GeÃ§erli dosya adÄ±nÄ± al"""
        filename = self.filename_var.get()
        if not filename.endswith(".wav"):
            filename += ".wav"
        return filename
    
    def run_analysis(self, audio_file, is_quick=False, restore_states=None):
        """KapsamlÄ± ses analizi yap"""
        try:
            # Modelleri yÃ¼kle
            if not self.load_models():
                return
            
            start_time = time.time()
            
            # Ses dosyasÄ±nÄ± yÃ¼kle
            self.add_log(f"ğŸ“‚ Ses dosyasÄ± yÃ¼kleniyor: {os.path.basename(audio_file)}")
            audio_data, sample_rate = librosa.load(audio_file, sr=None)
            
            # GeliÅŸmiÅŸ duygu analizi filtreleme sistemi
            if self.enable_noise_reduction.get():
                self.add_log("ğŸ”‡ GÃ¼rÃ¼ltÃ¼ azaltma iÅŸlemi baÅŸlatÄ±lÄ±yor...")
                # Standart gÃ¼rÃ¼ltÃ¼ azaltma
                audio_data = self.apply_noise_reduction(audio_data, sample_rate)
                
                # Duygu analizi iÃ§in geliÅŸmiÅŸ filtreleme
                if self.enable_emotion.get():
                    audio_data = self.apply_advanced_emotion_filtering(audio_data, sample_rate)
                    
                    # Spektral domain iyileÅŸtirmeleri
                    audio_data = self.apply_spectral_emotion_enhancement(audio_data, sample_rate)
                    
                    # Psiko-akustik filtreleme
                    audio_data = self.apply_psychoacoustic_filtering(audio_data, sample_rate)
                
                self.noise_reduced_audio = audio_data
            
            # 1. Voice Activity Detection (VAD)
            if self.enable_vad.get():
                self.add_log("ğŸ¯ Ses Aktivitesi Tespiti yapÄ±lÄ±yor...")
                self.vad_result = self.run_vad_analysis(audio_file)
                
            # 2. Diyarizasyon
            self.add_log("ğŸ‘¥ KonuÅŸmacÄ± diyarizasyonu yapÄ±lÄ±yor...")
            self.diarization_result = self.run_diarization(audio_file)
            
            # 3. Ã–rtÃ¼ÅŸen konuÅŸma tespiti
            if self.enable_overlap.get():
                self.add_log("ğŸ—£ï¸ Ã–rtÃ¼ÅŸen konuÅŸma tespiti yapÄ±lÄ±yor...")
                self.overlap_result = self.run_overlap_detection(audio_file)
            
            # 4. Ses ayrÄ±ÅŸtÄ±rma
            if self.enable_separation.get():
                self.add_log("ğŸ¼ Ses ayrÄ±ÅŸtÄ±rma iÅŸlemi yapÄ±lÄ±yor...")
                self.run_speech_separation(audio_file)
            
            # 5. KonuÅŸmacÄ± tanÄ±ma ve embedding'ler
            self.add_log("ğŸ” KonuÅŸmacÄ± embedding'leri Ã§Ä±karÄ±lÄ±yor...")
            self.extract_speaker_embeddings(audio_file, self.diarization_result)
            
            # 6. Transkripsiyon
            self.add_log("ğŸ“ KonuÅŸma transkripte ediliyor...")
            self.run_speaker_based_transcription(audio_file, self.diarization_result)
            
            # 7. Duygu analizi
            if self.enable_emotion.get():
                self.add_log("ğŸ˜Š Duygu analizi yapÄ±lÄ±yor...")
                self.emotion_result = self.run_ml_emotion_analysis(audio_data, sample_rate)
            
            # 8. Cinsiyet ve yaÅŸ analizi
            self.add_log("ğŸ‘¥ Cinsiyet ve yaÅŸ analizi baÅŸlatÄ±lÄ±yor...")
            self.gender_age_result = self.run_gender_age_analysis(audio_data, sample_rate, self.diarization_result)
            
            # 9. GÃ¶rselleÅŸtirmeleri gÃ¼ncelle
            self.update_advanced_visualizations(audio_data, sample_rate)
            
            # 10. DetaylÄ± rapor oluÅŸtur
            self.generate_detailed_analysis_report()
            
            # SÃ¼re hesapla
            end_time = time.time()
            total_time = end_time - start_time
            
            self.add_log(f"ğŸ‰ Analiz tamamlandÄ±! Toplam sÃ¼re: {total_time:.2f} saniye")
            self.status_label.config(text="âœ… Analiz tamamlandÄ±")
            
            # HÄ±zlÄ± analiz durumunu geri yÃ¼kle
            if is_quick and restore_states:
                for key, value in restore_states.items():
                    if key == 'vad':
                        self.enable_vad.set(value)
                    elif key == 'overlap':
                        self.enable_overlap.set(value)
                    elif key == 'emotion':
                        self.enable_emotion.set(value)
                    elif key == 'noise':
                        self.enable_noise_reduction.set(value)
                    elif key == 'separation':
                        self.enable_separation.set(value)
            
        except Exception as e:
            self.add_log(f"âŒ Analiz hatasÄ±: {e}")
            self.status_label.config(text="âŒ Analiz hatasÄ±!")
            messagebox.showerror("Analiz HatasÄ±", f"Analiz sÄ±rasÄ±nda hata oluÅŸtu:\n{e}")
        finally:
            # DÃ¼ÄŸmeleri yeniden etkinleÅŸtir
            self.analyze_button.config(state=tk.NORMAL)
            self.quick_analyze_button.config(state=tk.NORMAL)
    
    def run_diarization(self, audio_file):
        """PyAnnote diyarizasyon iÅŸlemini Ã§alÄ±ÅŸtÄ±r"""
        try:
            # Diyarizasyon uygula
            diarization = self.pipelines['diarization'](audio_file)
            
            # SonuÃ§larÄ± iÅŸle ve gÃ¶ster
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"ğŸ¯ Diyarizasyon SonuÃ§larÄ±: {os.path.basename(audio_file)}\n")
            self.result_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # KonuÅŸmacÄ±larÄ± ve zaman aralÄ±klarÄ±nÄ± gÃ¶ster
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                start = turn.start
                end = turn.end
                duration = end - start
                speaker_segments.append({
                    'speaker': speaker,
                    'start': start,
                    'end': end,
                    'duration': duration
                })
                result_line = f"ğŸ‘¤ {speaker}: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n"
                self.result_text.insert(tk.END, result_line)
            
            # KonuÅŸmacÄ± istatistiklerini hesapla
            speaker_stats = {}
            total_speech_time = 0
            for segment in speaker_segments:
                speaker = segment['speaker']
                duration = segment['duration']
                if speaker not in speaker_stats:
                    speaker_stats[speaker] = {'total_time': 0, 'segment_count': 0}
                speaker_stats[speaker]['total_time'] += duration
                speaker_stats[speaker]['segment_count'] += 1
                total_speech_time += duration
            
            # Ä°statistikleri gÃ¶ster
            self.result_text.insert(tk.END, "\nğŸ“Š KonuÅŸmacÄ± Ä°statistikleri:\n")
            self.result_text.insert(tk.END, f"ğŸ“ˆ Toplam konuÅŸma sÃ¼resi: {total_speech_time:.2f} saniye\n")
            self.result_text.insert(tk.END, f"ğŸ‘¥ Toplam konuÅŸmacÄ± sayÄ±sÄ±: {len(speaker_stats)}\n\n")
            
            for speaker, stats in speaker_stats.items():
                percentage = (stats['total_time'] / total_speech_time) * 100 if total_speech_time > 0 else 0
                self.result_text.insert(tk.END, 
                    f"{speaker}:\n"
                    f"  â±ï¸ SÃ¼re: {stats['total_time']:.2f}s ({percentage:.1f}%)\n"
                    f"  ğŸ’¬ Segment sayÄ±sÄ±: {stats['segment_count']}\n"
                    f"  ğŸ“ Ortalama segment: {stats['total_time']/stats['segment_count']:.2f}s\n"
                )
                
                # Cinsiyet ve yaÅŸ bilgilerini ekle (eÄŸer analiz yapÄ±lmÄ±ÅŸsa)
                if hasattr(self, 'gender_age_result') and self.gender_age_result:
                    if 'detailed' in self.gender_age_result and 'speaker_based' in self.gender_age_result['detailed']:
                        speaker_results = self.gender_age_result['detailed']['speaker_based']
                        if speaker in speaker_results:
                            speaker_data = speaker_results[speaker]
                            
                            # En yÃ¼ksek skorlu cinsiyet ve yaÅŸ
                            dominant_gender = max(speaker_data['gender'].items(), key=lambda x: x[1])
                            dominant_age = max(speaker_data['age'].items(), key=lambda x: x[1])
                            confidence = speaker_data.get('confidence', 0.5)
                            
                            # Ä°konlar
                            gender_icons = {'erkek': 'ğŸ‘¨', 'kadÄ±n': 'ğŸ‘©', 'belirsiz': 'â“'}
                            age_icons = {
                                'Ã§ocuk': 'ğŸ‘¶', 'genÃ§': 'ğŸ§’', 'yetiÅŸkin': 'ğŸ‘¤', 
                                'orta_yaÅŸ': 'ğŸ§‘', 'yaÅŸlÄ±': 'ğŸ‘´'
                            }
                            
                            gender_icon = gender_icons.get(dominant_gender[0], 'â“')
                            age_icon = age_icons.get(dominant_age[0], 'â“')
                            
                            self.result_text.insert(tk.END, 
                                f"  {gender_icon} Cinsiyet: {dominant_gender[0].capitalize()} ({dominant_gender[1]*100:.1f}%)\n"
                                f"  {age_icon} YaÅŸ Grubu: {dominant_age[0].capitalize()} ({dominant_age[1]*100:.1f}%)\n"
                                f"  ğŸ¯ GÃ¼ven: {confidence*100:.1f}%\n"
                            )
                
                self.result_text.insert(tk.END, "\n")
            
            self.add_log("âœ… Diyarizasyon tamamlandÄ±.")
            return diarization
            
        except Exception as e:
            self.add_log(f"âŒ Diyarizasyon hatasÄ±: {e}")
            return None
    
    def apply_noise_reduction(self, audio_data, sample_rate):
        """GÃ¼rÃ¼ltÃ¼ azaltma uygula"""
        try:
            # Ã–nce ses verisini temizle
            audio_data = self.clean_audio_buffer(audio_data)
            
            # NoiseReduce kÃ¼tÃ¼phanesi ile gÃ¼rÃ¼ltÃ¼ azaltma (daha muhafazakar)
            reduced_noise = nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=0.6)  # 0.8'den 0.6'ya dÃ¼ÅŸÃ¼rdÃ¼k
            
            # Sonucu tekrar temizle
            reduced_noise = self.clean_audio_buffer(reduced_noise)
            
            self.add_log("âœ… GÃ¼rÃ¼ltÃ¼ azaltma tamamlandÄ±")
            return reduced_noise
        except Exception as e:
            self.add_log(f"âš ï¸ GÃ¼rÃ¼ltÃ¼ azaltma hatasÄ±: {e}")
            # Hata durumunda temizlenmiÅŸ orijinal veriyi dÃ¶ndÃ¼r
            return self.clean_audio_buffer(audio_data)
    
    def apply_advanced_emotion_filtering(self, audio_data, sample_rate):
        """Duygu analizi iÃ§in geliÅŸmiÅŸ filtreleme sistemi"""
        try:
            self.add_log("ğŸ­ Duygu analizi iÃ§in geliÅŸmiÅŸ filtreleme baÅŸlatÄ±lÄ±yor...")
            
            # 1. Adaptif GÃ¼rÃ¼ltÃ¼ Azaltma - Duygu tonlarÄ±nÄ± koruyucu
            filtered_audio = self.adaptive_noise_reduction(audio_data, sample_rate)
            
            # 2. Vokal Frekans Vurgulama (Ä°nsan sesi 80-8000 Hz)
            filtered_audio = self.enhance_vocal_frequencies(filtered_audio, sample_rate)
            
            # 3. Duygusal Tonlama KorumasÄ±
            filtered_audio = self.preserve_emotional_tones(filtered_audio, sample_rate)
            
            # 4. Dinamik AralÄ±k Optimizasyonu
            filtered_audio = self.optimize_dynamic_range(filtered_audio)
            
            # 5. Kahkaha ve Ã–zel Ses Desenlerini Koruma
            filtered_audio = self.preserve_laughter_patterns(filtered_audio, sample_rate)
            
            self.add_log("âœ… GeliÅŸmiÅŸ duygu filtreleme tamamlandÄ±")
            return filtered_audio
            
        except Exception as e:
            self.add_log(f"âŒ GeliÅŸmiÅŸ filtreleme hatasÄ±: {e}")
            return audio_data
    
    def adaptive_noise_reduction(self, audio_data, sample_rate):
        """Adaptif gÃ¼rÃ¼ltÃ¼ azaltma - duygu tonlarÄ±nÄ± korur"""
        try:
            # Ses enerjisine gÃ¶re adaptif filtreleme
            energy_threshold = np.percentile(np.abs(audio_data), 70)
            
            if energy_threshold < 0.01:  # Ã‡ok sessiz ses
                # Hafif filtreleme - duygusal nÃ¼anslarÄ± korur
                return nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=0.3)
            elif energy_threshold > 0.1:  # YÃ¼ksek enerjili ses (baÄŸÄ±rma, kahkaha)
                # Orta seviye filtreleme
                return nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=0.5)
            else:  # Normal konuÅŸma
                # Standart filtreleme
                return nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=0.6)
                
        except Exception as e:
            return audio_data
    
    def enhance_vocal_frequencies(self, audio_data, sample_rate):
        """Ä°nsan sesi frekanslarÄ±nÄ± vurgula (80-8000 Hz)"""
        try:
            # FFT ile frekans alanÄ±na geÃ§
            fft = np.fft.fft(audio_data)
            freqs = np.fft.fftfreq(len(audio_data), 1/sample_rate)
            
            # Ä°nsan sesi frekans maskesi oluÅŸtur
            vocal_mask = (np.abs(freqs) >= 80) & (np.abs(freqs) <= 8000)
            
            # Vokal frekanslarÄ± hafifÃ§e vurgula
            fft[vocal_mask] *= 1.2
            
            # Ã‡ok yÃ¼ksek frekanslarÄ± azalt (gÃ¼rÃ¼ltÃ¼ olabilir)
            high_freq_mask = np.abs(freqs) > 8000
            fft[high_freq_mask] *= 0.7
            
            # Geri dÃ¶nÃ¼ÅŸtÃ¼r
            enhanced_audio = np.real(np.fft.ifft(fft))
            
            # AÅŸÄ±rÄ± bÃ¼yÃ¼me kontrolÃ¼
            max_val = np.max(np.abs(enhanced_audio))
            if max_val > 1.0:
                enhanced_audio = enhanced_audio / max_val * 0.95
                
            return enhanced_audio.astype(np.float32)
            
        except Exception as e:
            return audio_data
    
    def preserve_emotional_tones(self, audio_data, sample_rate):
        """Duygusal tonlamalarÄ± koruyucu filtreleme"""
        try:
            # Pitch tracking ile duygusal tonlama tespiti
            pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sample_rate)
            
            # Pitch deÄŸiÅŸkenliÄŸi analizi
            pitch_values = pitches[pitches > 0]
            if len(pitch_values) > 0:
                pitch_variance = np.var(pitch_values)
                
                # YÃ¼ksek pitch varyansÄ± = duygusal konuÅŸma
                if pitch_variance > 1000:  # Duygusal konuÅŸma tespit edildi
                    # Daha az agresif filtreleme uygula
                    return nr.reduce_noise(y=audio_data, sr=sample_rate, 
                                         prop_decrease=0.4)  # Ã‡ok hafif
                else:
                    # Normal filtreleme
                    return nr.reduce_noise(y=audio_data, sr=sample_rate, 
                                         prop_decrease=0.6)
            else:
                return audio_data
                
        except Exception as e:
            return audio_data
    
    def optimize_dynamic_range(self, audio_data):
        """Dinamik aralÄ±ÄŸÄ± optimize et - duygu analizine uygun"""
        try:
            # Ses seviyesi analizi
            rms = np.sqrt(np.mean(audio_data**2))
            
            if rms < 0.01:  # Ã‡ok sessiz
                # Hafif amplifikasyon
                amplified = audio_data * 2.0
                return np.clip(amplified, -1.0, 1.0)
            elif rms > 0.3:  # Ã‡ok yÃ¼ksek
                # Hafif kompresyon
                compressed = audio_data * 0.7
                return compressed
            else:
                return audio_data
                
        except Exception as e:
            return audio_data
    
    def preserve_laughter_patterns(self, audio_data, sample_rate):
        """Kahkaha ve Ã¶zel ses desenlerini koruma"""
        try:
            # KÄ±sa-dÃ¶nem enerji analizi (kahkaha tespiti iÃ§in)
            frame_length = int(0.025 * sample_rate)  # 25ms
            hop_length = int(0.01 * sample_rate)     # 10ms
            
            energy_frames = []
            for i in range(0, len(audio_data) - frame_length, hop_length):
                frame = audio_data[i:i + frame_length]
                energy = np.sum(frame ** 2)
                energy_frames.append(energy)
            
            energy_frames = np.array(energy_frames)
            
            # Ani enerji artÄ±ÅŸlarÄ± (kahkaha gÃ¶stergesi)
            energy_diff = np.diff(energy_frames)
            sudden_peaks = np.where(energy_diff > np.percentile(energy_diff, 90))[0]
            
            if len(sudden_peaks) > 3:  # Muhtemelen kahkaha var
                # Ã‡ok hafif filtreleme - kahkaha desenlerini koru
                return nr.reduce_noise(y=audio_data, sr=sample_rate, 
                                     prop_decrease=0.2)
            else:
                return audio_data
                
        except Exception as e:
            return audio_data
    
    def apply_spectral_emotion_enhancement(self, audio_data, sample_rate):
        """Spektral domain'de duygu analizi iÃ§in Ã¶zel iyileÅŸtirmeler"""
        try:
            self.add_log("ğŸµ Spektral duygu iyileÅŸtirmesi baÅŸlatÄ±lÄ±yor...")
            
            # STFT ile spektral analiz
            stft = librosa.stft(audio_data, n_fft=2048, hop_length=512)
            magnitude = np.abs(stft)
            phase = np.angle(stft)
            
            # Frekans bantlarÄ± tanÄ±mla
            freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=2048)
            
            # Duygu-spesifik frekans bantlarÄ±
            emotion_bands = {
                'fundamental': (80, 300),    # Temel ses perdesi
                'formants': (300, 3000),     # Formant frekanslarÄ±
                'brightness': (3000, 8000),  # ParlaklÄ±k (mutluluk gÃ¶stergesi)
                'breathiness': (8000, 12000) # Nefes sesleri (duygusal durum)
            }
            
            # Her bant iÃ§in Ã¶zel iÅŸlem
            enhanced_magnitude = magnitude.copy()
            
            for band_name, (low_freq, high_freq) in emotion_bands.items():
                # Frekans maskesi oluÅŸtur
                freq_mask = (freqs >= low_freq) & (freqs <= high_freq)
                
                if band_name == 'brightness':
                    # ParlaklÄ±k bandÄ±nÄ± hafif vurgula (mutluluk iÃ§in)
                    enhanced_magnitude[freq_mask] *= 1.1
                elif band_name == 'formants':
                    # Formant bandÄ±nÄ± gÃ¼Ã§lendir (konuÅŸma netliÄŸi iÃ§in)
                    enhanced_magnitude[freq_mask] *= 1.05
                elif band_name == 'breathiness':
                    # Nefes seslerini azalt ama tamamen silme
                    enhanced_magnitude[freq_mask] *= 0.9
            
            # Geri dÃ¶nÃ¼ÅŸtÃ¼r
            enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
            enhanced_audio = librosa.istft(enhanced_stft, hop_length=512)
            
            # Seviye kontrolÃ¼
            max_val = np.max(np.abs(enhanced_audio))
            if max_val > 1.0:
                enhanced_audio = enhanced_audio / max_val * 0.95
            
            self.add_log("âœ… Spektral duygu iyileÅŸtirmesi tamamlandÄ±")
            return enhanced_audio.astype(np.float32)
            
        except Exception as e:
            self.add_log(f"âŒ Spektral iyileÅŸtirme hatasÄ±: {e}")
            return audio_data
    
    def apply_psychoacoustic_filtering(self, audio_data, sample_rate):
        """Psiko-akustik prensiplere dayalÄ± filtreleme"""
        try:
            self.add_log("ğŸ§  Psiko-akustik filtreleme baÅŸlatÄ±lÄ±yor...")
            
            # Ä°nsan iÅŸitme eÄŸrisi (A-weighting benzeri)
            freqs = np.fft.fftfreq(len(audio_data), 1/sample_rate)
            fft = np.fft.fft(audio_data)
            
            # Ä°nsan kulaÄŸÄ±nÄ±n hassasiyet eÄŸrisi
            def hearing_sensitivity(f):
                """Ä°nsan kulaÄŸÄ±nÄ±n frekans hassasiyeti"""
                f = np.abs(f)
                if f < 20:
                    return 0.1
                elif f < 200:
                    return 0.3 + 0.7 * (f - 20) / 180
                elif f < 1000:
                    return 1.0
                elif f < 4000:
                    return 1.0 + 0.2 * (f - 1000) / 3000  # Maksimum hassasiyet
                elif f < 8000:
                    return 1.2 - 0.3 * (f - 4000) / 4000
                else:
                    return 0.9 * np.exp(-(f - 8000) / 8000)
            
            # Hassasiyet eÄŸrisini uygula
            sensitivity_curve = np.array([hearing_sensitivity(f) for f in freqs])
            
            # FFT'yi aÄŸÄ±rlÄ±klandÄ±r
            weighted_fft = fft * sensitivity_curve
            
            # Geri dÃ¶nÃ¼ÅŸtÃ¼r
            filtered_audio = np.real(np.fft.ifft(weighted_fft))
            
            # Normalize
            max_val = np.max(np.abs(filtered_audio))
            if max_val > 0:
                filtered_audio = filtered_audio / max_val * np.max(np.abs(audio_data))
            
            self.add_log("âœ… Psiko-akustik filtreleme tamamlandÄ±")
            return filtered_audio.astype(np.float32)
            
        except Exception as e:
            self.add_log(f"âŒ Psiko-akustik filtreleme hatasÄ±: {e}")
            return audio_data
    
    def run_gender_age_analysis(self, audio_data, sample_rate, diarization_result=None):
        """KapsamlÄ± cinsiyet ve yaÅŸ analizi"""
        try:
            self.add_log("ğŸ‘¥ Cinsiyet ve yaÅŸ analizi baÅŸlatÄ±lÄ±yor...")
            
            # Ses verisini temizle
            audio_data = self.clean_audio_buffer(audio_data)
            
            # Ã‡oklu yaklaÅŸÄ±m ile analiz
            results = {}
            
            # 1. Ses Ã¶zellik tabanlÄ± analiz
            feature_based_results = self.feature_based_gender_age_analysis(audio_data, sample_rate)
            results['feature_based'] = feature_based_results
            
            # 2. Frekans domain analizi
            frequency_based_results = self.frequency_domain_gender_age_analysis(audio_data, sample_rate)
            results['frequency_based'] = frequency_based_results
            
            # 3. Deep Learning tabanlÄ± analiz (Transformers)
            try:
                dl_results = self.deep_learning_gender_age_analysis(audio_data, sample_rate)
                results['deep_learning'] = dl_results
            except Exception as e:
                self.add_log(f"âš ï¸ Deep learning analizi baÅŸarÄ±sÄ±z: {e}")
                results['deep_learning'] = None
            
            # 4. KonuÅŸmacÄ± bazlÄ± analiz (eÄŸer diyarizasyon varsa)
            if diarization_result:
                speaker_results = self.speaker_based_gender_age_analysis(audio_data, sample_rate, diarization_result)
                results['speaker_based'] = speaker_results
            
            # 5. SonuÃ§larÄ± birleÅŸtir (ensemble)
            final_results = self.ensemble_gender_age_results(results)
            
            # 6. SonuÃ§larÄ± kaydet ve gÃ¶ster
            self.gender_age_result = final_results
            self.display_gender_age_results(final_results)
            
            # 7. KonuÅŸmacÄ± bazlÄ± sonuÃ§larÄ± log'a yazdÄ±r
            if 'detailed' in final_results and 'speaker_based' in final_results['detailed']:
                speaker_results = final_results['detailed']['speaker_based']
                if speaker_results:
                    self.add_log("ğŸ‘¥ KonuÅŸmacÄ± bazlÄ± sonuÃ§lar:")
                    for speaker, speaker_data in speaker_results.items():
                        dominant_gender = max(speaker_data['gender'].items(), key=lambda x: x[1])
                        dominant_age = max(speaker_data['age'].items(), key=lambda x: x[1])
                        confidence = speaker_data.get('confidence', 0.5)
                        
                        gender_icons = {'erkek': 'ğŸ‘¨', 'kadÄ±n': 'ğŸ‘©', 'belirsiz': 'â“'}
                        age_icons = {'Ã§ocuk': 'ğŸ‘¶', 'genÃ§': 'ğŸ§’', 'yetiÅŸkin': 'ğŸ‘¤', 'orta_yaÅŸ': 'ğŸ§‘', 'yaÅŸlÄ±': 'ğŸ‘´'}
                        
                        gender_icon = gender_icons.get(dominant_gender[0], 'â“')
                        age_icon = age_icons.get(dominant_age[0], 'â“')
                        
                        self.add_log(f"  ğŸ¤ {speaker}: {gender_icon} {dominant_gender[0]} ({dominant_gender[1]*100:.1f}%), "
                                   f"{age_icon} {dominant_age[0]} ({dominant_age[1]*100:.1f}%), gÃ¼ven: {confidence*100:.1f}%")
            
            self.add_log("âœ… Cinsiyet ve yaÅŸ analizi tamamlandÄ±")
            return final_results
            
        except Exception as e:
            self.add_log(f"âŒ Cinsiyet ve yaÅŸ analizi hatasÄ±: {e}")
            return {}
    
    def feature_based_gender_age_analysis(self, audio_data, sample_rate):
        """Ses Ã¶zellik tabanlÄ± cinsiyet ve yaÅŸ analizi"""
        try:
            self.add_log("ğŸµ Ã–zellik tabanlÄ± cinsiyet-yaÅŸ analizi...")
            
            # GeliÅŸmiÅŸ Ã¶zellik Ã§Ä±karÄ±mÄ±
            features = self.extract_advanced_audio_features(audio_data, sample_rate)
            
            if not features:
                return self.get_default_gender_age_results()
            
            # CÄ°NSÄ°YET ANALÄ°ZÄ°
            gender_scores = {'erkek': 0.0, 'kadÄ±n': 0.0, 'belirsiz': 0.0}
            
            # Temel perde analizi (en gÃ¼venilir gÃ¶sterge)
            if features['pitch_mean'] < 165:  # Erkek sesi (genelde 85-165 Hz)
                gender_scores['erkek'] += 0.4
            elif features['pitch_mean'] > 165:  # KadÄ±n sesi (genelde 165-265 Hz)
                gender_scores['kadÄ±n'] += 0.4
            else:
                gender_scores['belirsiz'] += 0.2
            
            # Formant frekanslarÄ± (ikinci en gÃ¼venilir)
            if features['spectral_centroid_mean'] < 1200:  # Erkek formantlarÄ± daha dÃ¼ÅŸÃ¼k
                gender_scores['erkek'] += 0.3
            elif features['spectral_centroid_mean'] > 1400:  # KadÄ±n formantlarÄ± daha yÃ¼ksek
                gender_scores['kadÄ±n'] += 0.3
            
            # Ses kalÄ±nlÄ±ÄŸÄ± ve tonu
            if features['spectral_bandwidth_mean'] > 2000:  # GeniÅŸ spektrum = genelde erkek
                gender_scores['erkek'] += 0.2
            elif features['spectral_bandwidth_mean'] < 1500:  # Dar spektrum = genelde kadÄ±n
                gender_scores['kadÄ±n'] += 0.2
            
            # KonuÅŸma hÄ±zÄ± ve ritim
            if features['speaking_rate'] > 3:  # HÄ±zlÄ± konuÅŸma
                gender_scores['kadÄ±n'] += 0.1  # Ä°statistiksel olarak kadÄ±nlar daha hÄ±zlÄ± konuÅŸur
            elif features['speaking_rate'] < 2:  # YavaÅŸ konuÅŸma
                gender_scores['erkek'] += 0.1
            
            # YAÅ ANALÄ°ZÄ°
            age_scores = {'Ã§ocuk': 0.0, 'genÃ§': 0.0, 'yetiÅŸkin': 0.0, 'orta_yaÅŸ': 0.0, 'yaÅŸlÄ±': 0.0}
            
            # Perde deÄŸiÅŸkenliÄŸi (yaÅŸ ile ters orantÄ±lÄ±)
            if features['pitch_std'] > 80:  # YÃ¼ksek deÄŸiÅŸkenlik = genÃ§
                age_scores['Ã§ocuk'] += 0.2
                age_scores['genÃ§'] += 0.3
            elif features['pitch_std'] < 30:  # DÃ¼ÅŸÃ¼k deÄŸiÅŸkenlik = yaÅŸlÄ±
                age_scores['orta_yaÅŸ'] += 0.2
                age_scores['yaÅŸlÄ±'] += 0.3
            else:
                age_scores['yetiÅŸkin'] += 0.3
            
            # Ses titremesi (yaÅŸla artar)
            if features['zcr_std'] > 0.05:  # YÃ¼ksek titreme
                age_scores['yaÅŸlÄ±'] += 0.3
            elif features['zcr_std'] < 0.02:  # DÃ¼ÅŸÃ¼k titreme
                age_scores['Ã§ocuk'] += 0.1
                age_scores['genÃ§'] += 0.2
                age_scores['yetiÅŸkin'] += 0.2
            
            # KonuÅŸma hÄ±zÄ± ve duraklama
            if features['speaking_rate'] > 4:  # Ã‡ok hÄ±zlÄ±
                age_scores['Ã§ocuk'] += 0.2
                age_scores['genÃ§'] += 0.1
            elif features['speaking_rate'] < 1.5:  # Ã‡ok yavaÅŸ
                age_scores['yaÅŸlÄ±'] += 0.3
            
            # Sessizlik oranÄ± (yaÅŸla artar)
            if features['silence_ratio'] > 0.6:  # Ã‡ok sessizlik
                age_scores['yaÅŸlÄ±'] += 0.2
            elif features['silence_ratio'] < 0.2:  # Az sessizlik
                age_scores['genÃ§'] += 0.2
            
            # Enerji kararlÄ±lÄ±ÄŸÄ±
            if features['energy_variance'] < 0.0001:  # Ã‡ok kararlÄ±
                age_scores['yetiÅŸkin'] += 0.2
                age_scores['orta_yaÅŸ'] += 0.1
            elif features['energy_variance'] > 0.001:  # DeÄŸiÅŸken
                age_scores['Ã§ocuk'] += 0.1
                age_scores['genÃ§'] += 0.2
            
            # Normalize et
            gender_total = sum(gender_scores.values())
            if gender_total > 0:
                for gender in gender_scores:
                    gender_scores[gender] /= gender_total
            else:
                gender_scores = {'erkek': 0.5, 'kadÄ±n': 0.5, 'belirsiz': 0.0}
            
            age_total = sum(age_scores.values())
            if age_total > 0:
                for age in age_scores:
                    age_scores[age] /= age_total
            else:
                age_scores = {'yetiÅŸkin': 0.6, 'genÃ§': 0.3, 'orta_yaÅŸ': 0.1, 'Ã§ocuk': 0.0, 'yaÅŸlÄ±': 0.0}
            
            return {
                'gender': gender_scores,
                'age': age_scores,
                'confidence': self.calculate_gender_age_confidence(features, gender_scores, age_scores)
            }
            
        except Exception as e:
            self.add_log(f"âŒ Ã–zellik tabanlÄ± analiz hatasÄ±: {e}")
            return self.get_default_gender_age_results()
    
    def frequency_domain_gender_age_analysis(self, audio_data, sample_rate):
        """Frekans domain cinsiyet ve yaÅŸ analizi"""
        try:
            self.add_log("ğŸ“Š Frekans domain analizi...")
            
            # FFT analizi
            fft = np.fft.fft(audio_data)
            freqs = np.fft.fftfreq(len(audio_data), 1/sample_rate)
            magnitude = np.abs(fft)
            
            # Frekans bantlarÄ±
            low_freq = magnitude[(np.abs(freqs) >= 80) & (np.abs(freqs) <= 300)]    # Temel frekans
            mid_freq = magnitude[(np.abs(freqs) >= 300) & (np.abs(freqs) <= 3000)]  # Formant bÃ¶lgesi
            high_freq = magnitude[(np.abs(freqs) >= 3000) & (np.abs(freqs) <= 8000)] # YÃ¼ksek frekanslar
            
            # Enerji daÄŸÄ±lÄ±mÄ±
            low_energy = np.sum(low_freq)
            mid_energy = np.sum(mid_freq)
            high_energy = np.sum(high_freq)
            total_energy = low_energy + mid_energy + high_energy
            
            if total_energy == 0:
                return self.get_default_gender_age_results()
            
            low_ratio = low_energy / total_energy
            mid_ratio = mid_energy / total_energy
            high_ratio = high_energy / total_energy
            
            # Cinsiyet analizi
            gender_scores = {'erkek': 0.0, 'kadÄ±n': 0.0, 'belirsiz': 0.0}
            
            if low_ratio > 0.4:  # DÃ¼ÅŸÃ¼k frekans dominant = erkek
                gender_scores['erkek'] += 0.4
            elif high_ratio > 0.3:  # YÃ¼ksek frekans dominant = kadÄ±n
                gender_scores['kadÄ±n'] += 0.4
            else:
                gender_scores['belirsiz'] += 0.2
            
            # Orta frekans analizi (formantlar)
            if mid_ratio > 0.5:
                # Formant detayÄ± iÃ§in daha derinlemesine analiz
                formant_peak_freq = freqs[np.abs(freqs) <= 3000][np.argmax(magnitude[(np.abs(freqs) >= 300) & (np.abs(freqs) <= 3000)])] + 300
                
                if formant_peak_freq < 1000:  # DÃ¼ÅŸÃ¼k formant = erkek
                    gender_scores['erkek'] += 0.3
                elif formant_peak_freq > 1200:  # YÃ¼ksek formant = kadÄ±n
                    gender_scores['kadÄ±n'] += 0.3
            
            # YaÅŸ analizi
            age_scores = {'Ã§ocuk': 0.0, 'genÃ§': 0.0, 'yetiÅŸkin': 0.0, 'orta_yaÅŸ': 0.0, 'yaÅŸlÄ±': 0.0}
            
            # Ã‡ok yÃ¼ksek frekanslar (Ã§ocuk sesi gÃ¶stergesi)
            ultra_high = magnitude[np.abs(freqs) > 8000]
            if len(ultra_high) > 0 and np.sum(ultra_high) / total_energy > 0.1:
                age_scores['Ã§ocuk'] += 0.3
            
            # Frekans daÄŸÄ±lÄ±mÄ±nÄ±n dÃ¼zenliliÄŸi
            spectral_flatness = np.mean(magnitude) / (np.max(magnitude) + 1e-10)
            
            if spectral_flatness > 0.1:  # DÃ¼zensiz spektrum = yaÅŸlÄ±
                age_scores['yaÅŸlÄ±'] += 0.3
            elif spectral_flatness < 0.05:  # DÃ¼zenli spektrum = genÃ§/yetiÅŸkin
                age_scores['genÃ§'] += 0.2
                age_scores['yetiÅŸkin'] += 0.2
            
            # Harmonik yapÄ± analizi
            try:
                harmonics = []
                fundamental_freq = freqs[np.argmax(magnitude)]
                for i in range(2, 6):  # 2. ile 5. harmonikler
                    harmonic_freq = fundamental_freq * i
                    if harmonic_freq < sample_rate / 2:
                        harmonic_idx = np.argmin(np.abs(freqs - harmonic_freq))
                        harmonics.append(magnitude[harmonic_idx])
                
                if harmonics:
                    harmonic_strength = np.mean(harmonics) / (np.max(magnitude) + 1e-10)
                    
                    if harmonic_strength > 0.3:  # GÃ¼Ã§lÃ¼ harmonikler = genÃ§
                        age_scores['genÃ§'] += 0.2
                        age_scores['yetiÅŸkin'] += 0.1
                    elif harmonic_strength < 0.1:  # ZayÄ±f harmonikler = yaÅŸlÄ±
                        age_scores['yaÅŸlÄ±'] += 0.2
            except:
                pass
            
            # Normalize
            gender_total = sum(gender_scores.values())
            if gender_total > 0:
                for gender in gender_scores:
                    gender_scores[gender] /= gender_total
            
            age_total = sum(age_scores.values())
            if age_total > 0:
                for age in age_scores:
                    age_scores[age] /= age_total
            
            return {
                'gender': gender_scores,
                'age': age_scores,
                'confidence': 0.7  # Orta gÃ¼ven
            }
            
        except Exception as e:
            self.add_log(f"âŒ Frekans domain analizi hatasÄ±: {e}")
            return self.get_default_gender_age_results()
    
    def deep_learning_gender_age_analysis(self, audio_data, sample_rate):
        """Deep Learning tabanlÄ± cinsiyet ve yaÅŸ analizi"""
        try:
            self.add_log("ğŸ¤– Deep Learning cinsiyet-yaÅŸ analizi...")
            
            # Transformers ile analiz
            from transformers import pipeline, Wav2Vec2Processor, Wav2Vec2Model
            
            # Ses verisini uygun formata Ã§evir
            if sample_rate != 16000:
                audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
                sample_rate = 16000
            
            # Ã‡oklu model yaklaÅŸÄ±mÄ±
            models_to_try = [
                "facebook/wav2vec2-large-xlsr-53",
                "microsoft/unispeech-sat-base-plus",
                "facebook/hubert-large-ls960-ft"
            ]
            
            results = []
            
            for model_name in models_to_try:
                try:
                    self.add_log(f"ğŸ”„ Model deneniyor: {model_name}")
                    
                    # Model ve processor yÃ¼kle
                    processor = Wav2Vec2Processor.from_pretrained(model_name)
                    model = Wav2Vec2Model.from_pretrained(model_name)
                    
                    # Ses verisini iÅŸle
                    inputs = processor(audio_data, sampling_rate=sample_rate, return_tensors="pt")
                    
                    # Model Ã§Ä±ktÄ±sÄ±nÄ± al
                    with torch.no_grad():
                        outputs = model(**inputs)
                        hidden_states = outputs.last_hidden_state
                    
                    # Ã–zellik vektÃ¶rÃ¼nÃ¼ Ã§Ä±kar (ortalama pooling)
                    feature_vector = torch.mean(hidden_states, dim=1).squeeze().numpy()
                    
                    # Ã–zellik vektÃ¶rÃ¼nÃ¼ cinsiyet ve yaÅŸ analizine Ã§evir
                    gender_age_result = self.analyze_deep_features(feature_vector)
                    results.append(gender_age_result)
                    
                    self.add_log(f"âœ… Model baÅŸarÄ±lÄ±: {model_name}")
                    break  # Ä°lk baÅŸarÄ±lÄ± model ile devam et
                    
                except Exception as model_error:
                    self.add_log(f"âŒ Model hatasÄ± {model_name}: {model_error}")
                    continue
            
            if results:
                return results[0]  # Ä°lk baÅŸarÄ±lÄ± sonucu dÃ¶ndÃ¼r
            else:
                self.add_log("âš ï¸ HiÃ§bir deep learning model Ã§alÄ±ÅŸmadÄ±")
                return self.get_default_gender_age_results()
                
        except ImportError:
            self.add_log("âš ï¸ Transformers kÃ¼tÃ¼phanesi yok, alternatif yÃ¶ntem kullanÄ±lÄ±yor")
            return self.get_default_gender_age_results()
        except Exception as e:
            self.add_log(f"âŒ Deep learning analizi hatasÄ±: {e}")
            return self.get_default_gender_age_results()
    
    def analyze_deep_features(self, feature_vector):
        """Deep learning Ã¶zellik vektÃ¶rÃ¼nÃ¼ cinsiyet ve yaÅŸ analizine Ã§evir"""
        try:
            # Ã–zellik vektÃ¶rÃ¼ istatistikleri
            mean_val = np.mean(feature_vector)
            std_val = np.std(feature_vector)
            max_val = np.max(feature_vector)
            min_val = np.min(feature_vector)
            
            # Basit kural tabanlÄ± analiz (gerÃ§ek projede eÄŸitilmiÅŸ classifier kullanÄ±lÄ±r)
            gender_scores = {'erkek': 0.5, 'kadÄ±n': 0.5, 'belirsiz': 0.0}
            age_scores = {'Ã§ocuk': 0.1, 'genÃ§': 0.3, 'yetiÅŸkin': 0.4, 'orta_yaÅŸ': 0.2, 'yaÅŸlÄ±': 0.0}
            
            # Ã–zellik vektÃ¶rÃ¼ analizine dayalÄ± basit kurallar
            if mean_val > 0.1:
                gender_scores['kadÄ±n'] += 0.2
                gender_scores['erkek'] -= 0.2
            elif mean_val < -0.1:
                gender_scores['erkek'] += 0.2
                gender_scores['kadÄ±n'] -= 0.2
            
            if std_val > 0.5:
                age_scores['genÃ§'] += 0.2
                age_scores['yaÅŸlÄ±'] -= 0.1
            elif std_val < 0.2:
                age_scores['yaÅŸlÄ±'] += 0.2
                age_scores['genÃ§'] -= 0.1
            
            # Normalize
            gender_total = sum(gender_scores.values())
            if gender_total > 0:
                for gender in gender_scores:
                    gender_scores[gender] /= gender_total
            
            age_total = sum(age_scores.values())
            if age_total > 0:
                for age in age_scores:
                    age_scores[age] /= age_total
            
            return {
                'gender': gender_scores,
                'age': age_scores,
                'confidence': 0.6
            }
            
        except Exception as e:
            return self.get_default_gender_age_results()
    
    def speaker_based_gender_age_analysis(self, audio_data, sample_rate, diarization_result):
        """KonuÅŸmacÄ± bazlÄ± cinsiyet ve yaÅŸ analizi"""
        try:
            self.add_log("ğŸ‘¥ KonuÅŸmacÄ± bazlÄ± cinsiyet-yaÅŸ analizi...")
            
            speaker_results = {}
            
            # Her konuÅŸmacÄ± iÃ§in ayrÄ± analiz
            for turn, _, speaker in diarization_result.itertracks(yield_label=True):
                start_sample = int(turn.start * sample_rate)
                end_sample = int(turn.end * sample_rate)
                
                # KonuÅŸmacÄ± segmentini al
                if start_sample < len(audio_data) and end_sample <= len(audio_data):
                    speaker_audio = audio_data[start_sample:end_sample]
                    
                    if len(speaker_audio) > 1024:  # Yeterli veri varsa
                        # Bu konuÅŸmacÄ± iÃ§in analiz yap
                        speaker_analysis = self.feature_based_gender_age_analysis(speaker_audio, sample_rate)
                        
                        if speaker not in speaker_results:
                            speaker_results[speaker] = []
                        speaker_results[speaker].append(speaker_analysis)
            
            # Her konuÅŸmacÄ± iÃ§in ortalama sonuÃ§ hesapla
            final_speaker_results = {}
            for speaker, analyses in speaker_results.items():
                if analyses:
                    # Cinsiyet skorlarÄ±nÄ± ortala
                    avg_gender = {}
                    avg_age = {}
                    
                    for gender in ['erkek', 'kadÄ±n', 'belirsiz']:
                        scores = [analysis['gender'][gender] for analysis in analyses if 'gender' in analysis]
                        avg_gender[gender] = np.mean(scores) if scores else 0.0
                    
                    for age in ['Ã§ocuk', 'genÃ§', 'yetiÅŸkin', 'orta_yaÅŸ', 'yaÅŸlÄ±']:
                        scores = [analysis['age'][age] for analysis in analyses if 'age' in analysis]
                        avg_age[age] = np.mean(scores) if scores else 0.0
                    
                    final_speaker_results[speaker] = {
                        'gender': avg_gender,
                        'age': avg_age,
                        'confidence': np.mean([analysis.get('confidence', 0.5) for analysis in analyses])
                    }
            
            return final_speaker_results
            
        except Exception as e:
            self.add_log(f"âŒ KonuÅŸmacÄ± bazlÄ± analiz hatasÄ±: {e}")
            return {}
    
    def ensemble_gender_age_results(self, results):
        """FarklÄ± analiz yÃ¶ntemlerinin sonuÃ§larÄ±nÄ± birleÅŸtir"""
        try:
            # AÄŸÄ±rlÄ±klar
            weights = {
                'feature_based': 0.4,    # En gÃ¼venilir
                'frequency_based': 0.3,  # Ä°kinci gÃ¼venilir
                'deep_learning': 0.2,    # ÃœÃ§Ã¼ncÃ¼ gÃ¼venilir
                'speaker_based': 0.1     # Destekleyici
            }
            
            # Genel cinsiyet ve yaÅŸ skorlarÄ±
            ensemble_gender = {'erkek': 0.0, 'kadÄ±n': 0.0, 'belirsiz': 0.0}
            ensemble_age = {'Ã§ocuk': 0.0, 'genÃ§': 0.0, 'yetiÅŸkin': 0.0, 'orta_yaÅŸ': 0.0, 'yaÅŸlÄ±': 0.0}
            total_weight = 0.0
            
            # Her yÃ¶ntemin sonuÃ§larÄ±nÄ± aÄŸÄ±rlÄ±klÄ± olarak birleÅŸtir
            for method, result in results.items():
                if result and method in weights:
                    weight = weights[method]
                    
                    if isinstance(result, dict) and 'gender' in result:
                        # Tekil sonuÃ§
                        for gender in ensemble_gender:
                            if gender in result['gender']:
                                ensemble_gender[gender] += result['gender'][gender] * weight
                        
                        for age in ensemble_age:
                            if age in result['age']:
                                ensemble_age[age] += result['age'][age] * weight
                        
                        total_weight += weight
                    
                    elif isinstance(result, dict):
                        # KonuÅŸmacÄ± bazlÄ± sonuÃ§lar
                        speaker_count = len(result)
                        if speaker_count > 0:
                            speaker_weight = weight / speaker_count
                            
                            for speaker_result in result.values():
                                for gender in ensemble_gender:
                                    if gender in speaker_result['gender']:
                                        ensemble_gender[gender] += speaker_result['gender'][gender] * speaker_weight
                                
                                for age in ensemble_age:
                                    if age in speaker_result['age']:
                                        ensemble_age[age] += speaker_result['age'][age] * speaker_weight
                            
                            total_weight += weight
            
            # Normalize et
            if total_weight > 0:
                for gender in ensemble_gender:
                    ensemble_gender[gender] /= total_weight
                for age in ensemble_age:
                    ensemble_age[age] /= total_weight
            else:
                # VarsayÄ±lan deÄŸerler
                ensemble_gender = {'erkek': 0.5, 'kadÄ±n': 0.5, 'belirsiz': 0.0}
                ensemble_age = {'yetiÅŸkin': 0.6, 'genÃ§': 0.3, 'orta_yaÅŸ': 0.1, 'Ã§ocuk': 0.0, 'yaÅŸlÄ±': 0.0}
            
            # GÃ¼ven skoru hesapla
            confidence = min(total_weight, 1.0)
            
            return {
                'overall': {
                    'gender': ensemble_gender,
                    'age': ensemble_age,
                    'confidence': confidence
                },
                'detailed': results
            }
            
        except Exception as e:
            self.add_log(f"âŒ Ensemble birleÅŸtirme hatasÄ±: {e}")
            return self.get_default_gender_age_results()
    
    def calculate_gender_age_confidence(self, features, gender_scores, age_scores):
        """Cinsiyet ve yaÅŸ analizi gÃ¼ven skorunu hesapla"""
        try:
            # Ses kalitesi faktÃ¶rleri
            quality_factors = {
                'pitch_clarity': 1.0 if features['pitch_mean'] > 50 else 0.5,
                'energy_level': min(features['energy'] * 100, 1.0),
                'voice_activity': features['voice_activity_ratio'],
                'spectral_clarity': 1 - features['spectral_flatness_mean']
            }
            
            # Skor daÄŸÄ±lÄ±mÄ± analizi
            max_gender_score = max(gender_scores.values())
            max_age_score = max(age_scores.values())
            
            gender_confidence = max_gender_score
            age_confidence = max_age_score
            
            # Genel gÃ¼ven skoru
            quality_score = np.mean(list(quality_factors.values()))
            prediction_confidence = (gender_confidence + age_confidence) / 2
            
            overall_confidence = (quality_score * 0.6) + (prediction_confidence * 0.4)
            
            return min(overall_confidence, 1.0)
            
        except Exception as e:
            return 0.5  # Orta gÃ¼ven
    
    def display_gender_age_results(self, results):
        """Cinsiyet ve yaÅŸ analizi sonuÃ§larÄ±nÄ± gÃ¶ster"""
        try:
            self.gender_age_text.delete(1.0, tk.END)
            self.gender_age_text.insert(tk.END, f"ğŸ‘¥ Cinsiyet ve YaÅŸ Analizi SonuÃ§larÄ±\n")
            self.gender_age_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if 'overall' in results:
                overall = results['overall']
                
                # Genel sonuÃ§lar
                self.gender_age_text.insert(tk.END, f"ğŸ¯ GENEL SONUÃ‡LAR\n")
                self.gender_age_text.insert(tk.END, f"{'='*30}\n")
                self.gender_age_text.insert(tk.END, f"ğŸ¯ GÃ¼ven Skoru: {overall['confidence']*100:.1f}%\n\n")
                
                # Cinsiyet sonuÃ§larÄ±
                self.gender_age_text.insert(tk.END, f"ğŸ‘¤ CÄ°NSÄ°YET ANALÄ°ZÄ°:\n")
                gender_sorted = sorted(overall['gender'].items(), key=lambda x: x[1], reverse=True)
                
                for i, (gender, score) in enumerate(gender_sorted):
                    percentage = score * 100
                    bar_length = int(percentage / 2.5)
                    bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
                    
                    # Ä°kon ve renk
                    icons = {'erkek': 'ğŸ‘¨', 'kadÄ±n': 'ğŸ‘©', 'belirsiz': 'â“'}
                    icon = icons.get(gender, 'â“')
                    rank_icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
                    
                    self.gender_age_text.insert(tk.END, 
                        f"{rank_icon} {icon} {gender.capitalize()}: {percentage:.1f}% {bar}\n")
                
                # YaÅŸ sonuÃ§larÄ±
                self.gender_age_text.insert(tk.END, f"\nğŸ‚ YAÅ GRUBU ANALÄ°ZÄ°:\n")
                age_sorted = sorted(overall['age'].items(), key=lambda x: x[1], reverse=True)
                
                for i, (age, score) in enumerate(age_sorted):
                    percentage = score * 100
                    bar_length = int(percentage / 2.5)
                    bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
                    
                    # Ä°kon ve yaÅŸ aralÄ±ÄŸÄ±
                    age_info = {
                        'Ã§ocuk': ('ğŸ‘¶', '0-12 yaÅŸ'),
                        'genÃ§': ('ğŸ§’', '13-25 yaÅŸ'),
                        'yetiÅŸkin': ('ğŸ‘¤', '26-45 yaÅŸ'),
                        'orta_yaÅŸ': ('ğŸ§‘', '46-65 yaÅŸ'),
                        'yaÅŸlÄ±': ('ğŸ‘´', '65+ yaÅŸ')
                    }
                    icon, age_range = age_info.get(age, ('â“', 'Belirsiz'))
                    rank_icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
                    
                    self.gender_age_text.insert(tk.END, 
                        f"{rank_icon} {icon} {age.capitalize()} ({age_range}): {percentage:.1f}% {bar}\n")
                
                # Dominant tahminler
                dominant_gender = gender_sorted[0][0]
                dominant_age = age_sorted[0][0]
                
                self.gender_age_text.insert(tk.END, f"\nğŸ¯ SONUÃ‡ Ã–ZETÄ°:\n")
                self.gender_age_text.insert(tk.END, f"ğŸ‘¤ Tahmin Edilen Cinsiyet: {dominant_gender.capitalize()}\n")
                self.gender_age_text.insert(tk.END, f"ğŸ‚ Tahmin Edilen YaÅŸ Grubu: {dominant_age.capitalize()}\n")
                
                # GÃ¼ven seviyesi yorumu
                confidence = overall['confidence']
                if confidence > 0.8:
                    conf_text = "Ã‡ok YÃ¼ksek âœ¨"
                elif confidence > 0.6:
                    conf_text = "YÃ¼ksek âœ…"
                elif confidence > 0.4:
                    conf_text = "Orta âš ï¸"
                else:
                    conf_text = "DÃ¼ÅŸÃ¼k âŒ"
                    
                self.gender_age_text.insert(tk.END, f"ğŸ¯ Analiz GÃ¼venilirliÄŸi: {conf_text}\n")
                
                # KonuÅŸmacÄ± bazlÄ± detaylÄ± sonuÃ§lar
                if 'detailed' in results and 'speaker_based' in results['detailed']:
                    speaker_results = results['detailed']['speaker_based']
                    if speaker_results:
                        self.gender_age_text.insert(tk.END, f"\nğŸ‘¥ KONUÅMACI BAZLI DETAYLI ANALÄ°Z:\n")
                        self.gender_age_text.insert(tk.END, f"{'='*45}\n")
                        
                        for speaker, speaker_data in speaker_results.items():
                            self.gender_age_text.insert(tk.END, f"\nğŸ¤ {speaker}:\n")
                            self.gender_age_text.insert(tk.END, f"{'â”€'*25}\n")
                            
                            # Cinsiyet detaylarÄ±
                            self.gender_age_text.insert(tk.END, f"ğŸ‘¤ CÄ°NSÄ°YET SKORLARI:\n")
                            gender_sorted = sorted(speaker_data['gender'].items(), key=lambda x: x[1], reverse=True)
                            for i, (gender, score) in enumerate(gender_sorted):
                                percentage = score * 100
                                bar_length = int(percentage / 5)  # 5% per character
                                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                                
                                icons = {'erkek': 'ğŸ‘¨', 'kadÄ±n': 'ğŸ‘©', 'belirsiz': 'â“'}
                                icon = icons.get(gender, 'â“')
                                rank_icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
                                
                                self.gender_age_text.insert(tk.END, 
                                    f"  {rank_icon} {icon} {gender.capitalize()}: {percentage:.1f}% {bar}\n")
                            
                            # YaÅŸ detaylarÄ±
                            self.gender_age_text.insert(tk.END, f"\nğŸ‚ YAÅ GRUBU SKORLARI:\n")
                            age_sorted = sorted(speaker_data['age'].items(), key=lambda x: x[1], reverse=True)
                            for i, (age, score) in enumerate(age_sorted):
                                percentage = score * 100
                                bar_length = int(percentage / 5)  # 5% per character
                                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                                
                                age_info = {
                                    'Ã§ocuk': ('ğŸ‘¶', '0-12 yaÅŸ'),
                                    'genÃ§': ('ğŸ§’', '13-25 yaÅŸ'),
                                    'yetiÅŸkin': ('ğŸ‘¤', '26-45 yaÅŸ'),
                                    'orta_yaÅŸ': ('ğŸ§‘', '46-65 yaÅŸ'),
                                    'yaÅŸlÄ±': ('ğŸ‘´', '65+ yaÅŸ')
                                }
                                icon, age_range = age_info.get(age, ('â“', 'Belirsiz'))
                                rank_icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
                                
                                self.gender_age_text.insert(tk.END, 
                                    f"  {rank_icon} {icon} {age.capitalize()} ({age_range}): {percentage:.1f}% {bar}\n")
                            
                            # Bu konuÅŸmacÄ±nÄ±n sonuÃ§ Ã¶zeti
                            dominant_gender = gender_sorted[0][0]
                            dominant_age = age_sorted[0][0]
                            confidence = speaker_data.get('confidence', 0.5)
                            
                            self.gender_age_text.insert(tk.END, f"\nğŸ¯ {speaker} SONUÃ‡ Ã–ZETÄ°:\n")
                            self.gender_age_text.insert(tk.END, f"  ğŸ‘¤ Tahmin: {dominant_gender.capitalize()}\n")
                            self.gender_age_text.insert(tk.END, f"  ğŸ‚ YaÅŸ Grubu: {dominant_age.capitalize()}\n")
                            self.gender_age_text.insert(tk.END, f"  ğŸ¯ GÃ¼ven: {confidence*100:.1f}%\n")
                
                # DiÄŸer analiz yÃ¶ntemlerinin sonuÃ§larÄ±
                if 'detailed' in results:
                    self.gender_age_text.insert(tk.END, f"\nğŸ“Š DÄ°ÄER ANALÄ°Z YÃ–NTEMLERÄ°:\n")
                    self.gender_age_text.insert(tk.END, f"{'='*35}\n")
                    
                    for method, result in results['detailed'].items():
                        if result and method != 'speaker_based':
                            self.gender_age_text.insert(tk.END, f"\nğŸ”¬ {method.replace('_', ' ').title()}:\n")
                            
                            if isinstance(result, dict) and 'gender' in result:
                                # Tekil sonuÃ§
                                dominant_g = max(result['gender'].items(), key=lambda x: x[1])
                                dominant_a = max(result['age'].items(), key=lambda x: x[1])
                                
                                self.gender_age_text.insert(tk.END, 
                                    f"  ğŸ‘¤ Cinsiyet: {dominant_g[0]} ({dominant_g[1]*100:.1f}%)\n")
                                self.gender_age_text.insert(tk.END, 
                                    f"  ğŸ‚ YaÅŸ: {dominant_a[0]} ({dominant_a[1]*100:.1f}%)\n")
            
            # GÃ¶rselleÅŸtirmeleri gÃ¼ncelle
            self.plot_gender_age_analysis()
            
        except Exception as e:
            self.add_log(f"âŒ Cinsiyet-yaÅŸ sonuÃ§ gÃ¶rÃ¼ntÃ¼leme hatasÄ±: {e}")
    
    def get_default_gender_age_results(self):
        """VarsayÄ±lan cinsiyet ve yaÅŸ sonuÃ§larÄ±"""
        return {
            'gender': {'erkek': 0.5, 'kadÄ±n': 0.5, 'belirsiz': 0.0},
            'age': {'yetiÅŸkin': 0.6, 'genÃ§': 0.3, 'orta_yaÅŸ': 0.1, 'Ã§ocuk': 0.0, 'yaÅŸlÄ±': 0.0},
            'confidence': 0.3
        }
    
    def plot_gender_age_analysis(self):
        """Cinsiyet ve yaÅŸ analizi gÃ¶rselleÅŸtirmesi"""
        try:
            if not self.gender_age_result or 'overall' not in self.gender_age_result:
                return
                
            overall = self.gender_age_result['overall']
            
            # Cinsiyet pie chart
            self.ax_gender.clear()
            gender_data = overall['gender']
            gender_labels = list(gender_data.keys())
            gender_values = list(gender_data.values())
            gender_colors = [GENDER_COLORS.get(gender, '#95A5A6') for gender in gender_labels]
            
            wedges, texts, autotexts = self.ax_gender.pie(gender_values, labels=gender_labels, 
                                                         colors=gender_colors, autopct='%1.1f%%',
                                                         startangle=90)
            self.ax_gender.set_title('ğŸ‘¤ Cinsiyet DaÄŸÄ±lÄ±mÄ±')
            
            # YaÅŸ pie chart
            self.ax_age.clear()
            age_data = overall['age']
            age_labels = list(age_data.keys())
            age_values = list(age_data.values())
            age_colors = [AGE_COLORS.get(age, '#95A5A6') for age in age_labels]
            
            wedges, texts, autotexts = self.ax_age.pie(age_values, labels=age_labels, 
                                                      colors=age_colors, autopct='%1.1f%%',
                                                      startangle=90)
            self.ax_age.set_title('ğŸ‚ YaÅŸ Grubu DaÄŸÄ±lÄ±mÄ±')
            
            # KonuÅŸmacÄ± bazlÄ± cinsiyet daÄŸÄ±lÄ±mÄ±
            self.ax_speaker_gender.clear()
            if 'detailed' in self.gender_age_result and 'speaker_based' in self.gender_age_result['detailed']:
                speaker_results = self.gender_age_result['detailed']['speaker_based']
                if speaker_results:
                    speakers = list(speaker_results.keys())
                    male_scores = [speaker_results[s]['gender']['erkek'] * 100 for s in speakers]
                    female_scores = [speaker_results[s]['gender']['kadÄ±n'] * 100 for s in speakers]
                    
                    x = np.arange(len(speakers))
                    width = 0.35
                    
                    self.ax_speaker_gender.bar(x - width/2, male_scores, width, label='Erkek', color=GENDER_COLORS['erkek'])
                    self.ax_speaker_gender.bar(x + width/2, female_scores, width, label='KadÄ±n', color=GENDER_COLORS['kadÄ±n'])
                    
                    self.ax_speaker_gender.set_xlabel('KonuÅŸmacÄ±lar')
                    self.ax_speaker_gender.set_ylabel('Skor (%)')
                    self.ax_speaker_gender.set_title('ğŸ‘¥ KonuÅŸmacÄ± Cinsiyet SkorlarÄ±')
                    self.ax_speaker_gender.set_xticks(x)
                    self.ax_speaker_gender.set_xticklabels(speakers)
                    self.ax_speaker_gender.legend()
                else:
                    self.ax_speaker_gender.text(0.5, 0.5, 'KonuÅŸmacÄ± verisi yok', 
                                              ha='center', va='center', transform=self.ax_speaker_gender.transAxes)
            else:
                self.ax_speaker_gender.text(0.5, 0.5, 'KonuÅŸmacÄ± analizi yapÄ±lmadÄ±', 
                                          ha='center', va='center', transform=self.ax_speaker_gender.transAxes)
            
            # GÃ¼ven skoru gÃ¶stergesi
            self.ax_speaker_age.clear()
            confidence = overall['confidence']
            
            # GÃ¼ven skoru gauge benzeri gÃ¶rselleÅŸtirme
            angles = np.linspace(0, np.pi, 100)
            values = np.ones_like(angles) * confidence
            
            self.ax_speaker_age.plot(angles, values, linewidth=10, color='green' if confidence > 0.7 else 'orange' if confidence > 0.4 else 'red')
            self.ax_speaker_age.fill_between(angles, 0, values, alpha=0.3, color='green' if confidence > 0.7 else 'orange' if confidence > 0.4 else 'red')
            self.ax_speaker_age.set_ylim(0, 1)
            self.ax_speaker_age.set_xlim(0, np.pi)
            self.ax_speaker_age.set_title(f'ğŸ¯ GÃ¼ven Skoru: {confidence*100:.1f}%')
            self.ax_speaker_age.text(np.pi/2, confidence/2, f'{confidence*100:.1f}%', 
                                   ha='center', va='center', fontsize=14, weight='bold')
            
            self.fig5.tight_layout()
            self.canvas5.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Cinsiyet-yaÅŸ gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def run_vad_analysis(self, audio_file):
        """Ses Aktivitesi Tespiti (VAD) yap"""
        try:
            vad_result = self.pipelines['vad'](audio_file)
            
            self.analysis_text.delete(1.0, tk.END)
            self.analysis_text.insert(tk.END, f"ğŸ¯ Ses Aktivitesi Tespiti SonuÃ§larÄ±\n")
            self.analysis_text.insert(tk.END, f"ğŸ“‚ Dosya: {os.path.basename(audio_file)}\n\n")
            
            total_speech = 0
            speech_segments = []
            
            for segment in vad_result.get_timeline():
                start = segment.start
                end = segment.end
                duration = end - start
                total_speech += duration
                speech_segments.append({'start': start, 'end': end, 'duration': duration})
                
                self.analysis_text.insert(tk.END, f"ğŸ—£ï¸ KonuÅŸma: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n")
            
            # Ä°statistikler
            audio_duration = librosa.get_duration(filename=audio_file)
            speech_ratio = (total_speech / audio_duration) * 100 if audio_duration > 0 else 0
            silence_duration = audio_duration - total_speech
            
            self.analysis_text.insert(tk.END, f"\nğŸ“Š VAD Ä°statistikleri:\n")
            self.analysis_text.insert(tk.END, f"ğŸ“ˆ Toplam ses sÃ¼resi: {audio_duration:.2f}s\n")
            self.analysis_text.insert(tk.END, f"ğŸ—£ï¸ KonuÅŸma sÃ¼resi: {total_speech:.2f}s ({speech_ratio:.1f}%)\n")
            self.analysis_text.insert(tk.END, f"ğŸ¤« Sessizlik sÃ¼resi: {silence_duration:.2f}s ({100-speech_ratio:.1f}%)\n")
            self.analysis_text.insert(tk.END, f"ğŸ“ KonuÅŸma segment sayÄ±sÄ±: {len(speech_segments)}\n")
            
            if speech_segments:
                avg_segment = total_speech / len(speech_segments)
                self.analysis_text.insert(tk.END, f"â±ï¸ Ortalama segment sÃ¼resi: {avg_segment:.2f}s\n")
            
            self.add_log("âœ… VAD analizi tamamlandÄ±")
            return vad_result
            
        except Exception as e:
            self.add_log(f"âŒ VAD analizi hatasÄ±: {e}")
            return None
    
    def run_overlap_detection(self, audio_file):
        """Ã–rtÃ¼ÅŸen konuÅŸma tespiti yap"""
        try:
            overlap_result = self.pipelines['overlap'](audio_file)
            
            self.analysis_text.insert(tk.END, f"\nğŸ—£ï¸ Ã–rtÃ¼ÅŸen KonuÅŸma Tespiti SonuÃ§larÄ±\n")
            self.analysis_text.insert(tk.END, f"ğŸ“‚ Dosya: {os.path.basename(audio_file)}\n\n")
            
            total_overlap = 0
            overlap_segments = []
            
            for segment in overlap_result.get_timeline():
                start = segment.start
                end = segment.end
                duration = end - start
                total_overlap += duration
                overlap_segments.append({'start': start, 'end': end, 'duration': duration})
                
                self.analysis_text.insert(tk.END, f"ğŸ”„ Ã–rtÃ¼ÅŸme: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n")
            
            # Ä°statistikler
            audio_duration = librosa.get_duration(filename=audio_file)
            overlap_ratio = (total_overlap / audio_duration) * 100 if audio_duration > 0 else 0
            
            self.analysis_text.insert(tk.END, f"\nğŸ“Š Ã–rtÃ¼ÅŸme Ä°statistikleri:\n")
            self.analysis_text.insert(tk.END, f"ğŸ”„ Toplam Ã¶rtÃ¼ÅŸme sÃ¼resi: {total_overlap:.2f}s ({overlap_ratio:.1f}%)\n")
            self.analysis_text.insert(tk.END, f"ğŸ“ Ã–rtÃ¼ÅŸme segment sayÄ±sÄ±: {len(overlap_segments)}\n")
            
            if overlap_segments:
                avg_overlap = total_overlap / len(overlap_segments)
                self.analysis_text.insert(tk.END, f"â±ï¸ Ortalama Ã¶rtÃ¼ÅŸme sÃ¼resi: {avg_overlap:.2f}s\n")
            
            self.add_log("âœ… Ã–rtÃ¼ÅŸme tespiti tamamlandÄ±")
            return overlap_result
            
        except Exception as e:
            self.add_log(f"âŒ Ã–rtÃ¼ÅŸme tespiti hatasÄ±: {e}")
            return None
    
    def extract_speaker_embeddings(self, audio_file, diarization):
        """KonuÅŸmacÄ± embedding'lerini Ã§Ä±kar"""
        try:
            if not diarization:
                return
                
            self.add_log("ğŸ” KonuÅŸmacÄ± embedding'leri Ã§Ä±karÄ±lÄ±yor...")
            
            # Ses dosyasÄ±nÄ± yÃ¼kle
            audio_data, sample_rate = librosa.load(audio_file, sr=16000)  # 16kHz
            
            # Her konuÅŸmacÄ± iÃ§in embedding Ã§Ä±kar
            embeddings = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                start_sample = int(turn.start * sample_rate)
                end_sample = int(turn.end * sample_rate)
                
                # Segment ses verisini al
                segment_audio = audio_data[start_sample:end_sample]
                
                # Basit Ã¶zellik Ã§Ä±karma (gerÃ§ek projede daha geliÅŸmiÅŸ yÃ¶ntemler kullanÄ±lÄ±r)
                features = {
                    'mfcc_mean': np.mean(librosa.feature.mfcc(y=segment_audio, sr=sample_rate)),
                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=segment_audio, sr=sample_rate)),
                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(segment_audio)),
                    'duration': turn.end - turn.start
                }
                
                if speaker not in embeddings:
                    embeddings[speaker] = []
                embeddings[speaker].append(features)
            
            # Ortalama embedding'leri hesapla
            for speaker in embeddings:
                speaker_features = embeddings[speaker]
                avg_features = {
                    'mfcc_mean': np.mean([f['mfcc_mean'] for f in speaker_features]),
                    'spectral_centroid': np.mean([f['spectral_centroid'] for f in speaker_features]),
                    'zero_crossing_rate': np.mean([f['zero_crossing_rate'] for f in speaker_features]),
                    'total_duration': sum([f['duration'] for f in speaker_features])
                }
                self.speaker_embeddings[speaker] = avg_features
            
            self.add_log("âœ… KonuÅŸmacÄ± embedding'leri Ã§Ä±karÄ±ldÄ±")
            
        except Exception as e:
            self.add_log(f"âŒ Embedding Ã§Ä±karma hatasÄ±: {e}")
    
    def run_speech_separation(self, audio_file):
        """Ses ayrÄ±ÅŸtÄ±rma iÅŸlemi yap"""
        try:
            self.add_log("ğŸ¼ Ses ayrÄ±ÅŸtÄ±rma baÅŸlatÄ±lÄ±yor...")
            # Bu Ã¶zellik iÃ§in geliÅŸmiÅŸ modeller gerekir
            # Åu an iÃ§in basit bir placeholder
            self.add_log("âš ï¸ Ses ayrÄ±ÅŸtÄ±rma Ã¶zelliÄŸi geliÅŸtirme aÅŸamasÄ±nda")
            
        except Exception as e:
            self.add_log(f"âŒ Ses ayrÄ±ÅŸtÄ±rma hatasÄ±: {e}")
    
    def extract_advanced_audio_features(self, audio_data, sample_rate):
        """GeliÅŸmiÅŸ ses Ã¶zelliklerini Ã§Ä±kar"""
        try:
            self.add_log("ğŸ” GeliÅŸmiÅŸ ses Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor...")
            
            # Ses verisini temizle ve kontrol et
            audio_data = self.clean_audio_buffer(audio_data)
            
            # Ses verisi boÅŸ veya Ã§ok kÄ±sa mÄ± kontrol et
            if len(audio_data) < 1024:  # Minimum 1024 sample
                self.add_log("âŒ Ses verisi Ã§ok kÄ±sa, varsayÄ±lan Ã¶zellikler dÃ¶ndÃ¼rÃ¼lÃ¼yor")
                return self.get_default_features()
            
            # Temel Ã¶zellikler
            mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=20)
            spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
            zero_crossing_rates = librosa.feature.zero_crossing_rate(audio_data)
            
            # GeliÅŸmiÅŸ spektral Ã¶zellikler
            chroma = librosa.feature.chroma(y=audio_data, sr=sample_rate)
            mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)
            tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sample_rate)
            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sample_rate)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sample_rate)
            spectral_flatness = librosa.feature.spectral_flatness(y=audio_data)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)
            
            # Prozodik Ã¶zellikler
            try:
                tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sample_rate)
                speaking_rate = len(beats) / (len(audio_data) / sample_rate)  # konuÅŸma hÄ±zÄ±
            except:
                tempo = 0
                speaking_rate = 0
            
            # Pitch analizi
            pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sample_rate)
            pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0
            pitch_std = np.std(pitches[pitches > 0]) if np.any(pitches > 0) else 0
            
            # Enerji ve gÃ¼Ã§ Ã¶zellikleri
            energy = np.mean(np.square(audio_data))
            rms_energy = np.mean(librosa.feature.rms(y=audio_data))
            
            # Formant analizi (basitleÅŸtirilmiÅŸ)
            # GerÃ§ek formant analizi iÃ§in daha karmaÅŸÄ±k algoritma gerekir
            stft = librosa.stft(audio_data)
            spectral_magnitude = np.abs(stft)
            formant_frequencies = []
            for frame in spectral_magnitude.T:
                peaks, _ = librosa.util.peak_pick(frame, pre_max=3, post_max=3, pre_avg=3, post_avg=5, delta=0.1, wait=10)
                if len(peaks) >= 2:
                    # Ä°lk iki formant yaklaÅŸÄ±mÄ±
                    formant_frequencies.append(peaks[:2])
            
            # Sessizlik analizi
            silence_threshold = 0.01
            silence_frames = np.where(np.abs(audio_data) < silence_threshold)[0]
            silence_ratio = len(silence_frames) / len(audio_data)
            
            # Vurgu ve tonlama (prosody) Ã¶zellikleri
            # KÄ±sa-dÃ¶nem enerji deÄŸiÅŸimleri
            frame_length = int(0.025 * sample_rate)  # 25ms
            hop_length = int(0.01 * sample_rate)     # 10ms
            
            short_time_energy = []
            for i in range(0, len(audio_data) - frame_length, hop_length):
                frame = audio_data[i:i + frame_length]
                energy_frame = np.sum(frame ** 2)
                short_time_energy.append(energy_frame)
            
            energy_variance = np.var(short_time_energy)
            energy_mean = np.mean(short_time_energy)
            
            # Ã–zellik sÃ¶zlÃ¼ÄŸÃ¼ - GÃ¼venli hesaplama
            features = {}
            
            # GÃ¼venli Ã¶zellik hesaplama fonksiyonu
            def safe_calc(func, default_val=0.0):
                try:
                    result = func()
                    if np.isfinite(result):
                        return float(result)
                    else:
                        return default_val
                except:
                    return default_val
            
            # Temel Ã¶zellikler
            features['mfcc_mean'] = safe_calc(lambda: np.mean(mfccs), 0.0)
            features['mfcc_std'] = safe_calc(lambda: np.std(mfccs), 1.0)
            features['spectral_centroid_mean'] = safe_calc(lambda: np.mean(spectral_centroids), 1000.0)
            features['spectral_centroid_std'] = safe_calc(lambda: np.std(spectral_centroids), 500.0)
            features['zcr_mean'] = safe_calc(lambda: np.mean(zero_crossing_rates), 0.05)
            features['zcr_std'] = safe_calc(lambda: np.std(zero_crossing_rates), 0.02)
            
            # GeliÅŸmiÅŸ spektral Ã¶zellikler
            features['chroma_mean'] = safe_calc(lambda: np.mean(chroma), 0.1)
            features['chroma_std'] = safe_calc(lambda: np.std(chroma), 0.05)
            features['mel_spectrogram_mean'] = safe_calc(lambda: np.mean(mel_spectrogram), 0.01)
            features['tonnetz_mean'] = safe_calc(lambda: np.mean(tonnetz), 0.0)
            features['spectral_contrast_mean'] = safe_calc(lambda: np.mean(spectral_contrast), 10.0)
            features['spectral_bandwidth_mean'] = safe_calc(lambda: np.mean(spectral_bandwidth), 1500.0)
            features['spectral_flatness_mean'] = safe_calc(lambda: np.mean(spectral_flatness), 0.1)
            features['spectral_rolloff_mean'] = safe_calc(lambda: np.mean(spectral_rolloff), 2000.0)
            
            # Prozodik Ã¶zellikler
            features['tempo'] = safe_calc(lambda: tempo, 100.0)
            features['speaking_rate'] = safe_calc(lambda: speaking_rate, 2.0)
            features['pitch_mean'] = safe_calc(lambda: pitch_mean, 200.0)
            features['pitch_std'] = safe_calc(lambda: pitch_std, 50.0)
            features['pitch_range'] = safe_calc(lambda: pitch_std / pitch_mean if pitch_mean > 0 else 0, 0.25)
            
            # Enerji Ã¶zellikleri
            features['energy'] = safe_calc(lambda: energy, 0.001)
            features['rms_energy'] = safe_calc(lambda: rms_energy, 0.01)
            features['energy_variance'] = safe_calc(lambda: energy_variance, 0.0001)
            features['energy_mean'] = safe_calc(lambda: energy_mean, 0.001)
            features['energy_dynamic_range'] = safe_calc(
                lambda: np.max(short_time_energy) - np.min(short_time_energy) if len(short_time_energy) > 0 else 0, 
                0.005
            )
            
            # Sessizlik ve duraklama
            features['silence_ratio'] = safe_calc(lambda: silence_ratio, 0.3)
            features['voice_activity_ratio'] = safe_calc(lambda: 1 - silence_ratio, 0.7)
            
            # Harmonik Ã¶zellikler
            features['harmonic_mean'] = safe_calc(lambda: np.mean(librosa.effects.harmonic(audio_data)), 0.001)
            features['percussive_mean'] = safe_calc(lambda: np.mean(librosa.effects.percussive(audio_data)), 0.001)
            
            self.add_log(f"âœ… {len(features)} geliÅŸmiÅŸ Ã¶zellik Ã§Ä±karÄ±ldÄ±")
            return features
            
        except Exception as e:
            self.add_log(f"âŒ GeliÅŸmiÅŸ Ã¶zellik Ã§Ä±karma hatasÄ±: {e}")
            return {}
    
    def run_emotion_analysis(self, audio_data, sample_rate):
        """Duygu analizi yap"""
        try:
            self.add_log("ğŸ˜Š Duygu analizi baÅŸlatÄ±lÄ±yor...")
            
            # Ses verisini temizle
            audio_data = self.clean_audio_buffer(audio_data)
            
            # Basit ses Ã¶zellik analizi ile duygu tahmini
            # GerÃ§ek projede daha geliÅŸmiÅŸ modeller kullanÄ±lÄ±r
            
            # Ses Ã¶zelliklerini gÃ¼venli ÅŸekilde Ã§Ä±kar
            try:
                mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)
                spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
                zero_crossing_rates = librosa.feature.zero_crossing_rate(audio_data)
                spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)
            except Exception as e:
                self.add_log(f"âš ï¸ Temel Ã¶zellik Ã§Ä±karma hatasÄ±: {e}")
                return self.get_default_emotion_scores()
            
            # Basit kural tabanlÄ± duygu tahmini - GÃ¼venli hesaplama
            try:
                energy = np.mean(np.square(audio_data))
                if not np.isfinite(energy):
                    energy = 0.001
            except:
                energy = 0.001
                
            try:
                pitch_mean = np.mean(spectral_centroids)
                if not np.isfinite(pitch_mean):
                    pitch_mean = 1000.0
            except:
                pitch_mean = 1000.0
                
            try:
                zcr_mean = np.mean(zero_crossing_rates)
                if not np.isfinite(zcr_mean):
                    zcr_mean = 0.05
            except:
                zcr_mean = 0.05
            
            # Duygu sÄ±nÄ±flandÄ±rmasÄ± (basitleÅŸtirilmiÅŸ)
            emotion_scores = {
                'mutlu': 0.0,
                'Ã¼zgÃ¼n': 0.0,
                'kÄ±zgÄ±n': 0.0,
                'sakin': 0.0,
                'heyecanlÄ±': 0.0,
                'stresli': 0.0
            }
            
            # Basit kurallar
            if energy > 0.01 and pitch_mean > 2000:
                emotion_scores['heyecanlÄ±'] += 0.3
                emotion_scores['mutlu'] += 0.2
            elif energy < 0.005:
                emotion_scores['sakin'] += 0.3
                emotion_scores['Ã¼zgÃ¼n'] += 0.2
            
            if zcr_mean > 0.1:
                emotion_scores['stresli'] += 0.2
                emotion_scores['kÄ±zgÄ±n'] += 0.1
            
            # Normalize et
            total_score = sum(emotion_scores.values())
            if total_score > 0:
                for emotion in emotion_scores:
                    emotion_scores[emotion] = emotion_scores[emotion] / total_score
            else:
                # VarsayÄ±lan deÄŸerler
                emotion_scores['sakin'] = 0.6
                emotion_scores['mutlu'] = 0.4
            
            # SonuÃ§larÄ± gÃ¶ster
            self.emotion_text.delete(1.0, tk.END)
            self.emotion_text.insert(tk.END, f"ğŸ˜Š Duygu Analizi SonuÃ§larÄ±\n")
            self.emotion_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # DuygularÄ± sÄ±ralÄ± gÃ¶ster
            sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)
            
            for emotion, score in sorted_emotions:
                percentage = score * 100
                bar_length = int(percentage / 5)  # 5% per character
                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                
                self.emotion_text.insert(tk.END, f"{emotion.capitalize()}: {percentage:.1f}% {bar}\n")
            
            # Dominant duygu
            dominant_emotion = sorted_emotions[0][0]
            self.emotion_text.insert(tk.END, f"\nğŸ¯ BaskÄ±n Duygu: {dominant_emotion.capitalize()}\n")
            
            # Ses Ã¶zellikleri
            self.emotion_text.insert(tk.END, f"\nğŸ“Š Ses Ã–zellikleri:\n")
            self.emotion_text.insert(tk.END, f"âš¡ Enerji: {energy:.4f}\n")
            self.emotion_text.insert(tk.END, f"ğŸµ Ortalama Perde: {pitch_mean:.1f} Hz\n")
            self.emotion_text.insert(tk.END, f"ğŸŒŠ Zero Crossing Rate: {zcr_mean:.3f}\n")
            
            self.add_log("âœ… Duygu analizi tamamlandÄ±")
            return emotion_scores
            
        except Exception as e:
            self.add_log(f"âŒ Duygu analizi hatasÄ±: {e}")
            return None
    
    def run_speaker_based_transcription(self, audio_file, diarization):
        """KonuÅŸmacÄ± bazlÄ± transkripsiyon yap"""
        try:
            self.add_log("KonuÅŸmacÄ± bazlÄ± transkripsiyon baÅŸlatÄ±lÄ±yor...")
            
            # SonuÃ§larÄ± gÃ¶ster
            self.transcript_text.delete(1.0, tk.END)
            self.transcript_text.insert(tk.END, f"KonuÅŸma Ä°Ã§eriÄŸi: {audio_file}\n")
            self.transcript_text.insert(tk.END, f"Model: {GPT_MODEL}\n\n")
            
            # Ã–nce tÃ¼m ses dosyasÄ±nÄ± transkript et
            self.add_log("TÃ¼m ses dosyasÄ± transkript ediliyor...")
            full_transcript = self.transcribe_audio_segment(audio_file)
            
            if not full_transcript:
                self.add_log("Transkripsiyon baÅŸarÄ±sÄ±z oldu. LÃ¼tfen API anahtarÄ±nÄ±zÄ± kontrol edin.")
                self.status_label.config(text="Transkripsiyon hatasÄ±!")
                self.analyze_button.config(state=tk.NORMAL)
                return
            
            self.add_log("Tam transkript alÄ±ndÄ±, konuÅŸmacÄ±lara gÃ¶re bÃ¶lÃ¼nÃ¼yor...")
            
            # Diyarizasyon sonuÃ§larÄ±nÄ± gÃ¶ster
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"Diyarizasyon SonuÃ§larÄ±: {audio_file}\n\n")
            
            # KonuÅŸmacÄ±larÄ± ve zaman aralÄ±klarÄ±nÄ± gÃ¶ster
            speaker_segments = []
            if diarization:
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    start = turn.start
                    end = turn.end
                    duration = end - start
                    result_line = f"{speaker}: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n"
                    self.result_text.insert(tk.END, result_line)
                    
                    # Segment bilgilerini kaydet
                    speaker_segments.append({
                        "speaker": speaker,
                        "start": start,
                        "end": end
                    })
            
            # KonuÅŸmacÄ± istatistiklerini hesapla
            speaker_stats = {}
            if diarization:
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    if speaker not in speaker_stats:
                        speaker_stats[speaker] = 0
                    speaker_stats[speaker] += turn.end - turn.start
            
            # Ä°statistikleri gÃ¶ster
            self.result_text.insert(tk.END, "\nKonuÅŸmacÄ± Ä°statistikleri:\n")
            if speaker_stats:
                for speaker, duration in speaker_stats.items():
                    self.result_text.insert(tk.END, f"{speaker}: {duration:.2f} saniye\n")
            
            # KonuÅŸmacÄ± bazlÄ± transkriptleri oluÅŸtur
            # Tam transkripti zaman aralÄ±klarÄ±na gÃ¶re bÃ¶l
            
            # Segmentleri zaman sÄ±rasÄ±na gÃ¶re sÄ±rala
            speaker_segments.sort(key=lambda x: x["start"])
            
            # Her segment iÃ§in transkript oluÅŸtur
            for i, segment in enumerate(speaker_segments):
                speaker = segment["speaker"]
                start = segment["start"]
                end = segment["end"]
                
                # Tam transkriptten bu konuÅŸmacÄ±nÄ±n konuÅŸma iÃ§eriÄŸini tahmin et
                # Burada basit bir yaklaÅŸÄ±m kullanÄ±yoruz
                # GerÃ§ek uygulamada daha geliÅŸmiÅŸ bir metin bÃ¶lme algoritmasÄ± gerekebilir
                
                # Her segment iÃ§in ayrÄ± transkript yap
                self.add_log(f"{speaker} iÃ§in segment transkript ediliyor: {start:.2f}s - {end:.2f}s")
                
                # Segment ses dosyasÄ±nÄ± oluÅŸtur
                import soundfile as sf
                import numpy as np
                
                # Ses dosyasÄ±nÄ± yÃ¼kle
                audio, sample_rate = sf.read(audio_file)
                
                # Segment sÄ±nÄ±rlarÄ±nÄ± hesapla
                start_sample = int(start * sample_rate)
                end_sample = int(end * sample_rate)
                
                # Segment sÄ±nÄ±rlarÄ±nÄ± kontrol et
                if start_sample >= len(audio) or end_sample > len(audio):
                    continue
                    
                # KonuÅŸmacÄ± segmentini kes
                segment_audio = audio[start_sample:end_sample]
                
                # GeÃ§ici dosya oluÅŸtur
                temp_dir = "temp_segments"
                os.makedirs(temp_dir, exist_ok=True)
                segment_file = os.path.join(temp_dir, f"segment_{i}.wav")
                sf.write(segment_file, segment_audio, sample_rate)
                
                # Bu segmenti transkript et
                segment_text = self.transcribe_audio_segment(segment_file)
                
                if segment_text and len(segment_text.strip()) > 0:
                    # Transkript sonucunu gÃ¶ster
                    self.transcript_text.insert(tk.END, f"[{start:.2f}s - {end:.2f}s] {speaker}: {segment_text}\n\n")
                else:
                    self.add_log(f"{speaker} iÃ§in segment transkripsiyon boÅŸ dÃ¶ndÃ¼")
            
            self.add_log("KonuÅŸmacÄ± bazlÄ± transkripsiyon tamamlandÄ±.")
            self.status_label.config(text="Analiz tamamlandÄ±")
            self.analyze_button.config(state=tk.NORMAL)
            
        except Exception as e:
            self.add_log(f"KonuÅŸmacÄ± bazlÄ± transkripsiyon hatasÄ±: {e}")
            self.status_label.config(text="Hata!")
            self.analyze_button.config(state=tk.NORMAL)
    
    def transcribe_audio_segment(self, audio_file):
        """Tek bir ses segmentini transkript et"""
        try:
            # gpt-4o-mini iÃ§in Whisper API kullan
            whisper_response = openai.audio.transcriptions.create(
                model="whisper-1",
                file=open(audio_file, "rb"),
                language=GPT_LANGUAGE
            )
            
            return whisper_response.text
            
        except Exception as e:
            self.add_log(f"Segment transkripsiyon hatasÄ±: {e}")
            return ""
    
    def add_log(self, message):
        """Log alanÄ±na mesaj ekle"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
        self.log_text.see(tk.END)
    
    def update_advanced_visualizations(self, audio_data, sample_rate):
        """GeliÅŸmiÅŸ gÃ¶rselleÅŸtirmeleri gÃ¼ncelle"""
        try:
            self.add_log("ğŸ“Š GÃ¶rselleÅŸtirmeler gÃ¼ncelleniyor...")
            
            # Ana dalga formu ve diyarizasyon gÃ¶rselleÅŸtirmesi
            self.update_comprehensive_plots(audio_data)
            
            # Diyarizasyon sonuÃ§larÄ±nÄ± Ã§iz
            if self.diarization_result:
                self.plot_diarization_timeline(audio_data, sample_rate)
            
            # Duygu analizi gÃ¶rselleÅŸtirmesi
            if self.emotion_result:
                if hasattr(self, 'temporal_emotion_history') and self.temporal_emotion_history:
                    self.plot_temporal_emotion_analysis()  # Zamansal gÃ¶rselleÅŸtirme
                else:
                    self.plot_emotion_analysis()  # Standart gÃ¶rselleÅŸtirme
            
            # Ä°statistik grafikleri
            self.plot_statistics()
            
            self.add_log("âœ… GÃ¶rselleÅŸtirmeler gÃ¼ncellendi")
            
        except Exception as e:
            self.add_log(f"âŒ GÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def plot_diarization_timeline(self, audio_data, sample_rate):
        """Diyarizasyon zaman Ã§izelgesi Ã§iz"""
        try:
            self.ax_diarization.clear()
            
            if not self.diarization_result:
                return
                
            # KonuÅŸmacÄ±larÄ± renk kodlarÄ± ile eÅŸleÅŸtir
            speakers = list(set(speaker for _, _, speaker in self.diarization_result.itertracks(yield_label=True)))
            speaker_colors = {speaker: SPEAKER_COLORS[i % len(SPEAKER_COLORS)] for i, speaker in enumerate(speakers)}
            
            # Y ekseni iÃ§in konuÅŸmacÄ± pozisyonlarÄ±
            speaker_positions = {speaker: i for i, speaker in enumerate(speakers)}
            
            # Her konuÅŸmacÄ± segmentini Ã§iz
            for turn, _, speaker in self.diarization_result.itertracks(yield_label=True):
                start = turn.start
                end = turn.end
                y_pos = speaker_positions[speaker]
                
                # Segment Ã§ubunu Ã§iz
                self.ax_diarization.barh(y_pos, end - start, left=start, height=0.8, 
                                        color=speaker_colors[speaker], alpha=0.7, 
                                        edgecolor='black', linewidth=0.5)
                
                # Segment Ã¼zerine sÃ¼re yaz (eÄŸer yeterince uzunsa)
                if end - start > 1.0:  # 1 saniyeden uzun segmentler iÃ§in
                    self.ax_diarization.text(start + (end - start) / 2, y_pos, 
                                           f'{end - start:.1f}s', 
                                           ha='center', va='center', fontsize=8, weight='bold')
            
            # VAD sonuÃ§larÄ±nÄ± ekle (varsa)
            if self.vad_result:
                audio_duration = len(audio_data) / sample_rate
                vad_y = len(speakers)  # En Ã¼ste VAD Ã§ubuÄŸu
                
                # Sessizlik bÃ¶lgeleri (gri)
                self.ax_diarization.barh(vad_y, audio_duration, left=0, height=0.3, 
                                        color='lightgray', alpha=0.5, label='Sessizlik')
                
                # KonuÅŸma bÃ¶lgeleri (yeÅŸil)
                for segment in self.vad_result.get_timeline():
                    self.ax_diarization.barh(vad_y, segment.end - segment.start, 
                                           left=segment.start, height=0.3, 
                                           color='lightgreen', alpha=0.7)
            
            # Ã–rtÃ¼ÅŸme bÃ¶lgelerini ekle (varsa)
            if self.overlap_result:
                for segment in self.overlap_result.get_timeline():
                    # TÃ¼m konuÅŸmacÄ±lar boyunca kÄ±rmÄ±zÄ± Ã§izgi
                    self.ax_diarization.axvspan(segment.start, segment.end, 
                                              alpha=0.3, color='red', 
                                              label='Ã–rtÃ¼ÅŸme' if segment == list(self.overlap_result.get_timeline())[0] else "")
            
            # Grafik ayarlarÄ±
            self.ax_diarization.set_xlabel('Zaman (saniye)')
            self.ax_diarization.set_ylabel('KonuÅŸmacÄ±lar')
            self.ax_diarization.set_title('ğŸ‘¥ KonuÅŸmacÄ± Zaman Ã‡izelgesi')
            
            # Y ekseni etiketleri
            all_labels = speakers[:]
            if self.vad_result:
                all_labels.append('VAD')
            
            self.ax_diarization.set_yticks(range(len(all_labels)))
            self.ax_diarization.set_yticklabels(all_labels)
            
            # Legend ekle
            if self.overlap_result and len(list(self.overlap_result.get_timeline())) > 0:
                self.ax_diarization.legend(loc='upper right')
            
            self.ax_diarization.grid(True, alpha=0.3)
            self.fig.tight_layout()
            self.canvas.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Diyarizasyon gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def plot_emotion_analysis(self):
        """Duygu analizi gÃ¶rselleÅŸtirmesi"""
        try:
            if not self.emotion_result:
                return
                
            self.ax_emotion.clear()
            
            # DuygularÄ± ve skorlarÄ± al
            emotions = list(self.emotion_result.keys())
            scores = list(self.emotion_result.values())
            percentages = [score * 100 for score in scores]
            
            # Renkleri eÅŸleÅŸtir
            colors = [EMOTION_COLORS.get(emotion, '#95A5A6') for emotion in emotions]
            
            # Pasta grafiÄŸi
            wedges, texts, autotexts = self.ax_emotion.pie(percentages, labels=emotions, 
                                                          colors=colors, autopct='%1.1f%%',
                                                          startangle=90, textprops={'fontsize': 10})
            
            # GrafiÄŸi gÃ¼zelleÅŸtir
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_weight('bold')
            
            self.ax_emotion.set_title('ğŸ˜Š Duygu DaÄŸÄ±lÄ±mÄ±', fontsize=14, weight='bold')
            
            # Dominant duyguyu vurgula
            max_emotion_idx = scores.index(max(scores))
            wedges[max_emotion_idx].set_edgecolor('black')
            wedges[max_emotion_idx].set_linewidth(3)
            
            self.canvas3.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Duygu gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def plot_statistics(self):
        """Ä°statistik grafikleri Ã§iz"""
        try:
            # KonuÅŸmacÄ± sÃ¼re daÄŸÄ±lÄ±mÄ±
            if self.diarization_result:
                speakers = {}
                for turn, _, speaker in self.diarization_result.itertracks(yield_label=True):
                    if speaker not in speakers:
                        speakers[speaker] = 0
                    speakers[speaker] += turn.end - turn.start
                
                # KonuÅŸmacÄ± sÃ¼releri bar grafiÄŸi
                self.ax_stats1.clear()
                speaker_names = list(speakers.keys())
                durations = list(speakers.values())
                colors = [SPEAKER_COLORS[i % len(SPEAKER_COLORS)] for i in range(len(speaker_names))]
                
                bars = self.ax_stats1.bar(speaker_names, durations, color=colors, alpha=0.7)
                self.ax_stats1.set_title('KonuÅŸmacÄ± SÃ¼releri')
                self.ax_stats1.set_ylabel('SÃ¼re (saniye)')
                self.ax_stats1.tick_params(axis='x', rotation=45)
                
                # DeÄŸerleri bar Ã¼zerine yaz
                for bar, duration in zip(bars, durations):
                    self.ax_stats1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                                       f'{duration:.1f}s', ha='center', va='bottom', fontsize=9)
            
            # KonuÅŸmacÄ± embedding Ã¶zellikleri
            if self.speaker_embeddings:
                self.ax_stats2.clear()
                
                # MFCC ortalama deÄŸerleri
                speakers = list(self.speaker_embeddings.keys())
                mfcc_values = [self.speaker_embeddings[s]['mfcc_mean'] for s in speakers]
                
                colors = [SPEAKER_COLORS[i % len(SPEAKER_COLORS)] for i in range(len(speakers))]
                self.ax_stats2.scatter(range(len(speakers)), mfcc_values, c=colors, s=100, alpha=0.7)
                self.ax_stats2.set_title('KonuÅŸmacÄ± MFCC OrtalamasÄ±')
                self.ax_stats2.set_xlabel('KonuÅŸmacÄ±lar')
                self.ax_stats2.set_ylabel('MFCC Ortalama')
                self.ax_stats2.set_xticks(range(len(speakers)))
                self.ax_stats2.set_xticklabels(speakers, rotation=45)
                self.ax_stats2.grid(True, alpha=0.3)
            
            # Ses aktivitesi istatistikleri
            if self.vad_result:
                self.ax_stats3.clear()
                
                speech_segments = list(self.vad_result.get_timeline())
                if speech_segments:
                    # Segment sÃ¼re daÄŸÄ±lÄ±mÄ± histogramÄ±
                    segment_durations = [seg.end - seg.start for seg in speech_segments]
                    
                    self.ax_stats3.hist(segment_durations, bins=min(20, len(segment_durations)), 
                                       color='lightgreen', alpha=0.7, edgecolor='black')
                    self.ax_stats3.set_title('KonuÅŸma Segment SÃ¼re DaÄŸÄ±lÄ±mÄ±')
                    self.ax_stats3.set_xlabel('Segment SÃ¼resi (saniye)')
                    self.ax_stats3.set_ylabel('Frekans')
                    self.ax_stats3.grid(True, alpha=0.3)
            
            # Duygu skorlarÄ± radar chart (basitleÅŸtirilmiÅŸ)
            if self.emotion_result:
                self.ax_stats4.clear()
                
                emotions = list(self.emotion_result.keys())
                scores = [self.emotion_result[e] * 100 for e in emotions]
                
                # Basit bar chart
                colors = [EMOTION_COLORS.get(emotion, '#95A5A6') for emotion in emotions]
                bars = self.ax_stats4.bar(emotions, scores, color=colors, alpha=0.7)
                self.ax_stats4.set_title('Duygu SkorlarÄ±')
                self.ax_stats4.set_ylabel('Skor (%)')
                self.ax_stats4.tick_params(axis='x', rotation=45)
                self.ax_stats4.set_ylim(0, 100)
                
                # DeÄŸerleri bar Ã¼zerine yaz
                for bar, score in zip(bars, scores):
                    self.ax_stats4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                                       f'{score:.1f}%', ha='center', va='bottom', fontsize=8)
            
            self.fig4.tight_layout()
            self.canvas4.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Ä°statistik gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def generate_detailed_analysis_report(self):
        """DetaylÄ± analiz raporu oluÅŸtur"""
        try:
            self.analysis_text.insert(tk.END, f"\n\nğŸ“‹ DETAYLI ANALÄ°Z RAPORU\n")
            self.analysis_text.insert(tk.END, f"{'='*50}\n")
            self.analysis_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            self.analysis_text.insert(tk.END, f"ğŸ“‚ Dosya: {os.path.basename(self.current_audio_file) if self.current_audio_file else 'KayÄ±t'}\n\n")
            
            # Genel istatistikler
            if self.current_audio_file:
                audio_duration = librosa.get_duration(filename=self.current_audio_file)
                self.analysis_text.insert(tk.END, f"â±ï¸ Toplam sÃ¼re: {audio_duration:.2f} saniye\n")
            
            # Diyarizasyon Ã¶zeti
            if self.diarization_result:
                speakers = list(set(speaker for _, _, speaker in self.diarization_result.itertracks(yield_label=True)))
                self.analysis_text.insert(tk.END, f"ğŸ‘¥ Tespit edilen konuÅŸmacÄ± sayÄ±sÄ±: {len(speakers)}\n")
                
                total_speech = sum(turn.end - turn.start for turn, _, _ in self.diarization_result.itertracks())
                self.analysis_text.insert(tk.END, f"ğŸ—£ï¸ Toplam konuÅŸma sÃ¼resi: {total_speech:.2f} saniye\n")
            
            # VAD Ã¶zeti
            if self.vad_result:
                speech_segments = list(self.vad_result.get_timeline())
                self.analysis_text.insert(tk.END, f"ğŸ¯ KonuÅŸma segment sayÄ±sÄ±: {len(speech_segments)}\n")
            
            # Ã–rtÃ¼ÅŸme Ã¶zeti
            if self.overlap_result:
                overlap_segments = list(self.overlap_result.get_timeline())
                total_overlap = sum(seg.end - seg.start for seg in overlap_segments)
                self.analysis_text.insert(tk.END, f"ğŸ”„ Toplam Ã¶rtÃ¼ÅŸme sÃ¼resi: {total_overlap:.2f} saniye\n")
            
            # Duygu Ã¶zeti
            if self.emotion_result:
                dominant_emotion = max(self.emotion_result.items(), key=lambda x: x[1])
                self.analysis_text.insert(tk.END, f"ğŸ˜Š BaskÄ±n duygu: {dominant_emotion[0].capitalize()} ({dominant_emotion[1]*100:.1f}%)\n")
            
            self.analysis_text.insert(tk.END, f"\n{'='*50}\n")
            
        except Exception as e:
            self.add_log(f"âŒ Rapor oluÅŸturma hatasÄ±: {e}")
    
    def start_live_analysis(self):
        """CanlÄ± analiz baÅŸlat"""
        try:
            if self.live_analysis_running:
                return
                
            self.live_analysis_running = True
            self.add_log("ğŸ”´ CanlÄ± analiz baÅŸlatÄ±ldÄ±")
            
            # CanlÄ± analiz iÃ§in ayrÄ± thread baÅŸlat
            self.live_analysis_thread = threading.Thread(target=self.live_analysis_worker)
            self.live_analysis_thread.daemon = True
            self.live_analysis_thread.start()
            
        except Exception as e:
            self.add_log(f"âŒ CanlÄ± analiz baÅŸlatma hatasÄ±: {e}")
    
    def stop_live_analysis(self):
        """CanlÄ± analizi durdur"""
        try:
            self.live_analysis_running = False
            self.add_log("â¹ï¸ CanlÄ± analiz durduruldu")
            
        except Exception as e:
            self.add_log(f"âŒ CanlÄ± analiz durdurma hatasÄ±: {e}")
    
    def live_analysis_worker(self):
        """CanlÄ± analiz worker fonksiyonu"""
        try:
            while self.live_analysis_running and self.is_recording:
                # Basit canlÄ± analiz - ses seviyesi gÃ¶sterimi
                time.sleep(0.1)  # 100ms gÃ¼ncelleme
                
                # GerÃ§ek projede burada gerÃ§ek zamanlÄ± analiz yapÄ±lÄ±r
                # Åu an iÃ§in sadece durum gÃ¶sterimi
                
                if not self.is_recording:
                    break
                    
        except Exception as e:
            self.add_log(f"âŒ CanlÄ± analiz worker hatasÄ±: {e}")
        finally:
            self.live_analysis_running = False
    
    def generate_report(self):
        """PDF raporu oluÅŸtur"""
        try:
            self.add_log("ğŸ“„ Rapor oluÅŸturuluyor...")
            
            # Basit metin raporu oluÅŸtur
            report_content = []
            report_content.append("ğŸ¤ GELIÅMIÅ SES ANALÄ°ZÄ° RAPORU")
            report_content.append("=" * 50)
            report_content.append(f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_content.append(f"ğŸ“‚ Dosya: {os.path.basename(self.current_audio_file) if self.current_audio_file else 'KayÄ±t'}")
            report_content.append("")
            
            # Diyarizasyon sonuÃ§larÄ±
            if self.diarization_result:
                report_content.append("ğŸ‘¥ DÄ°YARÄ°ZASYON SONUÃ‡LARI:")
                report_content.append("-" * 30)
                
                speakers = {}
                for turn, _, speaker in self.diarization_result.itertracks(yield_label=True):
                    if speaker not in speakers:
                        speakers[speaker] = []
                    speakers[speaker].append(f"{turn.start:.2f}s - {turn.end:.2f}s ({turn.end - turn.start:.2f}s)")
                
                for speaker, segments in speakers.items():
                    report_content.append(f"\n{speaker}:")
                    for segment in segments:
                        report_content.append(f"  â€¢ {segment}")
            
            # Duygu analizi sonuÃ§larÄ±
            if self.emotion_result:
                report_content.append("\n\nğŸ˜Š DUYGU ANALÄ°ZÄ° SONUÃ‡LARI:")
                report_content.append("-" * 30)
                sorted_emotions = sorted(self.emotion_result.items(), key=lambda x: x[1], reverse=True)
                for emotion, score in sorted_emotions:
                    report_content.append(f"{emotion.capitalize()}: {score*100:.1f}%")
            
            # Cinsiyet ve yaÅŸ analizi sonuÃ§larÄ±
            if hasattr(self, 'gender_age_result') and self.gender_age_result:
                report_content.append("\n\nğŸ‘¥ CÄ°NSÄ°YET VE YAÅ ANALÄ°ZÄ° SONUÃ‡LARI:")
                report_content.append("-" * 40)
                
                if 'overall' in self.gender_age_result:
                    overall = self.gender_age_result['overall']
                    
                    # Genel sonuÃ§lar
                    report_content.append("\nğŸ¯ GENEL SONUÃ‡LAR:")
                    dominant_gender = max(overall['gender'].items(), key=lambda x: x[1])
                    dominant_age = max(overall['age'].items(), key=lambda x: x[1])
                    confidence = overall.get('confidence', 0.5)
                    
                    report_content.append(f"Cinsiyet: {dominant_gender[0].capitalize()} ({dominant_gender[1]*100:.1f}%)")
                    report_content.append(f"YaÅŸ Grubu: {dominant_age[0].capitalize()} ({dominant_age[1]*100:.1f}%)")
                    report_content.append(f"GÃ¼ven Skoru: {confidence*100:.1f}%")
                    
                    # TÃ¼m cinsiyet skorlarÄ±
                    report_content.append("\nCinsiyet SkorlarÄ±:")
                    gender_sorted = sorted(overall['gender'].items(), key=lambda x: x[1], reverse=True)
                    for gender, score in gender_sorted:
                        report_content.append(f"  {gender.capitalize()}: {score*100:.1f}%")
                    
                    # TÃ¼m yaÅŸ skorlarÄ±
                    report_content.append("\nYaÅŸ Grubu SkorlarÄ±:")
                    age_sorted = sorted(overall['age'].items(), key=lambda x: x[1], reverse=True)
                    for age, score in age_sorted:
                        report_content.append(f"  {age.capitalize()}: {score*100:.1f}%")
                
                # KonuÅŸmacÄ± bazlÄ± sonuÃ§lar
                if 'detailed' in self.gender_age_result and 'speaker_based' in self.gender_age_result['detailed']:
                    speaker_results = self.gender_age_result['detailed']['speaker_based']
                    if speaker_results:
                        report_content.append("\n\nğŸ¤ KONUÅMACI BAZLI SONUÃ‡LAR:")
                        report_content.append("-" * 30)
                        
                        for speaker, speaker_data in speaker_results.items():
                            report_content.append(f"\n{speaker}:")
                            
                            # Cinsiyet sonuÃ§larÄ±
                            dominant_gender = max(speaker_data['gender'].items(), key=lambda x: x[1])
                            report_content.append(f"  Cinsiyet: {dominant_gender[0].capitalize()} ({dominant_gender[1]*100:.1f}%)")
                            
                            # YaÅŸ sonuÃ§larÄ±
                            dominant_age = max(speaker_data['age'].items(), key=lambda x: x[1])
                            report_content.append(f"  YaÅŸ Grubu: {dominant_age[0].capitalize()} ({dominant_age[1]*100:.1f}%)")
                            
                            # GÃ¼ven skoru
                            sp_confidence = speaker_data.get('confidence', 0.5)
                            report_content.append(f"  GÃ¼ven Skoru: {sp_confidence*100:.1f}%")
                            
                            # DetaylÄ± skorlar
                            report_content.append("  Cinsiyet DetaylarÄ±:")
                            for gender, score in sorted(speaker_data['gender'].items(), key=lambda x: x[1], reverse=True):
                                report_content.append(f"    {gender.capitalize()}: {score*100:.1f}%")
                            
                            report_content.append("  YaÅŸ DetaylarÄ±:")
                            for age, score in sorted(speaker_data['age'].items(), key=lambda x: x[1], reverse=True):
                                report_content.append(f"    {age.capitalize()}: {score*100:.1f}%")
            
            # Raporu dosyaya kaydet
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"ses_analizi_raporu_{timestamp}.txt"
            
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_content))
            
            self.add_log(f"âœ… Rapor oluÅŸturuldu: {report_filename}")
            messagebox.showinfo("Rapor", f"Rapor baÅŸarÄ±yla oluÅŸturuldu:\n{report_filename}")
            
        except Exception as e:
            self.add_log(f"âŒ Rapor oluÅŸturma hatasÄ±: {e}")
            messagebox.showerror("Hata", f"Rapor oluÅŸturulurken hata oluÅŸtu:\n{e}")
    
    def run_ml_emotion_analysis(self, audio_data, sample_rate):
        """Machine Learning tabanlÄ± duygu analizi"""
        try:
            self.add_log("ğŸ¤– ML tabanlÄ± duygu analizi baÅŸlatÄ±lÄ±yor...")
            
            # GeliÅŸmiÅŸ Ã¶zellik Ã§Ä±karÄ±mÄ±
            features = self.extract_advanced_audio_features(audio_data, sample_rate)
            
            if not features:
                return self.run_emotion_analysis(audio_data, sample_rate)  # Fallback
            
            # Ã–zellik vektÃ¶rÃ¼nÃ¼ hazÄ±rla
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # NaN deÄŸerleri temizle
            feature_vector = np.nan_to_num(feature_vector)
            
            # Ã–zellik normalizasyonu
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            feature_vector_normalized = scaler.fit_transform(feature_vector)
            
            # GeliÅŸmiÅŸ kural tabanlÄ± sistem (ML benzeri)
            emotion_scores = self.advanced_rule_based_classification(features)
            
            # EÄŸer gerÃ§ek ML modeli varsa kullan
            try:
                emotion_scores = self.use_pretrained_emotion_model(feature_vector_normalized)
            except:
                self.add_log("âš ï¸ Pretrained model bulunamadÄ±, geliÅŸmiÅŸ kurallar kullanÄ±lÄ±yor")
            
            # Temporal analiz (zaman serisi)
            temporal_scores = self.temporal_emotion_analysis(audio_data, sample_rate)
            
            # Zamansal veriyi sakla (gÃ¶rselleÅŸtirme iÃ§in)
            if hasattr(self, '_temp_temporal_data'):
                self.temporal_emotion_history = self._temp_temporal_data
            
            # SkorlarÄ± birleÅŸtir (ensemble)
            final_scores = self.ensemble_emotion_scores(emotion_scores, temporal_scores)
            
            # GÃ¼ven skoru hesapla
            confidence = self.calculate_confidence_score(features, final_scores)
            
            # Debug bilgilerini gÃ¶ster
            debug_info = self.debug_emotion_analysis(features)
            self.add_log("ğŸ” Debug bilgileri:")
            for line in debug_info.split('\n'):
                if line.strip():
                    self.add_log(line)
            
            # SonuÃ§larÄ± gÃ¶ster
            self.display_advanced_emotion_results(final_scores, features, confidence)
            
            self.add_log("âœ… ML tabanlÄ± duygu analizi tamamlandÄ±")
            return final_scores
            
        except Exception as e:
            self.add_log(f"âŒ ML duygu analizi hatasÄ±: {e}")
            # Fallback to basic analysis
            return self.run_emotion_analysis(audio_data, sample_rate)
    
    def advanced_rule_based_classification(self, features):
        """GeliÅŸmiÅŸ kural tabanlÄ± duygu sÄ±nÄ±flandÄ±rmasÄ±"""
        emotion_scores = {
            'mutlu': 0.0, 'Ã¼zgÃ¼n': 0.0, 'kÄ±zgÄ±n': 0.0,
            'sakin': 0.0, 'heyecanlÄ±': 0.0, 'stresli': 0.0,
            'ÅŸaÅŸkÄ±n': 0.0, 'korku': 0.0  # Yeni duygular
        }
        
        # GeliÅŸmiÅŸ kurallar - DÃœÅÃœK EÅÄ°KLER ile gÃ¼ncellendi
        
        # 1. MUTLULUK ve GÃœLME TESPÄ°TÄ° (Ã‡ok geliÅŸtirildi!)
        mutlu_score = 0.0
        
        # Ana mutluluk gÃ¶stergeleri
        if features['pitch_mean'] > 150:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 200 -> 150
            mutlu_score += 0.3
        if features['energy'] > 0.0005:  # Ã‡OK dÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.01 -> 0.0005
            mutlu_score += 0.3
        if features['pitch_range'] > 0.05:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.1 -> 0.05
            mutlu_score += 0.2
        if features['zcr_mean'] > 0.05:  # ZCR kahkaha iÃ§in Ã¶nemli
            mutlu_score += 0.2
        if features['spectral_bandwidth_mean'] > 1000:  # GeniÅŸ frekans = gÃ¼lme
            mutlu_score += 0.2
        if features['energy_dynamic_range'] > 0.001:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.01 -> 0.001
            mutlu_score += 0.2
            
        # KAHKAHA Ã¶zel tespiti
        if (features['pitch_mean'] > 200 and features['zcr_mean'] > 0.08 and 
            features['energy'] > 0.001):  # Kahkaha kombinasyonu
            mutlu_score += 0.5
            
        emotion_scores['mutlu'] = min(mutlu_score, 1.0)
        
        # 2. HEYECAN TESPÄ°TÄ° (GeliÅŸtirildi)
        heyecan_score = 0.0
        if features['energy'] > 0.001:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.015 -> 0.001
            heyecan_score += 0.3
        if features['pitch_std'] > 30:  # Perde deÄŸiÅŸkenliÄŸi
            heyecan_score += 0.2
        if features['zcr_mean'] > 0.07:
            heyecan_score += 0.2
        if features['energy_variance'] > 0.0001:
            heyecan_score += 0.2
        if features['voice_activity_ratio'] > 0.6:  # Aktif konuÅŸma
            heyecan_score += 0.1
            
        emotion_scores['heyecanlÄ±'] = min(heyecan_score, 1.0)
            
        # 3. ÃœZÃœNTÃœ tespiti (Daha spesifik)
        if (features['pitch_mean'] < 120 and features['energy'] < 0.0003 and  # Ã‡ok dÃ¼ÅŸÃ¼k
            features['speaking_rate'] < 2 and features['silence_ratio'] > 0.4):
            emotion_scores['Ã¼zgÃ¼n'] += 0.4
            
        # 4. Ã–FKE tespiti (GeliÅŸtirildi)
        if (features['zcr_mean'] > 0.12 and features['energy'] > 0.005 and  # Daha yÃ¼ksek eÅŸik
            features['spectral_bandwidth_mean'] > 2500 and features['energy_variance'] > 0.002):
            emotion_scores['kÄ±zgÄ±n'] += 0.5
            
        # 5. SAKÄ°NLÄ°K tespiti (Ã‡ok daha spesifik)
        sakin_score = 0.0
        if features['energy_variance'] < 0.00005:  # Ã‡ok dÃ¼ÅŸÃ¼k varyans
            sakin_score += 0.2
        if features['pitch_std'] < 20:  # Ã‡ok stabil perde
            sakin_score += 0.2
        if features['zcr_mean'] < 0.03:  # Ã‡ok dÃ¼ÅŸÃ¼k ZCR
            sakin_score += 0.2
        if features['silence_ratio'] > 0.5:  # Ã‡ok sessizlik
            sakin_score += 0.3
        if features['energy'] < 0.0002:  # Ã‡ok dÃ¼ÅŸÃ¼k enerji
            sakin_score += 0.3
            
        emotion_scores['sakin'] = min(sakin_score, 1.0)
            
        # 6. STRES tespiti
        if (features['zcr_std'] > 0.03 and features['energy_variance'] > 0.0003 and
            features['pitch_std'] > 60 and features['spectral_flatness_mean'] > 0.08):
            emotion_scores['stresli'] += 0.4
            
        # 7. ÅAÅKINLIK tespiti  
        if (features['pitch_range'] > 0.3 and features['energy_dynamic_range'] > 0.005 and
            features['speaking_rate'] < 3):
            emotion_scores['ÅŸaÅŸkÄ±n'] += 0.3
            
        # 8. KORKU tespiti
        if (features['zcr_mean'] > 0.10 and features['pitch_mean'] > 180 and
            features['energy_variance'] > 0.0005 and features['voice_activity_ratio'] < 0.5):
            emotion_scores['korku'] += 0.3
        
        # Ã–ZEL KAHKAHA TESPÄ°TÄ° - En Ã¼st Ã¶ncelik
        if self.detect_laughter_patterns(features):
            emotion_scores['mutlu'] += 0.6  # GÃ¼Ã§lÃ¼ kahkaha bonusu
            emotion_scores['heyecanlÄ±'] += 0.4
            emotion_scores['sakin'] *= 0.1  # SakinliÄŸi bastÄ±r
            emotion_scores['Ã¼zgÃ¼n'] *= 0.1   # ÃœzÃ¼ntÃ¼yÃ¼ bastÄ±r
        
        # Normalize skorlarÄ±
        total_score = sum(emotion_scores.values())
        if total_score > 0:
            for emotion in emotion_scores:
                emotion_scores[emotion] = emotion_scores[emotion] / total_score
        else:
            # Default deÄŸerler (kahkaha bulunamadÄ±ysa)
            emotion_scores['mutlu'] = 0.4
            emotion_scores['heyecanlÄ±'] = 0.3
            emotion_scores['sakin'] = 0.3
            
        return emotion_scores
    
    def detect_laughter_patterns(self, features):
        """Kahkaha desenlerini tespit et"""
        try:
            laughter_indicators = 0
            
            # 1. YÃ¼ksek frekans ama orta enerji (tipik kahkaha)
            if 1000 < features['pitch_mean'] < 4000:
                laughter_indicators += 1
                
            # 2. YÃ¼ksek ZCR (gÃ¼rÃ¼ltÃ¼lÃ¼, titrek ses)
            if features['zcr_mean'] > 0.08:
                laughter_indicators += 1
                
            # 3. GeniÅŸ spektral bant (ha-ha-ha sesi)
            if features['spectral_bandwidth_mean'] > 1500:
                laughter_indicators += 1
                
            # 4. Ortalama enerji (Ã§ok yÃ¼ksek deÄŸil ama var)
            if 0.0005 < features['energy'] < 0.01:
                laughter_indicators += 1
                
            # 5. DeÄŸiÅŸken enerji (patlamalar)
            if features['energy_variance'] > 0.0002:
                laughter_indicators += 1
                
            # 6. PerkÃ¼sif Ã¶zellikler (ha-ha ritmi)
            if features['percussive_mean'] > 0.001:
                laughter_indicators += 1
                
            # 4 veya daha fazla gÃ¶sterge = muhtemelen kahkaha
            return laughter_indicators >= 4
            
        except Exception as e:
            return False
    
    def temporal_emotion_analysis(self, audio_data, sample_rate, window_size=3.0):
        """Zamansal duygu analizi - ses boyunca duygu deÄŸiÅŸimi"""
        try:
            self.add_log("â° Zamansal duygu analizi yapÄ±lÄ±yor...")
            
            window_samples = int(window_size * sample_rate)
            hop_samples = window_samples // 2
            
            temporal_emotions = []
            self._temp_temporal_data = []  # GeÃ§ici veri saklama
            
            for start in range(0, len(audio_data) - window_samples, hop_samples):
                end = start + window_samples
                segment = audio_data[start:end]
                
                # Segment Ã¶zelliklerini Ã§Ä±kar
                segment_features = self.extract_advanced_audio_features(segment, sample_rate)
                
                if segment_features:
                    # Segment iÃ§in duygu analizi
                    segment_emotions = self.advanced_rule_based_classification(segment_features)
                    temporal_data = {
                        'time': start / sample_rate,
                        'emotions': segment_emotions
                    }
                    temporal_emotions.append(temporal_data)
                    self._temp_temporal_data.append(temporal_data)  # GÃ¶rselleÅŸtirme iÃ§in sakla
            
            # Zamansal ortalamalarÄ± hesapla
            if temporal_emotions:
                avg_emotions = {}
                for emotion in temporal_emotions[0]['emotions'].keys():
                    scores = [te['emotions'][emotion] for te in temporal_emotions]
                    avg_emotions[emotion] = np.mean(scores)
                
                return avg_emotions
            else:
                return {}
                
        except Exception as e:
            self.add_log(f"âŒ Zamansal analiz hatasÄ±: {e}")
            return {}
    
    def ensemble_emotion_scores(self, rule_scores, temporal_scores, weights=[0.6, 0.4]):
        """FarklÄ± analiz yÃ¶ntemlerinin skorlarÄ±nÄ± birleÅŸtir"""
        if not temporal_scores:
            return rule_scores
            
        ensemble_scores = {}
        
        for emotion in rule_scores.keys():
            if emotion in temporal_scores:
                ensemble_scores[emotion] = (
                    weights[0] * rule_scores[emotion] + 
                    weights[1] * temporal_scores[emotion]
                )
            else:
                ensemble_scores[emotion] = rule_scores[emotion]
        
        # Normalize
        total = sum(ensemble_scores.values())
        if total > 0:
            for emotion in ensemble_scores:
                ensemble_scores[emotion] /= total
                
        return ensemble_scores
    
    def calculate_confidence_score(self, features, emotion_scores):
        """Analiz gÃ¼ven skorunu hesapla"""
        try:
            # Ses kalitesi faktÃ¶rleri
            quality_factors = {
                'energy_level': min(features['energy'] * 100, 1.0),
                'voice_activity': features['voice_activity_ratio'],
                'signal_clarity': 1 - features['spectral_flatness_mean'],
                'pitch_stability': 1 / (1 + features['pitch_std'] / max(features['pitch_mean'], 1))
            }
            
            # Duygu skorlarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±
            max_emotion_score = max(emotion_scores.values())
            second_max = sorted(emotion_scores.values())[-2] if len(emotion_scores) > 1 else 0
            score_separation = max_emotion_score - second_max
            
            # Genel gÃ¼ven skoru
            quality_score = np.mean(list(quality_factors.values()))
            confidence = (quality_score * 0.7) + (score_separation * 0.3)
            
            return min(confidence, 1.0)
            
        except Exception as e:
            return 0.5  # Orta gÃ¼ven
    
    def display_advanced_emotion_results(self, emotion_scores, features, confidence):
        """GeliÅŸmiÅŸ duygu analizi sonuÃ§larÄ±nÄ± gÃ¶ster"""
        self.emotion_text.delete(1.0, tk.END)
        self.emotion_text.insert(tk.END, f"ğŸ¤– GeliÅŸmiÅŸ Duygu Analizi SonuÃ§larÄ±\n")
        self.emotion_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.emotion_text.insert(tk.END, f"ğŸ¯ GÃ¼ven Skoru: {confidence*100:.1f}%\n\n")
        
        # DuygularÄ± sÄ±ralÄ± gÃ¶ster
        sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)
        
        for i, (emotion, score) in enumerate(sorted_emotions):
            percentage = score * 100
            bar_length = int(percentage / 2.5)  # Daha hassas Ã§ubuk
            bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
            
            # Ä°kon ekle
            icons = {'mutlu': 'ğŸ˜Š', 'Ã¼zgÃ¼n': 'ğŸ˜¢', 'kÄ±zgÄ±n': 'ğŸ˜ ', 
                    'sakin': 'ğŸ˜Œ', 'heyecanlÄ±': 'ğŸ¤©', 'stresli': 'ğŸ˜°',
                    'ÅŸaÅŸkÄ±n': 'ğŸ˜®', 'korku': 'ğŸ˜¨'}
            icon = icons.get(emotion, 'ğŸ˜')
            
            rank_icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            
            self.emotion_text.insert(tk.END, 
                f"{rank_icon} {icon} {emotion.capitalize()}: {percentage:.1f}% {bar}\n")
        
        # Dominant duygu
        dominant_emotion, dominant_score = sorted_emotions[0]
        self.emotion_text.insert(tk.END, f"\nğŸ¯ BaskÄ±n Duygu: {dominant_emotion.capitalize()} ({dominant_score*100:.1f}%)\n")
        
        # DetaylÄ± ses Ã¶zellikleri
        self.emotion_text.insert(tk.END, f"\nğŸ“Š DetaylÄ± Ses Ã–zellikleri:\n")
        self.emotion_text.insert(tk.END, f"âš¡ Enerji Seviyesi: {features['energy']:.4f}\n")
        self.emotion_text.insert(tk.END, f"ğŸµ Ortalama Perde: {features['pitch_mean']:.1f} Hz\n")
        self.emotion_text.insert(tk.END, f"ğŸ“ˆ Perde DeÄŸiÅŸkenliÄŸi: {features['pitch_std']:.1f} Hz\n")
        self.emotion_text.insert(tk.END, f"ğŸ—£ï¸ KonuÅŸma HÄ±zÄ±: {features['speaking_rate']:.1f}\n")
        self.emotion_text.insert(tk.END, f"ğŸ¤« Sessizlik OranÄ±: {features['silence_ratio']*100:.1f}%\n")
        self.emotion_text.insert(tk.END, f"ğŸŒŠ ZCR Ortalama: {features['zcr_mean']:.3f}\n")
        self.emotion_text.insert(tk.END, f"ğŸ¼ Spektral Merkez: {features['spectral_centroid_mean']:.1f} Hz\n")
        self.emotion_text.insert(tk.END, f"ğŸ›ï¸ Enerji VaryansÄ±: {features['energy_variance']:.6f}\n")
        
        # GÃ¼ven seviyesi yorumu
        if confidence > 0.8:
            conf_text = "Ã‡ok YÃ¼ksek âœ¨"
        elif confidence > 0.6:
            conf_text = "YÃ¼ksek âœ…"
        elif confidence > 0.4:
            conf_text = "Orta âš ï¸"
        else:
            conf_text = "DÃ¼ÅŸÃ¼k âŒ"
            
        self.emotion_text.insert(tk.END, f"\nğŸ¯ Analiz GÃ¼venilirliÄŸi: {conf_text}\n")
    
    def use_pretrained_emotion_model(self, feature_vector):
        """Ã–nceden eÄŸitilmiÅŸ duygu modeli kullan"""
        try:
            # Hugging Face Transformers ile ses duygu tanÄ±ma
            from transformers import pipeline
            
            # Ã‡oklu model yaklaÅŸÄ±mÄ± - daha doÄŸru sonuÃ§lar iÃ§in
            models_to_try = [
                "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
                "facebook/wav2vec2-large-xlsr-53-turkish",  # TÃ¼rkÃ§e desteÄŸi
                "microsoft/unispeech-sat-base-plus"
            ]
            
            emotion_results = []
            
            for model_name in models_to_try:
                try:
                    self.add_log(f"ğŸ¤– Model deneniyor: {model_name}")
                    emotion_classifier = pipeline(
                        "audio-classification",
                        model=model_name,
                        device=0 if torch.cuda.is_available() else -1
                    )
                    
                    # GerÃ§ek ses dosyasÄ± ile analiz
                    if hasattr(self, 'current_audio_file') and self.current_audio_file:
                        result = emotion_classifier(self.current_audio_file)
                        emotion_results.append(result)
                        self.add_log(f"âœ… Model baÅŸarÄ±lÄ±: {model_name}")
                        break
                    
                except Exception as model_error:
                    self.add_log(f"âŒ Model hatasÄ± {model_name}: {model_error}")
                    continue
            
            if emotion_results:
                # SonuÃ§larÄ± normalize et
                return self.normalize_transformers_results(emotion_results[0])
            else:
                self.add_log("âš ï¸ HiÃ§bir transformer model Ã§alÄ±ÅŸmadÄ±, alternatif kullanÄ±lÄ±yor")
                return self.use_sklearn_emotion_model(feature_vector)
            
        except ImportError:
            self.add_log("âš ï¸ Transformers kÃ¼tÃ¼phanesi yok, alternatif yÃ¶ntem kullanÄ±lÄ±yor")
            return self.use_sklearn_emotion_model(feature_vector)
        except Exception as e:
            self.add_log(f"âŒ Pretrained model hatasÄ±: {e}")
            return self.use_sklearn_emotion_model(feature_vector)
    
    def normalize_transformers_results(self, transformer_results):
        """Transformer sonuÃ§larÄ±nÄ± normalize et"""
        try:
            # Transformer sonuÃ§larÄ±nÄ± kendi duygu kategorilerimize Ã§evir
            emotion_mapping = {
                'happy': 'mutlu',
                'joy': 'mutlu',
                'sad': 'Ã¼zgÃ¼n',
                'angry': 'kÄ±zgÄ±n',
                'calm': 'sakin',
                'neutral': 'sakin',
                'excited': 'heyecanlÄ±',
                'fear': 'stresli',
                'surprise': 'ÅŸaÅŸkÄ±n'
            }
            
            normalized_scores = {
                'mutlu': 0.0, 'Ã¼zgÃ¼n': 0.0, 'kÄ±zgÄ±n': 0.0,
                'sakin': 0.0, 'heyecanlÄ±': 0.0, 'stresli': 0.0
            }
            
            for result in transformer_results:
                label = result['label'].lower()
                score = result['score']
                
                # EÅŸleÅŸtirme yap
                for eng_emotion, tr_emotion in emotion_mapping.items():
                    if eng_emotion in label:
                        if tr_emotion in normalized_scores:
                            normalized_scores[tr_emotion] += score
                        break
            
            # Normalize et
            total_score = sum(normalized_scores.values())
            if total_score > 0:
                for emotion in normalized_scores:
                    normalized_scores[emotion] /= total_score
            
            return normalized_scores
            
        except Exception as e:
            self.add_log(f"âŒ SonuÃ§ normalizasyon hatasÄ±: {e}")
            return self.mock_pretrained_results()
    
    def use_sklearn_emotion_model(self, feature_vector):
        """Scikit-learn tabanlÄ± duygu modeli"""
        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.svm import SVC
            from sklearn.naive_bayes import GaussianNB
            
            # Mock eÄŸitilmiÅŸ model (gerÃ§ek projede Ã¶nceden eÄŸitilmiÅŸ model yÃ¼klenir)
            # model = joblib.load('emotion_rf_model.pkl')
            
            # Åimdilik geliÅŸmiÅŸ kural tabanlÄ± tahmin dÃ¶ndÃ¼r
            return self.mock_sklearn_results()
            
        except Exception as e:
            self.add_log(f"âŒ Sklearn model hatasÄ±: {e}")
            raise e
    
    def mock_pretrained_results(self):
        """Mock pretrained model sonuÃ§larÄ±"""
        return {
            'mutlu': 0.15, 'Ã¼zgÃ¼n': 0.10, 'kÄ±zgÄ±n': 0.08,
            'sakin': 0.25, 'heyecanlÄ±': 0.20, 'stresli': 0.12,
            'ÅŸaÅŸkÄ±n': 0.06, 'korku': 0.04
        }
    
    def mock_sklearn_results(self):
        """Mock sklearn model sonuÃ§larÄ±"""
        return {
            'mutlu': 0.18, 'Ã¼zgÃ¼n': 0.12, 'kÄ±zgÄ±n': 0.10,
            'sakin': 0.22, 'heyecanlÄ±': 0.18, 'stresli': 0.15,
            'ÅŸaÅŸkÄ±n': 0.03, 'korku': 0.02
        }
    
    def plot_temporal_emotion_analysis(self):
        """Zamansal duygu analizi gÃ¶rselleÅŸtirmesi"""
        try:
            if not hasattr(self, 'temporal_emotion_history') or not self.temporal_emotion_history:
                return
                
            self.ax_emotion.clear()
            
            # Zaman ekseni oluÅŸtur
            times = [te['time'] for te in self.temporal_emotion_history]
            
            # Her duygu iÃ§in zaman serisi Ã§iz
            emotion_colors = {
                'mutlu': '#2ECC71', 'Ã¼zgÃ¼n': '#3498DB', 'kÄ±zgÄ±n': '#E74C3C',
                'sakin': '#95A5A6', 'heyecanlÄ±': '#F39C12', 'stresli': '#E67E22',
                'ÅŸaÅŸkÄ±n': '#9B59B6', 'korku': '#34495E'
            }
            
            for emotion in ['mutlu', 'Ã¼zgÃ¼n', 'kÄ±zgÄ±n', 'sakin', 'heyecanlÄ±']:
                scores = [te['emotions'].get(emotion, 0) * 100 for te in self.temporal_emotion_history]
                color = emotion_colors.get(emotion, '#95A5A6')
                
                self.ax_emotion.plot(times, scores, label=emotion.capitalize(), 
                                   color=color, linewidth=2, marker='o', markersize=4)
            
            self.ax_emotion.set_xlabel('Zaman (saniye)')
            self.ax_emotion.set_ylabel('Duygu Skoru (%)')
            self.ax_emotion.set_title('â° Zamansal Duygu DeÄŸiÅŸimi')
            self.ax_emotion.legend(loc='upper right', fontsize=9)
            self.ax_emotion.grid(True, alpha=0.3)
            self.ax_emotion.set_ylim(0, 100)
            
            # Dominant duygu bÃ¶lgelerini vurgula
            for i in range(len(self.temporal_emotion_history) - 1):
                current_emotions = self.temporal_emotion_history[i]['emotions']
                dominant_emotion = max(current_emotions.keys(), key=current_emotions.get)
                dominant_color = emotion_colors.get(dominant_emotion, '#95A5A6')
                
                start_time = self.temporal_emotion_history[i]['time']
                end_time = self.temporal_emotion_history[i + 1]['time']
                
                self.ax_emotion.axvspan(start_time, end_time, alpha=0.1, color=dominant_color)
            
            self.canvas3.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Zamansal gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def assess_audio_quality(self, audio_data, sample_rate):
        """Ses kalitesini deÄŸerlendir"""
        try:
            quality_score = 0.0
            quality_factors = {}
            
            # 1. Sinyal-GÃ¼rÃ¼ltÃ¼ OranÄ± (SNR) tahmini
            signal_power = np.mean(audio_data ** 2)
            noise_threshold = 0.001
            noise_power = np.mean(audio_data[np.abs(audio_data) < noise_threshold] ** 2) if np.any(np.abs(audio_data) < noise_threshold) else 0.0001
            snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else 40
            quality_factors['snr'] = min(snr / 40, 1.0)  # 40 dB'yi maksimum kabul et
            
            # 2. Dinamik AralÄ±k
            dynamic_range = np.max(audio_data) - np.min(audio_data)
            quality_factors['dynamic_range'] = min(dynamic_range / 2.0, 1.0)  # -1 ile +1 arasÄ± max
            
            # 3. Kliping (Kesik) Tespiti
            clipping_threshold = 0.95
            clipped_samples = np.sum(np.abs(audio_data) > clipping_threshold)
            clipping_ratio = clipped_samples / len(audio_data)
            quality_factors['no_clipping'] = max(1.0 - clipping_ratio * 10, 0.0)
            
            # 4. Frekans DaÄŸÄ±lÄ±mÄ±
            freqs = np.fft.fftfreq(len(audio_data), 1/sample_rate)
            fft = np.abs(np.fft.fft(audio_data))
            
            # Ä°nsan sesi frekans aralÄ±ÄŸÄ± (80-8000 Hz)
            voice_freq_mask = (np.abs(freqs) >= 80) & (np.abs(freqs) <= 8000)
            voice_energy = np.sum(fft[voice_freq_mask])
            total_energy = np.sum(fft)
            voice_ratio = voice_energy / total_energy if total_energy > 0 else 0
            quality_factors['voice_frequency'] = voice_ratio
            
            # 5. Ses SÃ¼rekliliÄŸi (Sessizlik analizine gÃ¶re)
            silence_threshold = 0.01
            voice_frames = np.abs(audio_data) > silence_threshold
            continuity = np.sum(voice_frames) / len(audio_data)
            quality_factors['continuity'] = continuity
            
            # 6. Spektral DÃ¼zlÃ¼k (GÃ¼rÃ¼ltÃ¼ gÃ¶stergesi)
            spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=audio_data))
            quality_factors['spectral_clarity'] = max(1.0 - spectral_flatness, 0.0)
            
            # Genel kalite skoru hesapla
            weights = {
                'snr': 0.25,
                'dynamic_range': 0.15,
                'no_clipping': 0.20,
                'voice_frequency': 0.15,
                'continuity': 0.15,
                'spectral_clarity': 0.10
            }
            
            quality_score = sum(quality_factors[factor] * weights[factor] for factor in quality_factors)
            
            return {
                'overall_score': quality_score,
                'factors': quality_factors,
                'details': {
                    'snr_db': snr,
                    'dynamic_range': dynamic_range,
                    'clipping_percentage': clipping_ratio * 100,
                    'voice_frequency_ratio': voice_ratio * 100,
                    'voice_continuity': continuity * 100,
                    'spectral_flatness': spectral_flatness
                }
            }
            
        except Exception as e:
            self.add_log(f"âŒ Ses kalitesi analizi hatasÄ±: {e}")
            return {'overall_score': 0.5, 'factors': {}, 'details': {}}
    
    def debug_emotion_analysis(self, features):
        """Duygu analizi debug bilgileri"""
        debug_info = []
        
        debug_info.append(f"ğŸ” DUYGU ANALÄ°ZÄ° DEBUG BÄ°LGÄ°LERÄ°:")
        debug_info.append(f"{'='*40}")
        
        # Temel Ã¶zellikler
        debug_info.append(f"ğŸ“Š Temel Ã–zellikler:")
        debug_info.append(f"  âš¡ Enerji: {features['energy']:.6f}")
        debug_info.append(f"  ğŸµ Perde: {features['pitch_mean']:.1f} Hz")
        debug_info.append(f"  ğŸ“ˆ Perde Std: {features['pitch_std']:.1f} Hz")
        debug_info.append(f"  ğŸŒŠ ZCR: {features['zcr_mean']:.4f}")
        debug_info.append(f"  ğŸ“Š Spektral Bant: {features['spectral_bandwidth_mean']:.1f} Hz")
        debug_info.append(f"  ğŸ›ï¸ Enerji Varyans: {features['energy_variance']:.8f}")
        debug_info.append("")
        
        # MUTLULUK kontrolleri
        debug_info.append(f"ğŸ˜Š MUTLULUK KONTROL:")
        mutlu_tests = []
        if features['pitch_mean'] > 150:
            mutlu_tests.append("âœ… Perde > 150 Hz")
        else:
            mutlu_tests.append("âŒ Perde Ã§ok dÃ¼ÅŸÃ¼k")
            
        if features['energy'] > 0.0005:
            mutlu_tests.append("âœ… Enerji > 0.0005")
        else:
            mutlu_tests.append("âŒ Enerji Ã§ok dÃ¼ÅŸÃ¼k")
            
        if features['zcr_mean'] > 0.05:
            mutlu_tests.append("âœ… ZCR > 0.05")
        else:
            mutlu_tests.append("âŒ ZCR dÃ¼ÅŸÃ¼k")
            
        # Kahkaha testi
        laughter_detected = self.detect_laughter_patterns(features)
        if laughter_detected:
            mutlu_tests.append("ğŸ‰ KAHKAHA TESPÄ°T EDÄ°LDÄ°!")
        else:
            mutlu_tests.append("âŒ Kahkaha tespit edilmedi")
            
        for test in mutlu_tests:
            debug_info.append(f"  {test}")
        debug_info.append("")
        
        # SAKÄ°NLÄ°K kontrolleri
        debug_info.append(f"ğŸ˜Œ SAKÄ°NLÄ°K KONTROL:")
        sakin_tests = []
        if features['energy'] < 0.0002:
            sakin_tests.append("âœ… Ã‡ok dÃ¼ÅŸÃ¼k enerji")
        else:
            sakin_tests.append("âŒ Enerji yeterince dÃ¼ÅŸÃ¼k deÄŸil")
            
        if features['zcr_mean'] < 0.03:
            sakin_tests.append("âœ… Ã‡ok dÃ¼ÅŸÃ¼k ZCR")
        else:
            sakin_tests.append("âŒ ZCR yeterince dÃ¼ÅŸÃ¼k deÄŸil")
            
        if features['pitch_std'] < 20:
            sakin_tests.append("âœ… Stabil perde")
        else:
            sakin_tests.append("âŒ Perde deÄŸiÅŸken")
            
        for test in sakin_tests:
            debug_info.append(f"  {test}")
        debug_info.append("")
        
        return "\n".join(debug_info)
    
    def clean_audio_buffer(self, audio_data):
        """Ses verisini temizle - NaN ve Infinity deÄŸerleri kaldÄ±r"""
        try:
            self.add_log("ğŸ§¹ Ses verisi temizleniyor...")
            
            # NaN ve Infinity kontrolÃ¼
            if not np.isfinite(audio_data).all():
                self.add_log("âš ï¸ Ses verisinde NaN/Infinity deÄŸerleri tespit edildi, temizleniyor...")
                
                # NaN deÄŸerleri sÄ±fÄ±r ile deÄŸiÅŸtir
                audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
                
                # AÅŸÄ±rÄ± bÃ¼yÃ¼k deÄŸerleri kÄ±rp (-1, +1 aralÄ±ÄŸÄ±na)
                audio_data = np.clip(audio_data, -1.0, 1.0)
                
                # EÄŸer tÃ¼m veriler sÄ±fÄ±r olduysa, kÃ¼Ã§Ã¼k bir gÃ¼rÃ¼ltÃ¼ ekle
                if np.all(audio_data == 0):
                    self.add_log("âš ï¸ TÃ¼m ses verisi sÄ±fÄ±r, kÃ¼Ã§Ã¼k gÃ¼rÃ¼ltÃ¼ ekleniyor...")
                    audio_data = np.random.normal(0, 0.001, len(audio_data))
                
                self.add_log("âœ… Ses verisi temizlendi")
            
            return audio_data.astype(np.float32)
            
        except Exception as e:
            self.add_log(f"âŒ Ses temizleme hatasÄ±: {e}")
            # Son Ã§are: sessizlik dÃ¶ndÃ¼r
            return np.zeros(len(audio_data), dtype=np.float32)
    
    def get_default_features(self):
        """Hata durumunda kullanÄ±lacak varsayÄ±lan Ã¶zellikler"""
        return {
            # Temel Ã¶zellikler
            'mfcc_mean': 0.0,
            'mfcc_std': 1.0,
            'spectral_centroid_mean': 1000.0,
            'spectral_centroid_std': 500.0,
            'zcr_mean': 0.05,
            'zcr_std': 0.02,
            
            # GeliÅŸmiÅŸ spektral Ã¶zellikler
            'chroma_mean': 0.1,
            'chroma_std': 0.05,
            'mel_spectrogram_mean': 0.01,
            'tonnetz_mean': 0.0,
            'spectral_contrast_mean': 10.0,
            'spectral_bandwidth_mean': 1500.0,
            'spectral_flatness_mean': 0.1,
            'spectral_rolloff_mean': 2000.0,
            
            # Prozodik Ã¶zellikler
            'tempo': 100.0,
            'speaking_rate': 2.0,
            'pitch_mean': 200.0,
            'pitch_std': 50.0,
            'pitch_range': 0.25,
            
            # Enerji Ã¶zellikleri
            'energy': 0.001,
            'rms_energy': 0.01,
            'energy_variance': 0.0001,
            'energy_mean': 0.001,
            'energy_dynamic_range': 0.005,
            
            # Sessizlik ve duraklama
            'silence_ratio': 0.3,
            'voice_activity_ratio': 0.7,
            
            # Harmonik Ã¶zellikler
            'harmonic_mean': 0.001,
            'percussive_mean': 0.001,
        }
    
    def get_default_emotion_scores(self):
        """Hata durumunda kullanÄ±lacak varsayÄ±lan duygu skorlarÄ±"""
        return {
            'mutlu': 0.25,
            'Ã¼zgÃ¼n': 0.15,
            'kÄ±zgÄ±n': 0.10,
            'sakin': 0.30,
            'heyecanlÄ±': 0.15,
            'stresli': 0.05
        }
    
    def update_live_waveform(self):
        """CanlÄ± kayÄ±t sÄ±rasÄ±nda dalga formunu gÃ¼ncelle"""
        try:
            if len(self.recorded_audio) > 0:
                audio_array = np.array(self.recorded_audio[-SAMPLE_RATE:])  # Son 1 saniye
                if len(audio_array) > 100:  # Yeterli veri varsa
                    time_axis = np.arange(len(audio_array)) / SAMPLE_RATE
                    
                    # Ana dalga formu - sadece gÃ¼ncelle, layout deÄŸiÅŸikliÄŸi yapma
                    self.ax_waveform.clear()
                    self.ax_waveform.plot(time_axis, audio_array, color='#2E86AB', linewidth=0.8)
                    self.ax_waveform.set_ylim(-1, 1)
                    self.ax_waveform.set_title("ğŸ™ï¸ CanlÄ± KayÄ±t - Dalga Formu")
                    self.ax_waveform.grid(True, alpha=0.3)
                    
                    # Canvas'Ä± gÃ¼ncelle - ancak layout hesaplamasÄ± yapma
                    self.canvas.draw_idle()
                    
        except Exception as e:
            # CanlÄ± gÃ¼ncelleme hatalarÄ±nÄ± sessizce geÃ§
            pass
    
    def optimize_audio_device(self):
        """En uygun ses cihazÄ±nÄ± seÃ§ ve optimize et"""
        try:
            self.add_log("ğŸ§ Ses cihazlarÄ± taranÄ±yor...")
            
            # Mevcut cihazlarÄ± listele
            devices = sd.query_devices()
            
            # En iyi giriÅŸ cihazÄ±nÄ± bul
            best_input_device = None
            best_score = 0
            
            for i, device in enumerate(devices):
                if device['max_input_channels'] > 0:  # GiriÅŸ cihazÄ±
                    score = 0
                    
                    # Samplerate desteÄŸi
                    try:
                        sd.check_device(i, samplerate=SAMPLE_RATE)
                        score += 10
                    except:
                        continue
                    
                    # Kanal sayÄ±sÄ±
                    score += device['max_input_channels']
                    
                    # VarsayÄ±lan cihaz bonusu
                    if device == sd.query_devices(kind='input'):
                        score += 5
                    
                    # macOS iÃ§in Core Audio tercih et
                    if 'Core Audio' in device['hostapi_name']:
                        score += 3
                    
                    # USB/Harici cihaz bonusu
                    if 'USB' in device['name'] or 'External' in device['name']:
                        score += 2
                    
                    if score > best_score:
                        best_score = score
                        best_input_device = i
            
            if best_input_device is not None:
                sd.default.device[0] = best_input_device  # Input device
                device_info = devices[best_input_device]
                self.add_log(f"ğŸ¤ SeÃ§ilen cihaz: {device_info['name']}")
                self.add_log(f"ğŸ“Š API: {device_info['hostapi_name']}")
                self.add_log(f"ğŸ”Š Max kanallar: {device_info['max_input_channels']}")
                self.add_log(f"âš¡ Gecikme: {device_info['default_low_input_latency']*1000:.1f}ms")
            else:
                self.add_log("âš ï¸ Uygun ses cihazÄ± bulunamadÄ±, varsayÄ±lan kullanÄ±lacak")
                
            # Buffer ayarlarÄ±nÄ± optimize et
            latency = sd.query_devices(sd.default.device[0])['default_low_input_latency']
            optimal_blocksize = int(SAMPLE_RATE * latency)
            
            self.add_log(f"ğŸ”§ Optimal buffer boyutu: {optimal_blocksize} sample")
            return optimal_blocksize
            
        except Exception as e:
            self.add_log(f"âŒ Cihaz optimizasyonu hatasÄ±: {e}")
            return BUFFER_SIZE
    
    def update_recording_progress(self, progress, elapsed, total_duration):
        """KayÄ±t ilerlemesini gÃ¼ncelle"""
        try:
            remaining = total_duration - elapsed
            
            # Durum etiketi gÃ¼ncelle
            self.status_label.config(
                text=f"ğŸ™ï¸ KayÄ±t: {elapsed:.1f}s / {total_duration:.1f}s (%{progress:.1f})"
            )
            
            # Her 2 saniyede bir ilerleme logu
            if int(elapsed) % 2 == 0 and elapsed > 0:
                self.add_log(f"ğŸ“Š Ä°lerleme: %{progress:.1f} - {elapsed:.1f}s / {total_duration:.1f}s")
            
        except Exception as e:
            # Progress gÃ¼ncelleme hatalarÄ±nÄ± sessizce geÃ§
            pass
    
    def record_audio_simple(self, duration):
        """En basit kayÄ±t yÃ¶ntemi - fallback"""
        try:
            self.add_log("ğŸ™ï¸ FALLBACK: En basit kayÄ±t yÃ¶ntemi")
            
            # Ses cihazÄ±nÄ± optimize et
            self.optimize_audio_device()
            
            # Tek seferde tÃ¼m kaydÄ± al - eski yÃ¶ntem ama gÃ¼venilir
            total_samples = int(SAMPLE_RATE * duration)
            self.add_log(f"ğŸ“Š Tek seferde {total_samples} sample alÄ±nacak")
            
            # KayÄ±t baÅŸlat
            self.add_log("ğŸ”´ KayÄ±t baÅŸlÄ±yor...")
            start_time = time.time()
            
            # SoundDevice'Ä±n basit rec fonksiyonu
            audio_data = sd.rec(
                frames=total_samples,
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=DTYPE,
                blocking=False  # Non-blocking kayÄ±t
            )
            
            # KayÄ±t tamamlanana kadar bekle ve progress gÃ¶ster
            while sd.get_stream().active and self.is_recording:
                elapsed = time.time() - start_time
                progress = min((elapsed / duration) * 100, 100)
                
                self.root.after(1, lambda p=progress, e=elapsed: 
                    self.status_label.config(text=f"ğŸ™ï¸ Basit KayÄ±t: {e:.1f}s / {duration}s (%{p:.1f})"))
                
                if int(elapsed) % 2 == 0:  # Her 2 saniyede log
                    self.add_log(f"ğŸ“Š KayÄ±t sÃ¼rÃ¼yor: {elapsed:.1f}s")
                
                time.sleep(0.1)
                
                # Timeout kontrolÃ¼
                if elapsed > duration + 2:
                    break
            
            # KayÄ±t verisini al
            sd.wait()  # KayÄ±t tamamlanana kadar bekle
            
            # SonuÃ§larÄ± kontrol et
            self.recorded_audio = audio_data[:, 0] if audio_data.ndim > 1 else audio_data
            self.sample_rate = SAMPLE_RATE
            
            actual_duration = len(self.recorded_audio) / SAMPLE_RATE
            self.add_log(f"âœ… Basit kayÄ±t tamamlandÄ±!")
            self.add_log(f"ğŸ“Š Hedef: {duration}s, GerÃ§ek: {actual_duration:.2f}s")
            self.add_log(f"ğŸ”¢ Sample sayÄ±sÄ±: {len(self.recorded_audio)}")
            
            # Ses analizi
            if len(self.recorded_audio) > 0:
                max_amplitude = np.max(np.abs(self.recorded_audio))
                avg_amplitude = np.mean(np.abs(self.recorded_audio))
                
                self.add_log(f"ğŸ”Š Max seviye: {max_amplitude:.4f}")
                self.add_log(f"ğŸ“ˆ Ortalama seviye: {avg_amplitude:.4f}")
                
                # Normalize et (gerekirse)
                if max_amplitude > 0.95:
                    self.recorded_audio = self.recorded_audio / max_amplitude * 0.9
                    self.add_log("ğŸ”§ Ses seviyesi normalize edildi")
                
                # UI gÃ¼ncelle
                self.root.after(1, lambda: self.update_comprehensive_plots(self.recorded_audio))
                self.root.after(1, lambda: self.status_label.config(text="âœ… Basit kayÄ±t tamamlandÄ±"))
                
                # Dosyaya kaydet
                self.save_recording()
                
                # DÃ¼ÄŸmeleri etkinleÅŸtir
                self.root.after(1, lambda: self.analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.quick_analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.report_button.config(state=tk.NORMAL))
                
                return True
            else:
                self.add_log("âŒ KayÄ±t verisi alÄ±namadÄ±!")
                return False
                
        except Exception as e:
            self.add_log(f"âŒ Basit kayÄ±t hatasÄ±: {e}")
            return False
        finally:
            self.is_recording = False
            self.root.after(1, lambda: self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat"))

def main():
    """Ana fonksiyon"""
    # macOS uyumluluÄŸu iÃ§in
    import sys
    
    # Root pencere oluÅŸtur
    root = tk.Tk()
    
    # macOS'ta Tkinter'Ä± Ã¶n plana getir
    if sys.platform == "darwin":  # macOS
        try:
            # macOS'ta Tkinter uygulamasÄ±nÄ± aktifleÅŸtir
            from subprocess import call
            call(['osascript', '-e', 'tell application "Python" to activate'])
        except:
            pass
    
    # UygulamayÄ± baÅŸlat
    app = SesKayitAnaliz(root)
    
    # Pencereyi merkeze getir ve gÃ¶rÃ¼nÃ¼r yap
    root.update()
    root.after(100, lambda: root.focus_force())
    
    try:
        root.mainloop()
    except KeyboardInterrupt:
        print("\nUygulama kapatÄ±lÄ±yor...")
        root.quit()

if __name__ == "__main__":
    main() 