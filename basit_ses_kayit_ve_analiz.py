#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Basit Ses KaydÄ± ve PyAnnote Analizi

Bu program, mikrofon giriÅŸinden ses kaydÄ± alÄ±r, dosyaya kaydeder ve
PyAnnote.audio kullanarak konuÅŸmacÄ± diyarizasyonu yapar.
"""

import os
import time
import wave
import threading
import numpy as np
import sounddevice as sd
import torch
from pyannote.audio import Pipeline
from pyannote.audio import Model
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext, filedialog
from datetime import datetime
# macOS uyumluluÄŸu iÃ§in matplotlib backend ayarÄ±
import matplotlib
matplotlib.use('TkAgg')  # macOS'ta en uyumlu backend

import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.figure import Figure
import matplotlib.patches as patches

# macOS'ta font uyarÄ±larÄ±nÄ± bastÄ±r
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
import openai  # Whisper yerine OpenAI API kullanacaÄŸÄ±z
import base64  # Base64 kodlama iÃ§in
import librosa
import soundfile as sf
import noisereduce as nr
from scipy import signal
import seaborn as sns
from collections import defaultdict
import json
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings("ignore")

# HuggingFace eriÅŸim tokeni - kendi tokeninizle deÄŸiÅŸtirin
HF_TOKEN = "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"

# OpenAI API anahtarÄ± - kendi anahtarÄ±nÄ±zla deÄŸiÅŸtirin
OPENAI_API_KEY = "sk-proj-AZXoZV7vdTPzxP1GBiE7T3BlbkFJnUPhxjWfTcWBaod7INhj"  # Kendi API anahtarÄ±nÄ±zÄ± girin
openai.api_key = OPENAI_API_KEY

# Ses kaydÄ± parametreleri - Optimize edildi
SAMPLE_RATE = 44100  # Standart ses kalitesi
CHANNELS = 1         # Mono kayÄ±t
DTYPE = np.float32   # YÃ¼ksek hassasiyet
RECORDING_SECONDS = 30  # VarsayÄ±lan kayÄ±t sÃ¼resi

# Buffer parametreleri - Optimize edildi
BUFFER_SIZE = 1024      # KÃ¼Ã§Ã¼k ve gÃ¼venilir buffer boyutu  
BLOCK_DURATION = 0.1    # Her blok 100ms 
OVERLAP_SAMPLES = 0     # Overlap kullanmÄ±yoruz (sadece direkt kayÄ±t)

# GPT model parametresi
GPT_MODEL = "gpt-4o-mini"  # OpenAI'nin gpt-4o-mini modeli
GPT_LANGUAGE = "tr"  # TÃ¼rkÃ§e dil desteÄŸi

# Analiz modelleri
DIARIZATION_MODEL = "pyannote/speaker-diarization-3.1"
VAD_MODEL = "pyannote/voice-activity-detection"
OVERLAP_MODEL = "pyannote/overlapped-speech-detection"
SEGMENTATION_MODEL = "pyannote/segmentation"
SEPARATION_MODEL = "pyannote/speech-separation-ami-1.0"

# Renk paleti konuÅŸmacÄ±lar iÃ§in
SPEAKER_COLORS = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F']

# Duygu analizi iÃ§in renk paleti
EMOTION_COLORS = {
    'mutlu': '#2ECC71',
    'Ã¼zgÃ¼n': '#3498DB', 
    'kÄ±zgÄ±n': '#E74C3C',
    'sakin': '#95A5A6',
    'heyecanlÄ±': '#F39C12',
    'stresli': '#E67E22'
}

class SesKayitAnaliz:
    def __init__(self, root):
        self.root = root
        self.root.title("GeliÅŸmiÅŸ Ses Analizi ve PyAnnote Sistemi")
        
        # macOS uyumluluÄŸu iÃ§in pencere ayarlarÄ±
        self.root.geometry("1200x800")
        
        # macOS'ta state('zoomed') Ã§alÄ±ÅŸmaz, alternatif yaklaÅŸÄ±m
        try:
            # Windows/Linux iÃ§in
            if self.root.tk.call('tk', 'windowingsystem') == 'win32':
                self.root.state('zoomed')
            # macOS iÃ§in
            elif self.root.tk.call('tk', 'windowingsystem') == 'aqua':
                self.root.attributes('-zoomed', True)
            # Linux iÃ§in
            else:
                self.root.attributes('-zoomed', True)
        except:
            # Hata durumunda normal boyut kullan
            self.root.geometry("1200x800")
        
        # Pencereyi ekranÄ±n ortasÄ±na yerleÅŸtir
        self.root.update_idletasks()
        width = self.root.winfo_width()
        height = self.root.winfo_height()
        x = (self.root.winfo_screenwidth() // 2) - (width // 2)
        y = (self.root.winfo_screenheight() // 2) - (height // 2)
        self.root.geometry(f'{width}x{height}+{x}+{y}')
        
        # Minimum pencere boyutu
        self.root.minsize(800, 600)
        
        # Pencereyi gÃ¶rÃ¼nÃ¼r yap
        self.root.deiconify()
        self.root.lift()
        self.root.focus_force()
        
        # Ana Ã§erÃ§eve
        main_frame = ttk.Frame(root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # BaÅŸlÄ±k
        header = ttk.Label(main_frame, text="ğŸ¤ GeliÅŸmiÅŸ Ses Analizi ve KonuÅŸmacÄ± Diyarizasyonu ğŸ¤", 
                          font=("Helvetica", 18, "bold"))
        header.pack(pady=10)
        
        # Kontrol Ã§erÃ§evesi - Ãœst kÄ±sÄ±m
        control_frame_top = ttk.Frame(main_frame)
        control_frame_top.pack(fill=tk.X, pady=5)
        
        # KayÄ±t kontrolleri
        record_frame = ttk.LabelFrame(control_frame_top, text="ğŸ™ï¸ KayÄ±t Kontrolleri")
        record_frame.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        
        # KayÄ±t sÃ¼resi giriÅŸi
        ttk.Label(record_frame, text="KayÄ±t SÃ¼resi (saniye):").pack(side=tk.LEFT, padx=5)
        self.duration_var = tk.StringVar(value=str(RECORDING_SECONDS))
        duration_entry = ttk.Entry(record_frame, textvariable=self.duration_var, width=5)
        duration_entry.pack(side=tk.LEFT, padx=5)
        
        # KayÄ±t dosya adÄ± giriÅŸi
        ttk.Label(record_frame, text="Dosya AdÄ±:").pack(side=tk.LEFT, padx=5)
        self.filename_var = tk.StringVar(value=f"kayit_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav")
        filename_entry = ttk.Entry(record_frame, textvariable=self.filename_var, width=20)
        filename_entry.pack(side=tk.LEFT, padx=5)
        
        # BaÅŸlat/Durdur dÃ¼ÄŸmesi
        self.record_button = ttk.Button(record_frame, text="ğŸ”´ KaydÄ± BaÅŸlat", 
                                       command=self.toggle_recording)
        self.record_button.pack(side=tk.LEFT, padx=10)
        
        # Dosya seÃ§ dÃ¼ÄŸmesi
        self.load_button = ttk.Button(record_frame, text="ğŸ“ Dosya SeÃ§", 
                                     command=self.load_audio_file)
        self.load_button.pack(side=tk.LEFT, padx=5)
        
        # Analiz seÃ§enekleri Ã§erÃ§evesi
        analysis_frame = ttk.LabelFrame(control_frame_top, text="ğŸ”¬ Analiz SeÃ§enekleri")
        analysis_frame.pack(side=tk.RIGHT, padx=5)
        
        # Analiz seÃ§enekleri
        self.enable_vad = tk.BooleanVar(value=True)
        self.enable_overlap = tk.BooleanVar(value=True)
        self.enable_emotion = tk.BooleanVar(value=True)
        self.enable_noise_reduction = tk.BooleanVar(value=True)
        self.enable_separation = tk.BooleanVar(value=False)
        self.enable_live_analysis = tk.BooleanVar(value=False)
        
        ttk.Checkbutton(analysis_frame, text="Ses Aktivitesi Tespiti", variable=self.enable_vad).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Ã–rtÃ¼ÅŸen KonuÅŸma", variable=self.enable_overlap).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Duygu Analizi", variable=self.enable_emotion).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="GÃ¼rÃ¼ltÃ¼ Azaltma", variable=self.enable_noise_reduction).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="Ses AyrÄ±ÅŸtÄ±rma", variable=self.enable_separation).pack(anchor=tk.W)
        ttk.Checkbutton(analysis_frame, text="CanlÄ± Analiz", variable=self.enable_live_analysis).pack(anchor=tk.W)
        
        # Kontrol Ã§erÃ§evesi - Alt kÄ±sÄ±m
        control_frame_bottom = ttk.Frame(main_frame)
        control_frame_bottom.pack(fill=tk.X, pady=5)
        
        # Ana analiz dÃ¼ÄŸmeleri
        button_frame = ttk.Frame(control_frame_bottom)
        button_frame.pack(side=tk.LEFT)
        
        # Analiz dÃ¼ÄŸmesi
        self.analyze_button = ttk.Button(button_frame, text="ğŸ” Tam Analiz Yap", 
                                        command=self.analyze_recording,
                                        state=tk.DISABLED)
        self.analyze_button.pack(side=tk.LEFT, padx=5)
        
        # HÄ±zlÄ± analiz dÃ¼ÄŸmesi
        self.quick_analyze_button = ttk.Button(button_frame, text="âš¡ HÄ±zlÄ± Analiz", 
                                              command=self.quick_analyze,
                                              state=tk.DISABLED)
        self.quick_analyze_button.pack(side=tk.LEFT, padx=5)
        
        # Rapor oluÅŸtur dÃ¼ÄŸmesi
        self.report_button = ttk.Button(button_frame, text="ğŸ“„ Rapor OluÅŸtur", 
                                       command=self.generate_report,
                                       state=tk.DISABLED)
        self.report_button.pack(side=tk.LEFT, padx=5)
        
        # Durum etiketi
        self.status_label = ttk.Label(control_frame_bottom, text="ğŸŸ¢ HazÄ±r", 
                                     font=("Helvetica", 12, "bold"))
        self.status_label.pack(side=tk.RIGHT, padx=20)
        
        # Ä°Ã§erik Ã§erÃ§evesi
        content_frame = ttk.Frame(main_frame)
        content_frame.pack(fill=tk.BOTH, expand=True, pady=10)
        
        # Sol panel (gÃ¶rselleÅŸtirme)
        left_panel = ttk.Frame(content_frame)
        left_panel.pack(fill=tk.BOTH, expand=True, side=tk.LEFT, padx=(0, 5))
        
        # Notebook widget for multiple visualizations
        self.viz_notebook = ttk.Notebook(left_panel)
        self.viz_notebook.pack(fill=tk.BOTH, expand=True)
        
        # Ses gÃ¶rselleÅŸtirme sekmesi
        viz_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(viz_frame, text="ğŸŒŠ Dalga Formu")
        
        self.fig = Figure(figsize=(8, 6), dpi=100)
        
        # Alt grafik alanlarÄ± oluÅŸtur
        self.ax_waveform = self.fig.add_subplot(3, 1, 1)
        self.ax_spectrogram = self.fig.add_subplot(3, 1, 2)
        self.ax_diarization = self.fig.add_subplot(3, 1, 3)
        
        self.canvas = FigureCanvasTkAgg(self.fig, master=viz_frame)
        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Spektogram sekmesi
        spectrogram_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(spectrogram_frame, text="ğŸ“Š Spektogram")
        
        self.fig2 = Figure(figsize=(8, 6), dpi=100)
        self.ax_spec_detail = self.fig2.add_subplot(111)
        self.canvas2 = FigureCanvasTkAgg(self.fig2, master=spectrogram_frame)
        self.canvas2.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Duygu analizi sekmesi
        emotion_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(emotion_frame, text="ğŸ˜Š Duygu Analizi")
        
        self.fig3 = Figure(figsize=(8, 6), dpi=100)
        self.ax_emotion = self.fig3.add_subplot(111)
        self.canvas3 = FigureCanvasTkAgg(self.fig3, master=emotion_frame)
        self.canvas3.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Ä°statistik sekmesi
        stats_frame = ttk.Frame(self.viz_notebook)
        self.viz_notebook.add(stats_frame, text="ğŸ“ˆ Ä°statistikler")
        
        self.fig4 = Figure(figsize=(8, 6), dpi=100)
        self.ax_stats1 = self.fig4.add_subplot(2, 2, 1)
        self.ax_stats2 = self.fig4.add_subplot(2, 2, 2)
        self.ax_stats3 = self.fig4.add_subplot(2, 2, 3)
        self.ax_stats4 = self.fig4.add_subplot(2, 2, 4)
        self.canvas4 = FigureCanvasTkAgg(self.fig4, master=stats_frame)
        self.canvas4.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # BoÅŸ grafikleri gÃ¶ster (thread-safe)
        self.root.after(500, self.update_empty_plots)
        
        # SaÄŸ panel (log ve sonuÃ§lar)
        right_panel = ttk.Frame(content_frame)
        right_panel.pack(fill=tk.BOTH, expand=True, side=tk.RIGHT, padx=(5, 0))
        
        # SaÄŸ panel iÃ§in notebook
        self.right_notebook = ttk.Notebook(right_panel)
        self.right_notebook.pack(fill=tk.BOTH, expand=True)
        
        # Log sekmesi
        log_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(log_frame, text="ğŸ“‹ Ä°ÅŸlem GÃ¼nlÃ¼ÄŸÃ¼")
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=25, 
                                                 font=("Consolas", 9))
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # SonuÃ§ sekmesi
        result_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(result_frame, text="ğŸ¯ Diyarizasyon SonuÃ§larÄ±")
        
        self.result_text = scrolledtext.ScrolledText(result_frame, height=25, 
                                                    font=("Consolas", 9))
        self.result_text.pack(fill=tk.BOTH, expand=True)
        
        # Transkript sekmesi
        transcript_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(transcript_frame, text="ğŸ’¬ KonuÅŸma Ä°Ã§eriÄŸi")
        
        self.transcript_text = scrolledtext.ScrolledText(transcript_frame, height=25, 
                                                       font=("Consolas", 9))
        self.transcript_text.pack(fill=tk.BOTH, expand=True)
        
        # DetaylÄ± analiz sekmesi
        analysis_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(analysis_frame, text="ğŸ”¬ DetaylÄ± Analiz")
        
        self.analysis_text = scrolledtext.ScrolledText(analysis_frame, height=25, 
                                                      font=("Consolas", 9))
        self.analysis_text.pack(fill=tk.BOTH, expand=True)
        
        # Duygu analizi sekmesi
        emotion_analysis_frame = ttk.Frame(self.right_notebook)
        self.right_notebook.add(emotion_analysis_frame, text="ğŸ˜Š Duygu Raporu")
        
        self.emotion_text = scrolledtext.ScrolledText(emotion_analysis_frame, height=25, 
                                                     font=("Consolas", 9))
        self.emotion_text.pack(fill=tk.BOTH, expand=True)
        
        # Durum deÄŸiÅŸkenleri
        self.is_recording = False
        self.recorded_audio = []
        self.recording_thread = None
        self.current_audio_file = None
        self.sample_rate = SAMPLE_RATE
        
        # Analiz sonuÃ§larÄ±
        self.diarization_result = None
        self.vad_result = None
        self.overlap_result = None
        self.emotion_result = None
        self.speaker_embeddings = {}
        self.noise_reduced_audio = None
        
        # Pipeline'lar
        self.pipelines = {}
        self.models_loaded = False
        
        # GerÃ§ek zamanlÄ± analiz iÃ§in
        self.live_analysis_thread = None
        self.live_analysis_running = False
        
        # HoÅŸgeldin mesajÄ±
        self.add_log("ğŸš€ GeliÅŸmiÅŸ Ses Analizi Sistemi baÅŸlatÄ±ldÄ±!")
        self.add_log("ğŸ“ KullanÄ±m: Ses kaydÄ± yapÄ±n veya dosya yÃ¼kleyin, sonra analiz seÃ§eneklerini belirleyip analiz baÅŸlatÄ±n.")
        self.add_log("âš¡ Ä°pucu: HÄ±zlÄ± analiz iÃ§in sadece temel Ã¶zellikleri seÃ§in, tam analiz iÃ§in tÃ¼m seÃ§enekleri aktifleÅŸtirin.")
        self.add_log("ğŸ”§ Modeller ilk kullanÄ±mda otomatik olarak yÃ¼klenecektir.")
        
        # Pencereyi zorla gÃ¼ncelle ve gÃ¶rÃ¼nÃ¼r yap
        self.root.update_idletasks()
        self.root.update()
        
        # macOS uyumluluÄŸu iÃ§in son ayarlar
        self.root.after(100, self._final_setup)
    
    def _final_setup(self):
        """Son kurulum iÅŸlemleri - macOS uyumluluÄŸu"""
        try:
            self.root.lift()
            self.root.attributes('-topmost', True)
            self.root.after(100, lambda: self.root.attributes('-topmost', False))
            self.root.focus_force()
            self.add_log("âœ… ArayÃ¼z hazÄ±r!")
        except Exception as e:
            self.add_log(f"âš ï¸ ArayÃ¼z kurulum uyarÄ±sÄ±: {e}")
    
    def update_empty_plots(self):
        """BoÅŸ grafikleri gÃ¶ster"""
        # Ana dalga formu
        self.ax_waveform.clear()
        self.ax_waveform.text(0.5, 0.5, "ğŸ¤ Ses kaydÄ± bekleniyor...", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=12, color='gray', transform=self.ax_waveform.transAxes)
        self.ax_waveform.set_title("Ses Dalga Formu")
        self.ax_waveform.grid(True, alpha=0.3)
        
        # Spektogram
        self.ax_spectrogram.clear()
        self.ax_spectrogram.text(0.5, 0.5, "ğŸ“Š Spektogram gÃ¶rÃ¼nÃ¼mÃ¼", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=10, color='gray', transform=self.ax_spectrogram.transAxes)
        self.ax_spectrogram.set_title("Spektogram")
        
        # Diyarizasyon
        self.ax_diarization.clear()
        self.ax_diarization.text(0.5, 0.5, "ğŸ‘¥ KonuÅŸmacÄ± zaman Ã§izelgesi", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=10, color='gray', transform=self.ax_diarization.transAxes)
        self.ax_diarization.set_title("KonuÅŸmacÄ± Diyarizasyonu")
        self.ax_diarization.set_xlabel("Zaman (saniye)")
        
        self.fig.tight_layout()
        self.canvas.draw()
        
        # DetaylÄ± spektogram
        self.ax_spec_detail.clear()
        self.ax_spec_detail.text(0.5, 0.5, "ğŸ“ˆ DetaylÄ± frekans analizi iÃ§in ses yÃ¼kleyin", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=12, color='gray', transform=self.ax_spec_detail.transAxes)
        self.canvas2.draw()
        
        # Duygu analizi
        self.ax_emotion.clear()
        self.ax_emotion.text(0.5, 0.5, "ğŸ˜Š Duygu analizi sonuÃ§larÄ± burada gÃ¶rÃ¼necek", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=12, color='gray', transform=self.ax_emotion.transAxes)
        self.canvas3.draw()
        
        # Ä°statistikler
        for ax in [self.ax_stats1, self.ax_stats2, self.ax_stats3, self.ax_stats4]:
            ax.clear()
            ax.text(0.5, 0.5, "ğŸ“Š", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=20, color='lightgray', transform=ax.transAxes)
        self.canvas4.draw()
    
    def load_audio_file(self):
        """Ses dosyasÄ± yÃ¼kle"""
        file_path = filedialog.askopenfilename(
            title="Ses DosyasÄ± SeÃ§in",
            filetypes=[
                ("Ses DosyalarÄ±", "*.wav *.mp3 *.flac *.m4a *.ogg"),
                ("WAV DosyalarÄ±", "*.wav"),
                ("MP3 DosyalarÄ±", "*.mp3"),
                ("FLAC DosyalarÄ±", "*.flac"),
                ("TÃ¼m Dosyalar", "*.*")
            ]
        )
        
        if file_path:
            try:
                self.add_log(f"ğŸ“ Dosya yÃ¼kleniyor: {os.path.basename(file_path)}")
                
                # Ses dosyasÄ±nÄ± yÃ¼kle
                audio_data, sample_rate = librosa.load(file_path, sr=None)
                self.sample_rate = sample_rate
                self.current_audio_file = file_path
                self.recorded_audio = audio_data
                
                # Dalga formunu gÃ¼ncelle
                self.update_comprehensive_plots(audio_data)
                
                # Analiz dÃ¼ÄŸmelerini etkinleÅŸtir
                self.analyze_button.config(state=tk.NORMAL)
                self.quick_analyze_button.config(state=tk.NORMAL)
                
                self.add_log(f"âœ… Dosya baÅŸarÄ±yla yÃ¼klendi! SÃ¼re: {len(audio_data)/sample_rate:.2f} saniye")
                self.status_label.config(text="ğŸŸ¢ Dosya yÃ¼klendi - Analiz iÃ§in hazÄ±r")
                
            except Exception as e:
                self.add_log(f"âŒ Dosya yÃ¼kleme hatasÄ±: {e}")
                messagebox.showerror("Hata", f"Dosya yÃ¼klenirken hata oluÅŸtu:\n{e}")
    
    def update_comprehensive_plots(self, audio_data):
        """KapsamlÄ± gÃ¶rselleÅŸtirme gÃ¼ncelle"""
        time_axis = np.arange(len(audio_data)) / self.sample_rate
        
        # Ana dalga formu
        self.ax_waveform.clear()
        self.ax_waveform.plot(time_axis, audio_data, color='#2E86AB', linewidth=0.8)
        self.ax_waveform.set_xlim(0, max(time_axis))
        self.ax_waveform.set_ylim(-1, 1)
        self.ax_waveform.set_ylabel("Genlik")
        self.ax_waveform.set_title("ğŸŒŠ Ses Dalga Formu")
        self.ax_waveform.grid(True, alpha=0.3)
        
        # Spektogram
        self.ax_spectrogram.clear()
        f, t, Sxx = signal.spectrogram(audio_data, self.sample_rate, nperseg=1024)
        self.ax_spectrogram.pcolormesh(t, f[:len(f)//4], 10 * np.log10(Sxx[:len(f)//4]), 
                                      shading='gouraud', cmap='viridis')
        self.ax_spectrogram.set_ylabel('Frekans (Hz)')
        self.ax_spectrogram.set_title("ğŸ“Š Spektogram")
        
        # Diyarizasyon placeholder
        self.ax_diarization.clear()
        self.ax_diarization.text(0.5, 0.5, "ğŸ‘¥ Diyarizasyon iÃ§in analiz yapÄ±n", 
                   horizontalalignment='center', verticalalignment='center',
                   fontsize=10, color='gray', transform=self.ax_diarization.transAxes)
        self.ax_diarization.set_xlabel("Zaman (saniye)")
        self.ax_diarization.set_title("KonuÅŸmacÄ± Diyarizasyonu")
        
        self.fig.tight_layout()
        self.canvas.draw()
        
        # DetaylÄ± spektogram
        self.ax_spec_detail.clear()
        f_detail, t_detail, Sxx_detail = signal.spectrogram(audio_data, self.sample_rate, nperseg=2048)
        im = self.ax_spec_detail.pcolormesh(t_detail, f_detail, 10 * np.log10(Sxx_detail), 
                                           shading='gouraud', cmap='plasma')
        self.ax_spec_detail.set_ylabel('Frekans (Hz)')
        self.ax_spec_detail.set_xlabel('Zaman (saniye)')
        self.ax_spec_detail.set_title('ğŸµ DetaylÄ± Spektogram')
        plt.colorbar(im, ax=self.ax_spec_detail, label='GÃ¼Ã§ (dB)')
        self.canvas2.draw()
        
    def update_waveform_plot(self, audio_data):
        """BasitÃ§e dalga formunu gÃ¼ncelle"""
        self.update_comprehensive_plots(audio_data)
    
    def toggle_recording(self):
        """KaydÄ± baÅŸlat/durdur"""
        if not self.is_recording:
            # KayÄ±t sÃ¼resini al
            try:
                recording_seconds = int(self.duration_var.get())
                if recording_seconds <= 0:
                    messagebox.showerror("Hata", "KayÄ±t sÃ¼resi pozitif bir sayÄ± olmalÄ±dÄ±r!")
                    return
            except ValueError:
                messagebox.showerror("Hata", "GeÃ§erli bir kayÄ±t sÃ¼resi giriniz!")
                return
            
            # KaydÄ± baÅŸlat
            self.record_button.config(text="â¹ï¸ KaydÄ± Durdur")
            self.status_label.config(text="ğŸ”´ KayÄ±t yapÄ±lÄ±yor...")
            self.analyze_button.config(state=tk.DISABLED)
            self.quick_analyze_button.config(state=tk.DISABLED)
            self.recorded_audio = []
            self.is_recording = True
            
            # CanlÄ± analiz baÅŸlat
            if self.enable_live_analysis.get():
                self.start_live_analysis()
            
            # Ä°ÅŸ parÃ§acÄ±ÄŸÄ±nÄ± baÅŸlat
            self.recording_thread = threading.Thread(target=self.record_audio, 
                                                   args=(recording_seconds,))
            self.recording_thread.daemon = True
            self.recording_thread.start()
            
            self.add_log(f"ğŸ™ï¸ {recording_seconds} saniyelik kayÄ±t baÅŸlatÄ±ldÄ±.")
        else:
            # KaydÄ± durdur
            self.is_recording = False
            self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat")
            self.status_label.config(text="â¹ï¸ KayÄ±t durduruldu")
            self.add_log("â¹ï¸ KayÄ±t manuel olarak durduruldu.")
            
            # CanlÄ± analizi durdur
            if self.live_analysis_running:
                self.stop_live_analysis()
    
    def record_audio(self, duration):
        """Basit ve gÃ¼venilir ses kaydÄ±"""
        try:
            self.add_log("ğŸ™ï¸ Basit kayÄ±t sistemi baÅŸlatÄ±lÄ±yor...")
            
            # Ses cihazÄ±nÄ± optimize et
            self.optimize_audio_device()
            
            # Toplam sample sayÄ±sÄ±nÄ± hesapla
            total_samples = int(SAMPLE_RATE * duration)
            self.add_log(f"ğŸ“Š Hedef: {duration}s = {total_samples} sample")
            
            # KayÄ±t iÃ§in buffer
            self.recorded_audio = np.zeros(total_samples, dtype=DTYPE)
            current_frame = 0
            
            # Buffer boyutu - daha kÃ¼Ã§Ã¼k ve gÃ¼venilir
            block_size = 1024  # Sabit 1024 sample bloklar
            
            def audio_callback(indata, frames, time, status):
                nonlocal current_frame
                
                if status:
                    self.add_log(f"âš ï¸ Audio status: {status}")
                
                if self.is_recording and current_frame < total_samples:
                    # KaÃ§ sample alacaÄŸÄ±mÄ±zÄ± hesapla  
                    samples_to_take = min(frames, total_samples - current_frame)
                    
                    # Veriyi direkt kayÄ±t buffer'Ä±na kopyala
                    self.recorded_audio[current_frame:current_frame + samples_to_take] = indata[:samples_to_take, 0]
                    current_frame += samples_to_take
                    
                    # Progress gÃ¼ncelle (her 0.5 saniyede bir)
                    if current_frame % (SAMPLE_RATE // 2) < frames:
                        progress = (current_frame / total_samples) * 100
                        elapsed = current_frame / SAMPLE_RATE
                        self.root.after(1, lambda p=progress, e=elapsed: self.update_recording_progress(p, e, duration))
                    
                    # KayÄ±t tamamlandÄ± mÄ±?
                    if current_frame >= total_samples:
                        self.is_recording = False
            
            self.add_log(f"ğŸ¤ KayÄ±t baÅŸlÄ±yor: {block_size} sample bloklar")
            self.add_log(f"ğŸ”§ Sample rate: {SAMPLE_RATE} Hz")
            self.add_log(f"ğŸ“Š Dtype: {DTYPE}")
            self.add_log(f"ğŸ§ Channels: {CHANNELS}")
            
            # Debug - ses cihazÄ± bilgileri
            try:
                current_device = sd.query_devices(sd.default.device[0])
                self.add_log(f"ğŸ¤ Aktif cihaz: {current_device['name']}")
                self.add_log(f"âš¡ Desteklenen sample rate: {current_device['default_samplerate']}")
            except:
                pass
            
            # Stream baÅŸlat
            with sd.InputStream(
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=DTYPE,
                blocksize=block_size,
                callback=audio_callback,
                latency='low'
            ):
                start_time = time.time()
                
                # KayÄ±t dÃ¶ngÃ¼sÃ¼ - Ã§ok basit
                while self.is_recording and current_frame < total_samples:
                    time.sleep(0.1)  # 100ms bekle
                    
                    # Timeout kontrolÃ¼
                    if time.time() - start_time > duration + 2:  # 2 saniye extra
                        self.add_log("â° KayÄ±t timeout - zorla durduruluyor")
                        break
            
            # KayÄ±t iÅŸlemi bitti
            self.is_recording = False
            self.root.after(1, lambda: self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat"))
            
            # SonuÃ§larÄ± kontrol et
            actual_duration = current_frame / SAMPLE_RATE
            self.add_log(f"âœ… KayÄ±t tamamlandÄ±!")
            self.add_log(f"ğŸ“Š Hedef: {duration}s, GerÃ§ek: {actual_duration:.2f}s")
            self.add_log(f"ğŸ”¢ Sample sayÄ±sÄ±: {current_frame}/{total_samples}")
            
            # Kaydedilen veriyi kes (gereksiz sÄ±fÄ±rlarÄ± temizle)
            if current_frame > 0:
                self.recorded_audio = self.recorded_audio[:current_frame]
                self.sample_rate = SAMPLE_RATE
                
                # Ses seviyesi analizi
                max_amplitude = np.max(np.abs(self.recorded_audio))
                avg_amplitude = np.mean(np.abs(self.recorded_audio))
                
                self.add_log(f"ğŸ”Š Max seviye: {max_amplitude:.4f}")
                self.add_log(f"ğŸ“ˆ Ortalama seviye: {avg_amplitude:.4f}")
                
                # Normalize et (sadece gerekirse)
                if max_amplitude > 0.95:
                    self.recorded_audio = self.recorded_audio / max_amplitude * 0.9
                    self.add_log("ğŸ”§ Ses seviyesi normalize edildi")
                
                # Grafikleri gÃ¼ncelle
                self.root.after(1, lambda: self.update_comprehensive_plots(self.recorded_audio))
                self.root.after(1, lambda: self.status_label.config(text="âœ… KayÄ±t tamamlandÄ±"))
                
                # Dosyaya kaydet
                self.save_recording()
                
                # DÃ¼ÄŸmeleri etkinleÅŸtir
                self.root.after(1, lambda: self.analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.quick_analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.report_button.config(state=tk.NORMAL))
            else:
                self.add_log("âŒ HiÃ§ ses verisi alÄ±namadÄ±!")
                self.root.after(1, lambda: self.status_label.config(text="âŒ KayÄ±t baÅŸarÄ±sÄ±z"))
                
        except Exception as e:
            self.add_log(f"âŒ Callback kayÄ±t hatasÄ±: {e}")
            self.add_log("ğŸ”„ Fallback basit kayÄ±t yÃ¶ntemi deneniyor...")
            
            # Fallback - basit kayÄ±t yÃ¶ntemi
            self.is_recording = True  # Reset iÃ§in
            success = self.record_audio_simple(duration)
            
            if not success:
                self.is_recording = False
                self.root.after(1, lambda: self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat"))
                self.root.after(1, lambda: self.status_label.config(text="âŒ TÃ¼m kayÄ±t yÃ¶ntemleri baÅŸarÄ±sÄ±z!"))
        finally:
            # CanlÄ± analizi durdur
            if self.live_analysis_running:
                self.stop_live_analysis()
    
    def save_recording(self):
        """KaydÄ± yÃ¼ksek kaliteli WAV dosyasÄ±na kaydet"""
        try:
            filename = self.filename_var.get()
            if not filename.endswith(".wav"):
                filename += ".wav"
            
            # Soundfile kullanarak yÃ¼ksek kaliteli kayÄ±t
            # (wave modÃ¼lÃ¼nden daha iyi kalite)
            sf.write(filename, self.recorded_audio, SAMPLE_RATE, 
                    subtype='PCM_24')  # 24-bit yÃ¼ksek kalite
            
            self.current_audio_file = filename
            
            # Dosya bilgilerini gÃ¶ster
            file_size = os.path.getsize(filename) / (1024 * 1024)  # MB
            duration = len(self.recorded_audio) / SAMPLE_RATE
            
            self.add_log(f"ğŸ’¾ KayÄ±t '{filename}' dosyasÄ±na kaydedildi.")
            self.add_log(f"ğŸ“Š Dosya boyutu: {file_size:.2f} MB, SÃ¼re: {duration:.2f}s")
            self.add_log(f"ğŸµ Format: 24-bit PCM, {SAMPLE_RATE} Hz, {CHANNELS} kanal")
            
        except Exception as e:
            self.add_log(f"âŒ Dosya kaydetme hatasÄ±: {e}")
            # Fallback - basic wave format
            try:
                filename = self.filename_var.get()
                if not filename.endswith(".wav"):
                    filename += ".wav"
                    
                int_data = np.int16(self.recorded_audio * 32767)
                with wave.open(filename, 'wb') as wf:
                    wf.setnchannels(CHANNELS)
                    wf.setsampwidth(2)
                    wf.setframerate(SAMPLE_RATE)
                    wf.writeframes(int_data.tobytes())
                
                self.current_audio_file = filename
                self.add_log(f"ğŸ’¾ Fallback kayÄ±t baÅŸarÄ±lÄ±: {filename}")
            except Exception as e2:
                self.add_log(f"âŒ Fallback kayÄ±t da baÅŸarÄ±sÄ±z: {e2}")
    
    def load_models(self):
        """Analiz modellerini yÃ¼kle"""
        if self.models_loaded:
            return True
            
        try:
            self.add_log("ğŸ”„ Modeller yÃ¼kleniyor...")
            self.status_label.config(text="ğŸ”„ Modeller yÃ¼kleniyor...")
            
            # GPU kullanÄ±mÄ±nÄ± kontrol et
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.add_log(f"ğŸ’» Cihaz: {device}")
            
            # Diyarizasyon pipeline'Ä±
            if 'diarization' not in self.pipelines:
                self.pipelines['diarization'] = Pipeline.from_pretrained(
                    DIARIZATION_MODEL, use_auth_token=HF_TOKEN
                )
                self.pipelines['diarization'].to(device)
                self.add_log("âœ… Diyarizasyon modeli yÃ¼klendi")
            
            # VAD pipeline'Ä±
            if self.enable_vad.get() and 'vad' not in self.pipelines:
                try:
                    self.pipelines['vad'] = Pipeline.from_pretrained(
                        VAD_MODEL, use_auth_token=HF_TOKEN
                    )
                    self.pipelines['vad'].to(device)
                    self.add_log("âœ… Ses Aktivitesi Tespiti modeli yÃ¼klendi")
                except Exception as e:
                    self.add_log(f"âš ï¸ VAD modeli yÃ¼klenemedi: {e}")
            
            # Ã–rtÃ¼ÅŸen konuÅŸma tespiti pipeline'Ä±
            if self.enable_overlap.get() and 'overlap' not in self.pipelines:
                try:
                    self.pipelines['overlap'] = Pipeline.from_pretrained(
                        OVERLAP_MODEL, use_auth_token=HF_TOKEN
                    )
                    self.pipelines['overlap'].to(device)
                    self.add_log("âœ… Ã–rtÃ¼ÅŸen KonuÅŸma Tespiti modeli yÃ¼klendi")
                except Exception as e:
                    self.add_log(f"âš ï¸ Ã–rtÃ¼ÅŸme modeli yÃ¼klenemedi: {e}")
            
            self.models_loaded = True
            self.add_log("ğŸ‰ TÃ¼m modeller baÅŸarÄ±yla yÃ¼klendi!")
            return True
            
        except Exception as e:
            self.add_log(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            messagebox.showerror("Model HatasÄ±", f"Modeller yÃ¼klenirken hata oluÅŸtu:\n{e}")
            return False
    
    def quick_analyze(self):
        """HÄ±zlÄ± analiz yap (sadece diyarizasyon)"""
        if not self.current_audio_file and len(self.recorded_audio) == 0:
            messagebox.showwarning("UyarÄ±", "Ã–nce ses kaydÄ± yapÄ±n veya dosya yÃ¼kleyin!")
            return
            
        self.add_log("âš¡ HÄ±zlÄ± analiz baÅŸlatÄ±lÄ±yor...")
        self.status_label.config(text="âš¡ HÄ±zlÄ± analiz yapÄ±lÄ±yor...")
        self.quick_analyze_button.config(state=tk.DISABLED)
        
        # Sadece diyarizasyon iÃ§in diÄŸer seÃ§enekleri geÃ§ici olarak kapat
        original_states = {
            'vad': self.enable_vad.get(),
            'overlap': self.enable_overlap.get(),
            'emotion': self.enable_emotion.get(),
            'noise': self.enable_noise_reduction.get(),
            'separation': self.enable_separation.get()
        }
        
        # HÄ±zlÄ± analiz iÃ§in seÃ§enekleri kapat
        self.enable_vad.set(False)
        self.enable_overlap.set(False)
        self.enable_emotion.set(False)
        self.enable_noise_reduction.set(False)
        self.enable_separation.set(False)
        
        # Analizi baÅŸlat
        filename = self.current_audio_file or self.get_current_filename()
        threading.Thread(target=self.run_analysis, args=(filename, True, original_states)).start()
    
    def analyze_recording(self):
        """Tam analiz yap"""
        if not self.current_audio_file and len(self.recorded_audio) == 0:
            messagebox.showwarning("UyarÄ±", "Ã–nce ses kaydÄ± yapÄ±n veya dosya yÃ¼kleyin!")
            return
            
        filename = self.current_audio_file or self.get_current_filename()
        
        if not os.path.exists(filename):
            messagebox.showerror("Hata", f"'{filename}' dosyasÄ± bulunamadÄ±!")
            return
        
        self.add_log("ğŸ” Tam analiz baÅŸlatÄ±lÄ±yor...")
        self.status_label.config(text="ğŸ” Tam analiz yapÄ±lÄ±yor...")
        self.analyze_button.config(state=tk.DISABLED)
        
        # Ä°ÅŸ parÃ§acÄ±ÄŸÄ±nÄ± baÅŸlat
        threading.Thread(target=self.run_analysis, args=(filename,)).start()
    
    def get_current_filename(self):
        """GeÃ§erli dosya adÄ±nÄ± al"""
        filename = self.filename_var.get()
        if not filename.endswith(".wav"):
            filename += ".wav"
        return filename
    
    def run_analysis(self, audio_file, is_quick=False, restore_states=None):
        """KapsamlÄ± ses analizi yap"""
        try:
            # Modelleri yÃ¼kle
            if not self.load_models():
                return
            
            start_time = time.time()
            
            # Ses dosyasÄ±nÄ± yÃ¼kle
            self.add_log(f"ğŸ“‚ Ses dosyasÄ± yÃ¼kleniyor: {os.path.basename(audio_file)}")
            audio_data, sample_rate = librosa.load(audio_file, sr=None)
            
            # GÃ¼rÃ¼ltÃ¼ azaltma
            if self.enable_noise_reduction.get():
                self.add_log("ğŸ”‡ GÃ¼rÃ¼ltÃ¼ azaltma iÅŸlemi baÅŸlatÄ±lÄ±yor...")
                audio_data = self.apply_noise_reduction(audio_data, sample_rate)
                self.noise_reduced_audio = audio_data
            
            # 1. Voice Activity Detection (VAD)
            if self.enable_vad.get():
                self.add_log("ğŸ¯ Ses Aktivitesi Tespiti yapÄ±lÄ±yor...")
                self.vad_result = self.run_vad_analysis(audio_file)
                
            # 2. Diyarizasyon
            self.add_log("ğŸ‘¥ KonuÅŸmacÄ± diyarizasyonu yapÄ±lÄ±yor...")
            self.diarization_result = self.run_diarization(audio_file)
            
            # 3. Ã–rtÃ¼ÅŸen konuÅŸma tespiti
            if self.enable_overlap.get():
                self.add_log("ğŸ—£ï¸ Ã–rtÃ¼ÅŸen konuÅŸma tespiti yapÄ±lÄ±yor...")
                self.overlap_result = self.run_overlap_detection(audio_file)
            
            # 4. Ses ayrÄ±ÅŸtÄ±rma
            if self.enable_separation.get():
                self.add_log("ğŸ¼ Ses ayrÄ±ÅŸtÄ±rma iÅŸlemi yapÄ±lÄ±yor...")
                self.run_speech_separation(audio_file)
            
            # 5. KonuÅŸmacÄ± tanÄ±ma ve embedding'ler
            self.add_log("ğŸ” KonuÅŸmacÄ± embedding'leri Ã§Ä±karÄ±lÄ±yor...")
            self.extract_speaker_embeddings(audio_file, self.diarization_result)
            
            # 6. Transkripsiyon
            self.add_log("ğŸ“ KonuÅŸma transkripte ediliyor...")
            self.run_speaker_based_transcription(audio_file, self.diarization_result)
            
            # 7. Duygu analizi
            if self.enable_emotion.get():
                self.add_log("ğŸ˜Š Duygu analizi yapÄ±lÄ±yor...")
                self.emotion_result = self.run_ml_emotion_analysis(audio_data, sample_rate)
            
            # 8. GÃ¶rselleÅŸtirmeleri gÃ¼ncelle
            self.update_advanced_visualizations(audio_data, sample_rate)
            
            # 9. DetaylÄ± rapor oluÅŸtur
            self.generate_detailed_analysis_report()
            
            # SÃ¼re hesapla
            end_time = time.time()
            total_time = end_time - start_time
            
            self.add_log(f"ğŸ‰ Analiz tamamlandÄ±! Toplam sÃ¼re: {total_time:.2f} saniye")
            self.status_label.config(text="âœ… Analiz tamamlandÄ±")
            
            # HÄ±zlÄ± analiz durumunu geri yÃ¼kle
            if is_quick and restore_states:
                for key, value in restore_states.items():
                    if key == 'vad':
                        self.enable_vad.set(value)
                    elif key == 'overlap':
                        self.enable_overlap.set(value)
                    elif key == 'emotion':
                        self.enable_emotion.set(value)
                    elif key == 'noise':
                        self.enable_noise_reduction.set(value)
                    elif key == 'separation':
                        self.enable_separation.set(value)
            
        except Exception as e:
            self.add_log(f"âŒ Analiz hatasÄ±: {e}")
            self.status_label.config(text="âŒ Analiz hatasÄ±!")
            messagebox.showerror("Analiz HatasÄ±", f"Analiz sÄ±rasÄ±nda hata oluÅŸtu:\n{e}")
        finally:
            # DÃ¼ÄŸmeleri yeniden etkinleÅŸtir
            self.analyze_button.config(state=tk.NORMAL)
            self.quick_analyze_button.config(state=tk.NORMAL)
    
    def run_diarization(self, audio_file):
        """PyAnnote diyarizasyon iÅŸlemini Ã§alÄ±ÅŸtÄ±r"""
        try:
            # Diyarizasyon uygula
            diarization = self.pipelines['diarization'](audio_file)
            
            # SonuÃ§larÄ± iÅŸle ve gÃ¶ster
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"ğŸ¯ Diyarizasyon SonuÃ§larÄ±: {os.path.basename(audio_file)}\n")
            self.result_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # KonuÅŸmacÄ±larÄ± ve zaman aralÄ±klarÄ±nÄ± gÃ¶ster
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                start = turn.start
                end = turn.end
                duration = end - start
                speaker_segments.append({
                    'speaker': speaker,
                    'start': start,
                    'end': end,
                    'duration': duration
                })
                result_line = f"ğŸ‘¤ {speaker}: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n"
                self.result_text.insert(tk.END, result_line)
            
            # KonuÅŸmacÄ± istatistiklerini hesapla
            speaker_stats = {}
            total_speech_time = 0
            for segment in speaker_segments:
                speaker = segment['speaker']
                duration = segment['duration']
                if speaker not in speaker_stats:
                    speaker_stats[speaker] = {'total_time': 0, 'segment_count': 0}
                speaker_stats[speaker]['total_time'] += duration
                speaker_stats[speaker]['segment_count'] += 1
                total_speech_time += duration
            
            # Ä°statistikleri gÃ¶ster
            self.result_text.insert(tk.END, "\nğŸ“Š KonuÅŸmacÄ± Ä°statistikleri:\n")
            self.result_text.insert(tk.END, f"ğŸ“ˆ Toplam konuÅŸma sÃ¼resi: {total_speech_time:.2f} saniye\n")
            self.result_text.insert(tk.END, f"ğŸ‘¥ Toplam konuÅŸmacÄ± sayÄ±sÄ±: {len(speaker_stats)}\n\n")
            
            for speaker, stats in speaker_stats.items():
                percentage = (stats['total_time'] / total_speech_time) * 100 if total_speech_time > 0 else 0
                self.result_text.insert(tk.END, 
                    f"{speaker}:\n"
                    f"  â±ï¸ SÃ¼re: {stats['total_time']:.2f}s ({percentage:.1f}%)\n"
                    f"  ğŸ’¬ Segment sayÄ±sÄ±: {stats['segment_count']}\n"
                    f"  ğŸ“ Ortalama segment: {stats['total_time']/stats['segment_count']:.2f}s\n\n"
                )
            
            self.add_log("âœ… Diyarizasyon tamamlandÄ±.")
            return diarization
            
        except Exception as e:
            self.add_log(f"âŒ Diyarizasyon hatasÄ±: {e}")
            return None
    
    def apply_noise_reduction(self, audio_data, sample_rate):
        """GÃ¼rÃ¼ltÃ¼ azaltma uygula"""
        try:
            # Ã–nce ses verisini temizle
            audio_data = self.clean_audio_buffer(audio_data)
            
            # NoiseReduce kÃ¼tÃ¼phanesi ile gÃ¼rÃ¼ltÃ¼ azaltma (daha muhafazakar)
            reduced_noise = nr.reduce_noise(y=audio_data, sr=sample_rate, prop_decrease=0.6)  # 0.8'den 0.6'ya dÃ¼ÅŸÃ¼rdÃ¼k
            
            # Sonucu tekrar temizle
            reduced_noise = self.clean_audio_buffer(reduced_noise)
            
            self.add_log("âœ… GÃ¼rÃ¼ltÃ¼ azaltma tamamlandÄ±")
            return reduced_noise
        except Exception as e:
            self.add_log(f"âš ï¸ GÃ¼rÃ¼ltÃ¼ azaltma hatasÄ±: {e}")
            # Hata durumunda temizlenmiÅŸ orijinal veriyi dÃ¶ndÃ¼r
            return self.clean_audio_buffer(audio_data)
    
    def run_vad_analysis(self, audio_file):
        """Ses Aktivitesi Tespiti (VAD) yap"""
        try:
            vad_result = self.pipelines['vad'](audio_file)
            
            self.analysis_text.delete(1.0, tk.END)
            self.analysis_text.insert(tk.END, f"ğŸ¯ Ses Aktivitesi Tespiti SonuÃ§larÄ±\n")
            self.analysis_text.insert(tk.END, f"ğŸ“‚ Dosya: {os.path.basename(audio_file)}\n\n")
            
            total_speech = 0
            speech_segments = []
            
            for segment in vad_result.get_timeline():
                start = segment.start
                end = segment.end
                duration = end - start
                total_speech += duration
                speech_segments.append({'start': start, 'end': end, 'duration': duration})
                
                self.analysis_text.insert(tk.END, f"ğŸ—£ï¸ KonuÅŸma: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n")
            
            # Ä°statistikler
            audio_duration = librosa.get_duration(filename=audio_file)
            speech_ratio = (total_speech / audio_duration) * 100 if audio_duration > 0 else 0
            silence_duration = audio_duration - total_speech
            
            self.analysis_text.insert(tk.END, f"\nğŸ“Š VAD Ä°statistikleri:\n")
            self.analysis_text.insert(tk.END, f"ğŸ“ˆ Toplam ses sÃ¼resi: {audio_duration:.2f}s\n")
            self.analysis_text.insert(tk.END, f"ğŸ—£ï¸ KonuÅŸma sÃ¼resi: {total_speech:.2f}s ({speech_ratio:.1f}%)\n")
            self.analysis_text.insert(tk.END, f"ğŸ¤« Sessizlik sÃ¼resi: {silence_duration:.2f}s ({100-speech_ratio:.1f}%)\n")
            self.analysis_text.insert(tk.END, f"ğŸ“ KonuÅŸma segment sayÄ±sÄ±: {len(speech_segments)}\n")
            
            if speech_segments:
                avg_segment = total_speech / len(speech_segments)
                self.analysis_text.insert(tk.END, f"â±ï¸ Ortalama segment sÃ¼resi: {avg_segment:.2f}s\n")
            
            self.add_log("âœ… VAD analizi tamamlandÄ±")
            return vad_result
            
        except Exception as e:
            self.add_log(f"âŒ VAD analizi hatasÄ±: {e}")
            return None
    
    def run_overlap_detection(self, audio_file):
        """Ã–rtÃ¼ÅŸen konuÅŸma tespiti yap"""
        try:
            overlap_result = self.pipelines['overlap'](audio_file)
            
            self.analysis_text.insert(tk.END, f"\nğŸ—£ï¸ Ã–rtÃ¼ÅŸen KonuÅŸma Tespiti SonuÃ§larÄ±\n")
            self.analysis_text.insert(tk.END, f"ğŸ“‚ Dosya: {os.path.basename(audio_file)}\n\n")
            
            total_overlap = 0
            overlap_segments = []
            
            for segment in overlap_result.get_timeline():
                start = segment.start
                end = segment.end
                duration = end - start
                total_overlap += duration
                overlap_segments.append({'start': start, 'end': end, 'duration': duration})
                
                self.analysis_text.insert(tk.END, f"ğŸ”„ Ã–rtÃ¼ÅŸme: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n")
            
            # Ä°statistikler
            audio_duration = librosa.get_duration(filename=audio_file)
            overlap_ratio = (total_overlap / audio_duration) * 100 if audio_duration > 0 else 0
            
            self.analysis_text.insert(tk.END, f"\nğŸ“Š Ã–rtÃ¼ÅŸme Ä°statistikleri:\n")
            self.analysis_text.insert(tk.END, f"ğŸ”„ Toplam Ã¶rtÃ¼ÅŸme sÃ¼resi: {total_overlap:.2f}s ({overlap_ratio:.1f}%)\n")
            self.analysis_text.insert(tk.END, f"ğŸ“ Ã–rtÃ¼ÅŸme segment sayÄ±sÄ±: {len(overlap_segments)}\n")
            
            if overlap_segments:
                avg_overlap = total_overlap / len(overlap_segments)
                self.analysis_text.insert(tk.END, f"â±ï¸ Ortalama Ã¶rtÃ¼ÅŸme sÃ¼resi: {avg_overlap:.2f}s\n")
            
            self.add_log("âœ… Ã–rtÃ¼ÅŸme tespiti tamamlandÄ±")
            return overlap_result
            
        except Exception as e:
            self.add_log(f"âŒ Ã–rtÃ¼ÅŸme tespiti hatasÄ±: {e}")
            return None
    
    def extract_speaker_embeddings(self, audio_file, diarization):
        """KonuÅŸmacÄ± embedding'lerini Ã§Ä±kar"""
        try:
            if not diarization:
                return
                
            self.add_log("ğŸ” KonuÅŸmacÄ± embedding'leri Ã§Ä±karÄ±lÄ±yor...")
            
            # Ses dosyasÄ±nÄ± yÃ¼kle
            audio_data, sample_rate = librosa.load(audio_file, sr=16000)  # 16kHz
            
            # Her konuÅŸmacÄ± iÃ§in embedding Ã§Ä±kar
            embeddings = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                start_sample = int(turn.start * sample_rate)
                end_sample = int(turn.end * sample_rate)
                
                # Segment ses verisini al
                segment_audio = audio_data[start_sample:end_sample]
                
                # Basit Ã¶zellik Ã§Ä±karma (gerÃ§ek projede daha geliÅŸmiÅŸ yÃ¶ntemler kullanÄ±lÄ±r)
                features = {
                    'mfcc_mean': np.mean(librosa.feature.mfcc(y=segment_audio, sr=sample_rate)),
                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=segment_audio, sr=sample_rate)),
                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(segment_audio)),
                    'duration': turn.end - turn.start
                }
                
                if speaker not in embeddings:
                    embeddings[speaker] = []
                embeddings[speaker].append(features)
            
            # Ortalama embedding'leri hesapla
            for speaker in embeddings:
                speaker_features = embeddings[speaker]
                avg_features = {
                    'mfcc_mean': np.mean([f['mfcc_mean'] for f in speaker_features]),
                    'spectral_centroid': np.mean([f['spectral_centroid'] for f in speaker_features]),
                    'zero_crossing_rate': np.mean([f['zero_crossing_rate'] for f in speaker_features]),
                    'total_duration': sum([f['duration'] for f in speaker_features])
                }
                self.speaker_embeddings[speaker] = avg_features
            
            self.add_log("âœ… KonuÅŸmacÄ± embedding'leri Ã§Ä±karÄ±ldÄ±")
            
        except Exception as e:
            self.add_log(f"âŒ Embedding Ã§Ä±karma hatasÄ±: {e}")
    
    def run_speech_separation(self, audio_file):
        """Ses ayrÄ±ÅŸtÄ±rma iÅŸlemi yap"""
        try:
            self.add_log("ğŸ¼ Ses ayrÄ±ÅŸtÄ±rma baÅŸlatÄ±lÄ±yor...")
            # Bu Ã¶zellik iÃ§in geliÅŸmiÅŸ modeller gerekir
            # Åu an iÃ§in basit bir placeholder
            self.add_log("âš ï¸ Ses ayrÄ±ÅŸtÄ±rma Ã¶zelliÄŸi geliÅŸtirme aÅŸamasÄ±nda")
            
        except Exception as e:
            self.add_log(f"âŒ Ses ayrÄ±ÅŸtÄ±rma hatasÄ±: {e}")
    
    def extract_advanced_audio_features(self, audio_data, sample_rate):
        """GeliÅŸmiÅŸ ses Ã¶zelliklerini Ã§Ä±kar"""
        try:
            self.add_log("ğŸ” GeliÅŸmiÅŸ ses Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor...")
            
            # Ses verisini temizle ve kontrol et
            audio_data = self.clean_audio_buffer(audio_data)
            
            # Ses verisi boÅŸ veya Ã§ok kÄ±sa mÄ± kontrol et
            if len(audio_data) < 1024:  # Minimum 1024 sample
                self.add_log("âŒ Ses verisi Ã§ok kÄ±sa, varsayÄ±lan Ã¶zellikler dÃ¶ndÃ¼rÃ¼lÃ¼yor")
                return self.get_default_features()
            
            # Temel Ã¶zellikler
            mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=20)
            spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
            zero_crossing_rates = librosa.feature.zero_crossing_rate(audio_data)
            
            # GeliÅŸmiÅŸ spektral Ã¶zellikler
            chroma = librosa.feature.chroma(y=audio_data, sr=sample_rate)
            mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)
            tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sample_rate)
            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sample_rate)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sample_rate)
            spectral_flatness = librosa.feature.spectral_flatness(y=audio_data)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)
            
            # Prozodik Ã¶zellikler
            try:
                tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sample_rate)
                speaking_rate = len(beats) / (len(audio_data) / sample_rate)  # konuÅŸma hÄ±zÄ±
            except:
                tempo = 0
                speaking_rate = 0
            
            # Pitch analizi
            pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sample_rate)
            pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0
            pitch_std = np.std(pitches[pitches > 0]) if np.any(pitches > 0) else 0
            
            # Enerji ve gÃ¼Ã§ Ã¶zellikleri
            energy = np.mean(np.square(audio_data))
            rms_energy = np.mean(librosa.feature.rms(y=audio_data))
            
            # Formant analizi (basitleÅŸtirilmiÅŸ)
            # GerÃ§ek formant analizi iÃ§in daha karmaÅŸÄ±k algoritma gerekir
            stft = librosa.stft(audio_data)
            spectral_magnitude = np.abs(stft)
            formant_frequencies = []
            for frame in spectral_magnitude.T:
                peaks, _ = librosa.util.peak_pick(frame, pre_max=3, post_max=3, pre_avg=3, post_avg=5, delta=0.1, wait=10)
                if len(peaks) >= 2:
                    # Ä°lk iki formant yaklaÅŸÄ±mÄ±
                    formant_frequencies.append(peaks[:2])
            
            # Sessizlik analizi
            silence_threshold = 0.01
            silence_frames = np.where(np.abs(audio_data) < silence_threshold)[0]
            silence_ratio = len(silence_frames) / len(audio_data)
            
            # Vurgu ve tonlama (prosody) Ã¶zellikleri
            # KÄ±sa-dÃ¶nem enerji deÄŸiÅŸimleri
            frame_length = int(0.025 * sample_rate)  # 25ms
            hop_length = int(0.01 * sample_rate)     # 10ms
            
            short_time_energy = []
            for i in range(0, len(audio_data) - frame_length, hop_length):
                frame = audio_data[i:i + frame_length]
                energy_frame = np.sum(frame ** 2)
                short_time_energy.append(energy_frame)
            
            energy_variance = np.var(short_time_energy)
            energy_mean = np.mean(short_time_energy)
            
            # Ã–zellik sÃ¶zlÃ¼ÄŸÃ¼ - GÃ¼venli hesaplama
            features = {}
            
            # GÃ¼venli Ã¶zellik hesaplama fonksiyonu
            def safe_calc(func, default_val=0.0):
                try:
                    result = func()
                    if np.isfinite(result):
                        return float(result)
                    else:
                        return default_val
                except:
                    return default_val
            
            # Temel Ã¶zellikler
            features['mfcc_mean'] = safe_calc(lambda: np.mean(mfccs), 0.0)
            features['mfcc_std'] = safe_calc(lambda: np.std(mfccs), 1.0)
            features['spectral_centroid_mean'] = safe_calc(lambda: np.mean(spectral_centroids), 1000.0)
            features['spectral_centroid_std'] = safe_calc(lambda: np.std(spectral_centroids), 500.0)
            features['zcr_mean'] = safe_calc(lambda: np.mean(zero_crossing_rates), 0.05)
            features['zcr_std'] = safe_calc(lambda: np.std(zero_crossing_rates), 0.02)
            
            # GeliÅŸmiÅŸ spektral Ã¶zellikler
            features['chroma_mean'] = safe_calc(lambda: np.mean(chroma), 0.1)
            features['chroma_std'] = safe_calc(lambda: np.std(chroma), 0.05)
            features['mel_spectrogram_mean'] = safe_calc(lambda: np.mean(mel_spectrogram), 0.01)
            features['tonnetz_mean'] = safe_calc(lambda: np.mean(tonnetz), 0.0)
            features['spectral_contrast_mean'] = safe_calc(lambda: np.mean(spectral_contrast), 10.0)
            features['spectral_bandwidth_mean'] = safe_calc(lambda: np.mean(spectral_bandwidth), 1500.0)
            features['spectral_flatness_mean'] = safe_calc(lambda: np.mean(spectral_flatness), 0.1)
            features['spectral_rolloff_mean'] = safe_calc(lambda: np.mean(spectral_rolloff), 2000.0)
            
            # Prozodik Ã¶zellikler
            features['tempo'] = safe_calc(lambda: tempo, 100.0)
            features['speaking_rate'] = safe_calc(lambda: speaking_rate, 2.0)
            features['pitch_mean'] = safe_calc(lambda: pitch_mean, 200.0)
            features['pitch_std'] = safe_calc(lambda: pitch_std, 50.0)
            features['pitch_range'] = safe_calc(lambda: pitch_std / pitch_mean if pitch_mean > 0 else 0, 0.25)
            
            # Enerji Ã¶zellikleri
            features['energy'] = safe_calc(lambda: energy, 0.001)
            features['rms_energy'] = safe_calc(lambda: rms_energy, 0.01)
            features['energy_variance'] = safe_calc(lambda: energy_variance, 0.0001)
            features['energy_mean'] = safe_calc(lambda: energy_mean, 0.001)
            features['energy_dynamic_range'] = safe_calc(
                lambda: np.max(short_time_energy) - np.min(short_time_energy) if len(short_time_energy) > 0 else 0, 
                0.005
            )
            
            # Sessizlik ve duraklama
            features['silence_ratio'] = safe_calc(lambda: silence_ratio, 0.3)
            features['voice_activity_ratio'] = safe_calc(lambda: 1 - silence_ratio, 0.7)
            
            # Harmonik Ã¶zellikler
            features['harmonic_mean'] = safe_calc(lambda: np.mean(librosa.effects.harmonic(audio_data)), 0.001)
            features['percussive_mean'] = safe_calc(lambda: np.mean(librosa.effects.percussive(audio_data)), 0.001)
            
            self.add_log(f"âœ… {len(features)} geliÅŸmiÅŸ Ã¶zellik Ã§Ä±karÄ±ldÄ±")
            return features
            
        except Exception as e:
            self.add_log(f"âŒ GeliÅŸmiÅŸ Ã¶zellik Ã§Ä±karma hatasÄ±: {e}")
            return {}
    
    def run_emotion_analysis(self, audio_data, sample_rate):
        """Duygu analizi yap"""
        try:
            self.add_log("ğŸ˜Š Duygu analizi baÅŸlatÄ±lÄ±yor...")
            
            # Ses verisini temizle
            audio_data = self.clean_audio_buffer(audio_data)
            
            # Basit ses Ã¶zellik analizi ile duygu tahmini
            # GerÃ§ek projede daha geliÅŸmiÅŸ modeller kullanÄ±lÄ±r
            
            # Ses Ã¶zelliklerini gÃ¼venli ÅŸekilde Ã§Ä±kar
            try:
                mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)
                spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)
                zero_crossing_rates = librosa.feature.zero_crossing_rate(audio_data)
                spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)
            except Exception as e:
                self.add_log(f"âš ï¸ Temel Ã¶zellik Ã§Ä±karma hatasÄ±: {e}")
                return self.get_default_emotion_scores()
            
            # Basit kural tabanlÄ± duygu tahmini - GÃ¼venli hesaplama
            try:
                energy = np.mean(np.square(audio_data))
                if not np.isfinite(energy):
                    energy = 0.001
            except:
                energy = 0.001
                
            try:
                pitch_mean = np.mean(spectral_centroids)
                if not np.isfinite(pitch_mean):
                    pitch_mean = 1000.0
            except:
                pitch_mean = 1000.0
                
            try:
                zcr_mean = np.mean(zero_crossing_rates)
                if not np.isfinite(zcr_mean):
                    zcr_mean = 0.05
            except:
                zcr_mean = 0.05
            
            # Duygu sÄ±nÄ±flandÄ±rmasÄ± (basitleÅŸtirilmiÅŸ)
            emotion_scores = {
                'mutlu': 0.0,
                'Ã¼zgÃ¼n': 0.0,
                'kÄ±zgÄ±n': 0.0,
                'sakin': 0.0,
                'heyecanlÄ±': 0.0,
                'stresli': 0.0
            }
            
            # Basit kurallar
            if energy > 0.01 and pitch_mean > 2000:
                emotion_scores['heyecanlÄ±'] += 0.3
                emotion_scores['mutlu'] += 0.2
            elif energy < 0.005:
                emotion_scores['sakin'] += 0.3
                emotion_scores['Ã¼zgÃ¼n'] += 0.2
            
            if zcr_mean > 0.1:
                emotion_scores['stresli'] += 0.2
                emotion_scores['kÄ±zgÄ±n'] += 0.1
            
            # Normalize et
            total_score = sum(emotion_scores.values())
            if total_score > 0:
                for emotion in emotion_scores:
                    emotion_scores[emotion] = emotion_scores[emotion] / total_score
            else:
                # VarsayÄ±lan deÄŸerler
                emotion_scores['sakin'] = 0.6
                emotion_scores['mutlu'] = 0.4
            
            # SonuÃ§larÄ± gÃ¶ster
            self.emotion_text.delete(1.0, tk.END)
            self.emotion_text.insert(tk.END, f"ğŸ˜Š Duygu Analizi SonuÃ§larÄ±\n")
            self.emotion_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # DuygularÄ± sÄ±ralÄ± gÃ¶ster
            sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)
            
            for emotion, score in sorted_emotions:
                percentage = score * 100
                bar_length = int(percentage / 5)  # 5% per character
                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                
                self.emotion_text.insert(tk.END, f"{emotion.capitalize()}: {percentage:.1f}% {bar}\n")
            
            # Dominant duygu
            dominant_emotion = sorted_emotions[0][0]
            self.emotion_text.insert(tk.END, f"\nğŸ¯ BaskÄ±n Duygu: {dominant_emotion.capitalize()}\n")
            
            # Ses Ã¶zellikleri
            self.emotion_text.insert(tk.END, f"\nğŸ“Š Ses Ã–zellikleri:\n")
            self.emotion_text.insert(tk.END, f"âš¡ Enerji: {energy:.4f}\n")
            self.emotion_text.insert(tk.END, f"ğŸµ Ortalama Perde: {pitch_mean:.1f} Hz\n")
            self.emotion_text.insert(tk.END, f"ğŸŒŠ Zero Crossing Rate: {zcr_mean:.3f}\n")
            
            self.add_log("âœ… Duygu analizi tamamlandÄ±")
            return emotion_scores
            
        except Exception as e:
            self.add_log(f"âŒ Duygu analizi hatasÄ±: {e}")
            return None
    
    def run_speaker_based_transcription(self, audio_file, diarization):
        """KonuÅŸmacÄ± bazlÄ± transkripsiyon yap"""
        try:
            self.add_log("KonuÅŸmacÄ± bazlÄ± transkripsiyon baÅŸlatÄ±lÄ±yor...")
            
            # SonuÃ§larÄ± gÃ¶ster
            self.transcript_text.delete(1.0, tk.END)
            self.transcript_text.insert(tk.END, f"KonuÅŸma Ä°Ã§eriÄŸi: {audio_file}\n")
            self.transcript_text.insert(tk.END, f"Model: {GPT_MODEL}\n\n")
            
            # Ã–nce tÃ¼m ses dosyasÄ±nÄ± transkript et
            self.add_log("TÃ¼m ses dosyasÄ± transkript ediliyor...")
            full_transcript = self.transcribe_audio_segment(audio_file)
            
            if not full_transcript:
                self.add_log("Transkripsiyon baÅŸarÄ±sÄ±z oldu. LÃ¼tfen API anahtarÄ±nÄ±zÄ± kontrol edin.")
                self.status_label.config(text="Transkripsiyon hatasÄ±!")
                self.analyze_button.config(state=tk.NORMAL)
                return
            
            self.add_log("Tam transkript alÄ±ndÄ±, konuÅŸmacÄ±lara gÃ¶re bÃ¶lÃ¼nÃ¼yor...")
            
            # Diyarizasyon sonuÃ§larÄ±nÄ± gÃ¶ster
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"Diyarizasyon SonuÃ§larÄ±: {audio_file}\n\n")
            
            # KonuÅŸmacÄ±larÄ± ve zaman aralÄ±klarÄ±nÄ± gÃ¶ster
            speaker_segments = []
            if diarization:
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    start = turn.start
                    end = turn.end
                    duration = end - start
                    result_line = f"{speaker}: {start:.2f}s - {end:.2f}s ({duration:.2f}s)\n"
                    self.result_text.insert(tk.END, result_line)
                    
                    # Segment bilgilerini kaydet
                    speaker_segments.append({
                        "speaker": speaker,
                        "start": start,
                        "end": end
                    })
            
            # KonuÅŸmacÄ± istatistiklerini hesapla
            speaker_stats = {}
            if diarization:
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    if speaker not in speaker_stats:
                        speaker_stats[speaker] = 0
                    speaker_stats[speaker] += turn.end - turn.start
            
            # Ä°statistikleri gÃ¶ster
            self.result_text.insert(tk.END, "\nKonuÅŸmacÄ± Ä°statistikleri:\n")
            if speaker_stats:
                for speaker, duration in speaker_stats.items():
                    self.result_text.insert(tk.END, f"{speaker}: {duration:.2f} saniye\n")
            
            # KonuÅŸmacÄ± bazlÄ± transkriptleri oluÅŸtur
            # Tam transkripti zaman aralÄ±klarÄ±na gÃ¶re bÃ¶l
            
            # Segmentleri zaman sÄ±rasÄ±na gÃ¶re sÄ±rala
            speaker_segments.sort(key=lambda x: x["start"])
            
            # Her segment iÃ§in transkript oluÅŸtur
            for i, segment in enumerate(speaker_segments):
                speaker = segment["speaker"]
                start = segment["start"]
                end = segment["end"]
                
                # Tam transkriptten bu konuÅŸmacÄ±nÄ±n konuÅŸma iÃ§eriÄŸini tahmin et
                # Burada basit bir yaklaÅŸÄ±m kullanÄ±yoruz
                # GerÃ§ek uygulamada daha geliÅŸmiÅŸ bir metin bÃ¶lme algoritmasÄ± gerekebilir
                
                # Her segment iÃ§in ayrÄ± transkript yap
                self.add_log(f"{speaker} iÃ§in segment transkript ediliyor: {start:.2f}s - {end:.2f}s")
                
                # Segment ses dosyasÄ±nÄ± oluÅŸtur
                import soundfile as sf
                import numpy as np
                
                # Ses dosyasÄ±nÄ± yÃ¼kle
                audio, sample_rate = sf.read(audio_file)
                
                # Segment sÄ±nÄ±rlarÄ±nÄ± hesapla
                start_sample = int(start * sample_rate)
                end_sample = int(end * sample_rate)
                
                # Segment sÄ±nÄ±rlarÄ±nÄ± kontrol et
                if start_sample >= len(audio) or end_sample > len(audio):
                    continue
                    
                # KonuÅŸmacÄ± segmentini kes
                segment_audio = audio[start_sample:end_sample]
                
                # GeÃ§ici dosya oluÅŸtur
                temp_dir = "temp_segments"
                os.makedirs(temp_dir, exist_ok=True)
                segment_file = os.path.join(temp_dir, f"segment_{i}.wav")
                sf.write(segment_file, segment_audio, sample_rate)
                
                # Bu segmenti transkript et
                segment_text = self.transcribe_audio_segment(segment_file)
                
                if segment_text and len(segment_text.strip()) > 0:
                    # Transkript sonucunu gÃ¶ster
                    self.transcript_text.insert(tk.END, f"[{start:.2f}s - {end:.2f}s] {speaker}: {segment_text}\n\n")
                else:
                    self.add_log(f"{speaker} iÃ§in segment transkripsiyon boÅŸ dÃ¶ndÃ¼")
            
            self.add_log("KonuÅŸmacÄ± bazlÄ± transkripsiyon tamamlandÄ±.")
            self.status_label.config(text="Analiz tamamlandÄ±")
            self.analyze_button.config(state=tk.NORMAL)
            
        except Exception as e:
            self.add_log(f"KonuÅŸmacÄ± bazlÄ± transkripsiyon hatasÄ±: {e}")
            self.status_label.config(text="Hata!")
            self.analyze_button.config(state=tk.NORMAL)
    
    def transcribe_audio_segment(self, audio_file):
        """Tek bir ses segmentini transkript et"""
        try:
            # gpt-4o-mini iÃ§in Whisper API kullan
            whisper_response = openai.audio.transcriptions.create(
                model="whisper-1",
                file=open(audio_file, "rb"),
                language=GPT_LANGUAGE
            )
            
            return whisper_response.text
            
        except Exception as e:
            self.add_log(f"Segment transkripsiyon hatasÄ±: {e}")
            return ""
    
    def add_log(self, message):
        """Log alanÄ±na mesaj ekle"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
        self.log_text.see(tk.END)
    
    def update_advanced_visualizations(self, audio_data, sample_rate):
        """GeliÅŸmiÅŸ gÃ¶rselleÅŸtirmeleri gÃ¼ncelle"""
        try:
            self.add_log("ğŸ“Š GÃ¶rselleÅŸtirmeler gÃ¼ncelleniyor...")
            
            # Ana dalga formu ve diyarizasyon gÃ¶rselleÅŸtirmesi
            self.update_comprehensive_plots(audio_data)
            
            # Diyarizasyon sonuÃ§larÄ±nÄ± Ã§iz
            if self.diarization_result:
                self.plot_diarization_timeline(audio_data, sample_rate)
            
            # Duygu analizi gÃ¶rselleÅŸtirmesi
            if self.emotion_result:
                if hasattr(self, 'temporal_emotion_history') and self.temporal_emotion_history:
                    self.plot_temporal_emotion_analysis()  # Zamansal gÃ¶rselleÅŸtirme
                else:
                    self.plot_emotion_analysis()  # Standart gÃ¶rselleÅŸtirme
            
            # Ä°statistik grafikleri
            self.plot_statistics()
            
            self.add_log("âœ… GÃ¶rselleÅŸtirmeler gÃ¼ncellendi")
            
        except Exception as e:
            self.add_log(f"âŒ GÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def plot_diarization_timeline(self, audio_data, sample_rate):
        """Diyarizasyon zaman Ã§izelgesi Ã§iz"""
        try:
            self.ax_diarization.clear()
            
            if not self.diarization_result:
                return
                
            # KonuÅŸmacÄ±larÄ± renk kodlarÄ± ile eÅŸleÅŸtir
            speakers = list(set(speaker for _, _, speaker in self.diarization_result.itertracks(yield_label=True)))
            speaker_colors = {speaker: SPEAKER_COLORS[i % len(SPEAKER_COLORS)] for i, speaker in enumerate(speakers)}
            
            # Y ekseni iÃ§in konuÅŸmacÄ± pozisyonlarÄ±
            speaker_positions = {speaker: i for i, speaker in enumerate(speakers)}
            
            # Her konuÅŸmacÄ± segmentini Ã§iz
            for turn, _, speaker in self.diarization_result.itertracks(yield_label=True):
                start = turn.start
                end = turn.end
                y_pos = speaker_positions[speaker]
                
                # Segment Ã§ubunu Ã§iz
                self.ax_diarization.barh(y_pos, end - start, left=start, height=0.8, 
                                        color=speaker_colors[speaker], alpha=0.7, 
                                        edgecolor='black', linewidth=0.5)
                
                # Segment Ã¼zerine sÃ¼re yaz (eÄŸer yeterince uzunsa)
                if end - start > 1.0:  # 1 saniyeden uzun segmentler iÃ§in
                    self.ax_diarization.text(start + (end - start) / 2, y_pos, 
                                           f'{end - start:.1f}s', 
                                           ha='center', va='center', fontsize=8, weight='bold')
            
            # VAD sonuÃ§larÄ±nÄ± ekle (varsa)
            if self.vad_result:
                audio_duration = len(audio_data) / sample_rate
                vad_y = len(speakers)  # En Ã¼ste VAD Ã§ubuÄŸu
                
                # Sessizlik bÃ¶lgeleri (gri)
                self.ax_diarization.barh(vad_y, audio_duration, left=0, height=0.3, 
                                        color='lightgray', alpha=0.5, label='Sessizlik')
                
                # KonuÅŸma bÃ¶lgeleri (yeÅŸil)
                for segment in self.vad_result.get_timeline():
                    self.ax_diarization.barh(vad_y, segment.end - segment.start, 
                                           left=segment.start, height=0.3, 
                                           color='lightgreen', alpha=0.7)
            
            # Ã–rtÃ¼ÅŸme bÃ¶lgelerini ekle (varsa)
            if self.overlap_result:
                for segment in self.overlap_result.get_timeline():
                    # TÃ¼m konuÅŸmacÄ±lar boyunca kÄ±rmÄ±zÄ± Ã§izgi
                    self.ax_diarization.axvspan(segment.start, segment.end, 
                                              alpha=0.3, color='red', 
                                              label='Ã–rtÃ¼ÅŸme' if segment == list(self.overlap_result.get_timeline())[0] else "")
            
            # Grafik ayarlarÄ±
            self.ax_diarization.set_xlabel('Zaman (saniye)')
            self.ax_diarization.set_ylabel('KonuÅŸmacÄ±lar')
            self.ax_diarization.set_title('ğŸ‘¥ KonuÅŸmacÄ± Zaman Ã‡izelgesi')
            
            # Y ekseni etiketleri
            all_labels = speakers[:]
            if self.vad_result:
                all_labels.append('VAD')
            
            self.ax_diarization.set_yticks(range(len(all_labels)))
            self.ax_diarization.set_yticklabels(all_labels)
            
            # Legend ekle
            if self.overlap_result and len(list(self.overlap_result.get_timeline())) > 0:
                self.ax_diarization.legend(loc='upper right')
            
            self.ax_diarization.grid(True, alpha=0.3)
            self.fig.tight_layout()
            self.canvas.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Diyarizasyon gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def plot_emotion_analysis(self):
        """Duygu analizi gÃ¶rselleÅŸtirmesi"""
        try:
            if not self.emotion_result:
                return
                
            self.ax_emotion.clear()
            
            # DuygularÄ± ve skorlarÄ± al
            emotions = list(self.emotion_result.keys())
            scores = list(self.emotion_result.values())
            percentages = [score * 100 for score in scores]
            
            # Renkleri eÅŸleÅŸtir
            colors = [EMOTION_COLORS.get(emotion, '#95A5A6') for emotion in emotions]
            
            # Pasta grafiÄŸi
            wedges, texts, autotexts = self.ax_emotion.pie(percentages, labels=emotions, 
                                                          colors=colors, autopct='%1.1f%%',
                                                          startangle=90, textprops={'fontsize': 10})
            
            # GrafiÄŸi gÃ¼zelleÅŸtir
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_weight('bold')
            
            self.ax_emotion.set_title('ğŸ˜Š Duygu DaÄŸÄ±lÄ±mÄ±', fontsize=14, weight='bold')
            
            # Dominant duyguyu vurgula
            max_emotion_idx = scores.index(max(scores))
            wedges[max_emotion_idx].set_edgecolor('black')
            wedges[max_emotion_idx].set_linewidth(3)
            
            self.canvas3.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Duygu gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def plot_statistics(self):
        """Ä°statistik grafikleri Ã§iz"""
        try:
            # KonuÅŸmacÄ± sÃ¼re daÄŸÄ±lÄ±mÄ±
            if self.diarization_result:
                speakers = {}
                for turn, _, speaker in self.diarization_result.itertracks(yield_label=True):
                    if speaker not in speakers:
                        speakers[speaker] = 0
                    speakers[speaker] += turn.end - turn.start
                
                # KonuÅŸmacÄ± sÃ¼releri bar grafiÄŸi
                self.ax_stats1.clear()
                speaker_names = list(speakers.keys())
                durations = list(speakers.values())
                colors = [SPEAKER_COLORS[i % len(SPEAKER_COLORS)] for i in range(len(speaker_names))]
                
                bars = self.ax_stats1.bar(speaker_names, durations, color=colors, alpha=0.7)
                self.ax_stats1.set_title('KonuÅŸmacÄ± SÃ¼releri')
                self.ax_stats1.set_ylabel('SÃ¼re (saniye)')
                self.ax_stats1.tick_params(axis='x', rotation=45)
                
                # DeÄŸerleri bar Ã¼zerine yaz
                for bar, duration in zip(bars, durations):
                    self.ax_stats1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                                       f'{duration:.1f}s', ha='center', va='bottom', fontsize=9)
            
            # KonuÅŸmacÄ± embedding Ã¶zellikleri
            if self.speaker_embeddings:
                self.ax_stats2.clear()
                
                # MFCC ortalama deÄŸerleri
                speakers = list(self.speaker_embeddings.keys())
                mfcc_values = [self.speaker_embeddings[s]['mfcc_mean'] for s in speakers]
                
                colors = [SPEAKER_COLORS[i % len(SPEAKER_COLORS)] for i in range(len(speakers))]
                self.ax_stats2.scatter(range(len(speakers)), mfcc_values, c=colors, s=100, alpha=0.7)
                self.ax_stats2.set_title('KonuÅŸmacÄ± MFCC OrtalamasÄ±')
                self.ax_stats2.set_xlabel('KonuÅŸmacÄ±lar')
                self.ax_stats2.set_ylabel('MFCC Ortalama')
                self.ax_stats2.set_xticks(range(len(speakers)))
                self.ax_stats2.set_xticklabels(speakers, rotation=45)
                self.ax_stats2.grid(True, alpha=0.3)
            
            # Ses aktivitesi istatistikleri
            if self.vad_result:
                self.ax_stats3.clear()
                
                speech_segments = list(self.vad_result.get_timeline())
                if speech_segments:
                    # Segment sÃ¼re daÄŸÄ±lÄ±mÄ± histogramÄ±
                    segment_durations = [seg.end - seg.start for seg in speech_segments]
                    
                    self.ax_stats3.hist(segment_durations, bins=min(20, len(segment_durations)), 
                                       color='lightgreen', alpha=0.7, edgecolor='black')
                    self.ax_stats3.set_title('KonuÅŸma Segment SÃ¼re DaÄŸÄ±lÄ±mÄ±')
                    self.ax_stats3.set_xlabel('Segment SÃ¼resi (saniye)')
                    self.ax_stats3.set_ylabel('Frekans')
                    self.ax_stats3.grid(True, alpha=0.3)
            
            # Duygu skorlarÄ± radar chart (basitleÅŸtirilmiÅŸ)
            if self.emotion_result:
                self.ax_stats4.clear()
                
                emotions = list(self.emotion_result.keys())
                scores = [self.emotion_result[e] * 100 for e in emotions]
                
                # Basit bar chart
                colors = [EMOTION_COLORS.get(emotion, '#95A5A6') for emotion in emotions]
                bars = self.ax_stats4.bar(emotions, scores, color=colors, alpha=0.7)
                self.ax_stats4.set_title('Duygu SkorlarÄ±')
                self.ax_stats4.set_ylabel('Skor (%)')
                self.ax_stats4.tick_params(axis='x', rotation=45)
                self.ax_stats4.set_ylim(0, 100)
                
                # DeÄŸerleri bar Ã¼zerine yaz
                for bar, score in zip(bars, scores):
                    self.ax_stats4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                                       f'{score:.1f}%', ha='center', va='bottom', fontsize=8)
            
            self.fig4.tight_layout()
            self.canvas4.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Ä°statistik gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def generate_detailed_analysis_report(self):
        """DetaylÄ± analiz raporu oluÅŸtur"""
        try:
            self.analysis_text.insert(tk.END, f"\n\nğŸ“‹ DETAYLI ANALÄ°Z RAPORU\n")
            self.analysis_text.insert(tk.END, f"{'='*50}\n")
            self.analysis_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            self.analysis_text.insert(tk.END, f"ğŸ“‚ Dosya: {os.path.basename(self.current_audio_file) if self.current_audio_file else 'KayÄ±t'}\n\n")
            
            # Genel istatistikler
            if self.current_audio_file:
                audio_duration = librosa.get_duration(filename=self.current_audio_file)
                self.analysis_text.insert(tk.END, f"â±ï¸ Toplam sÃ¼re: {audio_duration:.2f} saniye\n")
            
            # Diyarizasyon Ã¶zeti
            if self.diarization_result:
                speakers = list(set(speaker for _, _, speaker in self.diarization_result.itertracks(yield_label=True)))
                self.analysis_text.insert(tk.END, f"ğŸ‘¥ Tespit edilen konuÅŸmacÄ± sayÄ±sÄ±: {len(speakers)}\n")
                
                total_speech = sum(turn.end - turn.start for turn, _, _ in self.diarization_result.itertracks())
                self.analysis_text.insert(tk.END, f"ğŸ—£ï¸ Toplam konuÅŸma sÃ¼resi: {total_speech:.2f} saniye\n")
            
            # VAD Ã¶zeti
            if self.vad_result:
                speech_segments = list(self.vad_result.get_timeline())
                self.analysis_text.insert(tk.END, f"ğŸ¯ KonuÅŸma segment sayÄ±sÄ±: {len(speech_segments)}\n")
            
            # Ã–rtÃ¼ÅŸme Ã¶zeti
            if self.overlap_result:
                overlap_segments = list(self.overlap_result.get_timeline())
                total_overlap = sum(seg.end - seg.start for seg in overlap_segments)
                self.analysis_text.insert(tk.END, f"ğŸ”„ Toplam Ã¶rtÃ¼ÅŸme sÃ¼resi: {total_overlap:.2f} saniye\n")
            
            # Duygu Ã¶zeti
            if self.emotion_result:
                dominant_emotion = max(self.emotion_result.items(), key=lambda x: x[1])
                self.analysis_text.insert(tk.END, f"ğŸ˜Š BaskÄ±n duygu: {dominant_emotion[0].capitalize()} ({dominant_emotion[1]*100:.1f}%)\n")
            
            self.analysis_text.insert(tk.END, f"\n{'='*50}\n")
            
        except Exception as e:
            self.add_log(f"âŒ Rapor oluÅŸturma hatasÄ±: {e}")
    
    def start_live_analysis(self):
        """CanlÄ± analiz baÅŸlat"""
        try:
            if self.live_analysis_running:
                return
                
            self.live_analysis_running = True
            self.add_log("ğŸ”´ CanlÄ± analiz baÅŸlatÄ±ldÄ±")
            
            # CanlÄ± analiz iÃ§in ayrÄ± thread baÅŸlat
            self.live_analysis_thread = threading.Thread(target=self.live_analysis_worker)
            self.live_analysis_thread.daemon = True
            self.live_analysis_thread.start()
            
        except Exception as e:
            self.add_log(f"âŒ CanlÄ± analiz baÅŸlatma hatasÄ±: {e}")
    
    def stop_live_analysis(self):
        """CanlÄ± analizi durdur"""
        try:
            self.live_analysis_running = False
            self.add_log("â¹ï¸ CanlÄ± analiz durduruldu")
            
        except Exception as e:
            self.add_log(f"âŒ CanlÄ± analiz durdurma hatasÄ±: {e}")
    
    def live_analysis_worker(self):
        """CanlÄ± analiz worker fonksiyonu"""
        try:
            while self.live_analysis_running and self.is_recording:
                # Basit canlÄ± analiz - ses seviyesi gÃ¶sterimi
                time.sleep(0.1)  # 100ms gÃ¼ncelleme
                
                # GerÃ§ek projede burada gerÃ§ek zamanlÄ± analiz yapÄ±lÄ±r
                # Åu an iÃ§in sadece durum gÃ¶sterimi
                
                if not self.is_recording:
                    break
                    
        except Exception as e:
            self.add_log(f"âŒ CanlÄ± analiz worker hatasÄ±: {e}")
        finally:
            self.live_analysis_running = False
    
    def generate_report(self):
        """PDF raporu oluÅŸtur"""
        try:
            self.add_log("ğŸ“„ Rapor oluÅŸturuluyor...")
            
            # Basit metin raporu oluÅŸtur
            report_content = []
            report_content.append("ğŸ¤ GELIÅMIÅ SES ANALÄ°ZÄ° RAPORU")
            report_content.append("=" * 50)
            report_content.append(f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_content.append(f"ğŸ“‚ Dosya: {os.path.basename(self.current_audio_file) if self.current_audio_file else 'KayÄ±t'}")
            report_content.append("")
            
            # Diyarizasyon sonuÃ§larÄ±
            if self.diarization_result:
                report_content.append("ğŸ‘¥ DÄ°YARÄ°ZASYON SONUÃ‡LARI:")
                report_content.append("-" * 30)
                
                speakers = {}
                for turn, _, speaker in self.diarization_result.itertracks(yield_label=True):
                    if speaker not in speakers:
                        speakers[speaker] = []
                    speakers[speaker].append(f"{turn.start:.2f}s - {turn.end:.2f}s ({turn.end - turn.start:.2f}s)")
                
                for speaker, segments in speakers.items():
                    report_content.append(f"\n{speaker}:")
                    for segment in segments:
                        report_content.append(f"  â€¢ {segment}")
            
            # Duygu analizi sonuÃ§larÄ±
            if self.emotion_result:
                report_content.append("\n\nğŸ˜Š DUYGU ANALÄ°ZÄ° SONUÃ‡LARI:")
                report_content.append("-" * 30)
                sorted_emotions = sorted(self.emotion_result.items(), key=lambda x: x[1], reverse=True)
                for emotion, score in sorted_emotions:
                    report_content.append(f"{emotion.capitalize()}: {score*100:.1f}%")
            
            # Raporu dosyaya kaydet
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_filename = f"ses_analizi_raporu_{timestamp}.txt"
            
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_content))
            
            self.add_log(f"âœ… Rapor oluÅŸturuldu: {report_filename}")
            messagebox.showinfo("Rapor", f"Rapor baÅŸarÄ±yla oluÅŸturuldu:\n{report_filename}")
            
        except Exception as e:
            self.add_log(f"âŒ Rapor oluÅŸturma hatasÄ±: {e}")
            messagebox.showerror("Hata", f"Rapor oluÅŸturulurken hata oluÅŸtu:\n{e}")
    
    def run_ml_emotion_analysis(self, audio_data, sample_rate):
        """Machine Learning tabanlÄ± duygu analizi"""
        try:
            self.add_log("ğŸ¤– ML tabanlÄ± duygu analizi baÅŸlatÄ±lÄ±yor...")
            
            # GeliÅŸmiÅŸ Ã¶zellik Ã§Ä±karÄ±mÄ±
            features = self.extract_advanced_audio_features(audio_data, sample_rate)
            
            if not features:
                return self.run_emotion_analysis(audio_data, sample_rate)  # Fallback
            
            # Ã–zellik vektÃ¶rÃ¼nÃ¼ hazÄ±rla
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # NaN deÄŸerleri temizle
            feature_vector = np.nan_to_num(feature_vector)
            
            # Ã–zellik normalizasyonu
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            feature_vector_normalized = scaler.fit_transform(feature_vector)
            
            # GeliÅŸmiÅŸ kural tabanlÄ± sistem (ML benzeri)
            emotion_scores = self.advanced_rule_based_classification(features)
            
            # EÄŸer gerÃ§ek ML modeli varsa kullan
            try:
                emotion_scores = self.use_pretrained_emotion_model(feature_vector_normalized)
            except:
                self.add_log("âš ï¸ Pretrained model bulunamadÄ±, geliÅŸmiÅŸ kurallar kullanÄ±lÄ±yor")
            
            # Temporal analiz (zaman serisi)
            temporal_scores = self.temporal_emotion_analysis(audio_data, sample_rate)
            
            # Zamansal veriyi sakla (gÃ¶rselleÅŸtirme iÃ§in)
            if hasattr(self, '_temp_temporal_data'):
                self.temporal_emotion_history = self._temp_temporal_data
            
            # SkorlarÄ± birleÅŸtir (ensemble)
            final_scores = self.ensemble_emotion_scores(emotion_scores, temporal_scores)
            
            # GÃ¼ven skoru hesapla
            confidence = self.calculate_confidence_score(features, final_scores)
            
            # Debug bilgilerini gÃ¶ster
            debug_info = self.debug_emotion_analysis(features)
            self.add_log("ğŸ” Debug bilgileri:")
            for line in debug_info.split('\n'):
                if line.strip():
                    self.add_log(line)
            
            # SonuÃ§larÄ± gÃ¶ster
            self.display_advanced_emotion_results(final_scores, features, confidence)
            
            self.add_log("âœ… ML tabanlÄ± duygu analizi tamamlandÄ±")
            return final_scores
            
        except Exception as e:
            self.add_log(f"âŒ ML duygu analizi hatasÄ±: {e}")
            # Fallback to basic analysis
            return self.run_emotion_analysis(audio_data, sample_rate)
    
    def advanced_rule_based_classification(self, features):
        """GeliÅŸmiÅŸ kural tabanlÄ± duygu sÄ±nÄ±flandÄ±rmasÄ±"""
        emotion_scores = {
            'mutlu': 0.0, 'Ã¼zgÃ¼n': 0.0, 'kÄ±zgÄ±n': 0.0,
            'sakin': 0.0, 'heyecanlÄ±': 0.0, 'stresli': 0.0,
            'ÅŸaÅŸkÄ±n': 0.0, 'korku': 0.0  # Yeni duygular
        }
        
        # GeliÅŸmiÅŸ kurallar - DÃœÅÃœK EÅÄ°KLER ile gÃ¼ncellendi
        
        # 1. MUTLULUK ve GÃœLME TESPÄ°TÄ° (Ã‡ok geliÅŸtirildi!)
        mutlu_score = 0.0
        
        # Ana mutluluk gÃ¶stergeleri
        if features['pitch_mean'] > 150:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 200 -> 150
            mutlu_score += 0.3
        if features['energy'] > 0.0005:  # Ã‡OK dÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.01 -> 0.0005
            mutlu_score += 0.3
        if features['pitch_range'] > 0.05:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.1 -> 0.05
            mutlu_score += 0.2
        if features['zcr_mean'] > 0.05:  # ZCR kahkaha iÃ§in Ã¶nemli
            mutlu_score += 0.2
        if features['spectral_bandwidth_mean'] > 1000:  # GeniÅŸ frekans = gÃ¼lme
            mutlu_score += 0.2
        if features['energy_dynamic_range'] > 0.001:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.01 -> 0.001
            mutlu_score += 0.2
            
        # KAHKAHA Ã¶zel tespiti
        if (features['pitch_mean'] > 200 and features['zcr_mean'] > 0.08 and 
            features['energy'] > 0.001):  # Kahkaha kombinasyonu
            mutlu_score += 0.5
            
        emotion_scores['mutlu'] = min(mutlu_score, 1.0)
        
        # 2. HEYECAN TESPÄ°TÄ° (GeliÅŸtirildi)
        heyecan_score = 0.0
        if features['energy'] > 0.001:  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼: 0.015 -> 0.001
            heyecan_score += 0.3
        if features['pitch_std'] > 30:  # Perde deÄŸiÅŸkenliÄŸi
            heyecan_score += 0.2
        if features['zcr_mean'] > 0.07:
            heyecan_score += 0.2
        if features['energy_variance'] > 0.0001:
            heyecan_score += 0.2
        if features['voice_activity_ratio'] > 0.6:  # Aktif konuÅŸma
            heyecan_score += 0.1
            
        emotion_scores['heyecanlÄ±'] = min(heyecan_score, 1.0)
            
        # 3. ÃœZÃœNTÃœ tespiti (Daha spesifik)
        if (features['pitch_mean'] < 120 and features['energy'] < 0.0003 and  # Ã‡ok dÃ¼ÅŸÃ¼k
            features['speaking_rate'] < 2 and features['silence_ratio'] > 0.4):
            emotion_scores['Ã¼zgÃ¼n'] += 0.4
            
        # 4. Ã–FKE tespiti (GeliÅŸtirildi)
        if (features['zcr_mean'] > 0.12 and features['energy'] > 0.005 and  # Daha yÃ¼ksek eÅŸik
            features['spectral_bandwidth_mean'] > 2500 and features['energy_variance'] > 0.002):
            emotion_scores['kÄ±zgÄ±n'] += 0.5
            
        # 5. SAKÄ°NLÄ°K tespiti (Ã‡ok daha spesifik)
        sakin_score = 0.0
        if features['energy_variance'] < 0.00005:  # Ã‡ok dÃ¼ÅŸÃ¼k varyans
            sakin_score += 0.2
        if features['pitch_std'] < 20:  # Ã‡ok stabil perde
            sakin_score += 0.2
        if features['zcr_mean'] < 0.03:  # Ã‡ok dÃ¼ÅŸÃ¼k ZCR
            sakin_score += 0.2
        if features['silence_ratio'] > 0.5:  # Ã‡ok sessizlik
            sakin_score += 0.3
        if features['energy'] < 0.0002:  # Ã‡ok dÃ¼ÅŸÃ¼k enerji
            sakin_score += 0.3
            
        emotion_scores['sakin'] = min(sakin_score, 1.0)
            
        # 6. STRES tespiti
        if (features['zcr_std'] > 0.03 and features['energy_variance'] > 0.0003 and
            features['pitch_std'] > 60 and features['spectral_flatness_mean'] > 0.08):
            emotion_scores['stresli'] += 0.4
            
        # 7. ÅAÅKINLIK tespiti  
        if (features['pitch_range'] > 0.3 and features['energy_dynamic_range'] > 0.005 and
            features['speaking_rate'] < 3):
            emotion_scores['ÅŸaÅŸkÄ±n'] += 0.3
            
        # 8. KORKU tespiti
        if (features['zcr_mean'] > 0.10 and features['pitch_mean'] > 180 and
            features['energy_variance'] > 0.0005 and features['voice_activity_ratio'] < 0.5):
            emotion_scores['korku'] += 0.3
        
        # Ã–ZEL KAHKAHA TESPÄ°TÄ° - En Ã¼st Ã¶ncelik
        if self.detect_laughter_patterns(features):
            emotion_scores['mutlu'] += 0.6  # GÃ¼Ã§lÃ¼ kahkaha bonusu
            emotion_scores['heyecanlÄ±'] += 0.4
            emotion_scores['sakin'] *= 0.1  # SakinliÄŸi bastÄ±r
            emotion_scores['Ã¼zgÃ¼n'] *= 0.1   # ÃœzÃ¼ntÃ¼yÃ¼ bastÄ±r
        
        # Normalize skorlarÄ±
        total_score = sum(emotion_scores.values())
        if total_score > 0:
            for emotion in emotion_scores:
                emotion_scores[emotion] = emotion_scores[emotion] / total_score
        else:
            # Default deÄŸerler (kahkaha bulunamadÄ±ysa)
            emotion_scores['mutlu'] = 0.4
            emotion_scores['heyecanlÄ±'] = 0.3
            emotion_scores['sakin'] = 0.3
            
        return emotion_scores
    
    def detect_laughter_patterns(self, features):
        """Kahkaha desenlerini tespit et"""
        try:
            laughter_indicators = 0
            
            # 1. YÃ¼ksek frekans ama orta enerji (tipik kahkaha)
            if 1000 < features['pitch_mean'] < 4000:
                laughter_indicators += 1
                
            # 2. YÃ¼ksek ZCR (gÃ¼rÃ¼ltÃ¼lÃ¼, titrek ses)
            if features['zcr_mean'] > 0.08:
                laughter_indicators += 1
                
            # 3. GeniÅŸ spektral bant (ha-ha-ha sesi)
            if features['spectral_bandwidth_mean'] > 1500:
                laughter_indicators += 1
                
            # 4. Ortalama enerji (Ã§ok yÃ¼ksek deÄŸil ama var)
            if 0.0005 < features['energy'] < 0.01:
                laughter_indicators += 1
                
            # 5. DeÄŸiÅŸken enerji (patlamalar)
            if features['energy_variance'] > 0.0002:
                laughter_indicators += 1
                
            # 6. PerkÃ¼sif Ã¶zellikler (ha-ha ritmi)
            if features['percussive_mean'] > 0.001:
                laughter_indicators += 1
                
            # 4 veya daha fazla gÃ¶sterge = muhtemelen kahkaha
            return laughter_indicators >= 4
            
        except Exception as e:
            return False
    
    def temporal_emotion_analysis(self, audio_data, sample_rate, window_size=3.0):
        """Zamansal duygu analizi - ses boyunca duygu deÄŸiÅŸimi"""
        try:
            self.add_log("â° Zamansal duygu analizi yapÄ±lÄ±yor...")
            
            window_samples = int(window_size * sample_rate)
            hop_samples = window_samples // 2
            
            temporal_emotions = []
            self._temp_temporal_data = []  # GeÃ§ici veri saklama
            
            for start in range(0, len(audio_data) - window_samples, hop_samples):
                end = start + window_samples
                segment = audio_data[start:end]
                
                # Segment Ã¶zelliklerini Ã§Ä±kar
                segment_features = self.extract_advanced_audio_features(segment, sample_rate)
                
                if segment_features:
                    # Segment iÃ§in duygu analizi
                    segment_emotions = self.advanced_rule_based_classification(segment_features)
                    temporal_data = {
                        'time': start / sample_rate,
                        'emotions': segment_emotions
                    }
                    temporal_emotions.append(temporal_data)
                    self._temp_temporal_data.append(temporal_data)  # GÃ¶rselleÅŸtirme iÃ§in sakla
            
            # Zamansal ortalamalarÄ± hesapla
            if temporal_emotions:
                avg_emotions = {}
                for emotion in temporal_emotions[0]['emotions'].keys():
                    scores = [te['emotions'][emotion] for te in temporal_emotions]
                    avg_emotions[emotion] = np.mean(scores)
                
                return avg_emotions
            else:
                return {}
                
        except Exception as e:
            self.add_log(f"âŒ Zamansal analiz hatasÄ±: {e}")
            return {}
    
    def ensemble_emotion_scores(self, rule_scores, temporal_scores, weights=[0.6, 0.4]):
        """FarklÄ± analiz yÃ¶ntemlerinin skorlarÄ±nÄ± birleÅŸtir"""
        if not temporal_scores:
            return rule_scores
            
        ensemble_scores = {}
        
        for emotion in rule_scores.keys():
            if emotion in temporal_scores:
                ensemble_scores[emotion] = (
                    weights[0] * rule_scores[emotion] + 
                    weights[1] * temporal_scores[emotion]
                )
            else:
                ensemble_scores[emotion] = rule_scores[emotion]
        
        # Normalize
        total = sum(ensemble_scores.values())
        if total > 0:
            for emotion in ensemble_scores:
                ensemble_scores[emotion] /= total
                
        return ensemble_scores
    
    def calculate_confidence_score(self, features, emotion_scores):
        """Analiz gÃ¼ven skorunu hesapla"""
        try:
            # Ses kalitesi faktÃ¶rleri
            quality_factors = {
                'energy_level': min(features['energy'] * 100, 1.0),
                'voice_activity': features['voice_activity_ratio'],
                'signal_clarity': 1 - features['spectral_flatness_mean'],
                'pitch_stability': 1 / (1 + features['pitch_std'] / max(features['pitch_mean'], 1))
            }
            
            # Duygu skorlarÄ±nÄ±n daÄŸÄ±lÄ±mÄ±
            max_emotion_score = max(emotion_scores.values())
            second_max = sorted(emotion_scores.values())[-2] if len(emotion_scores) > 1 else 0
            score_separation = max_emotion_score - second_max
            
            # Genel gÃ¼ven skoru
            quality_score = np.mean(list(quality_factors.values()))
            confidence = (quality_score * 0.7) + (score_separation * 0.3)
            
            return min(confidence, 1.0)
            
        except Exception as e:
            return 0.5  # Orta gÃ¼ven
    
    def display_advanced_emotion_results(self, emotion_scores, features, confidence):
        """GeliÅŸmiÅŸ duygu analizi sonuÃ§larÄ±nÄ± gÃ¶ster"""
        self.emotion_text.delete(1.0, tk.END)
        self.emotion_text.insert(tk.END, f"ğŸ¤– GeliÅŸmiÅŸ Duygu Analizi SonuÃ§larÄ±\n")
        self.emotion_text.insert(tk.END, f"ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.emotion_text.insert(tk.END, f"ğŸ¯ GÃ¼ven Skoru: {confidence*100:.1f}%\n\n")
        
        # DuygularÄ± sÄ±ralÄ± gÃ¶ster
        sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)
        
        for i, (emotion, score) in enumerate(sorted_emotions):
            percentage = score * 100
            bar_length = int(percentage / 2.5)  # Daha hassas Ã§ubuk
            bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
            
            # Ä°kon ekle
            icons = {'mutlu': 'ğŸ˜Š', 'Ã¼zgÃ¼n': 'ğŸ˜¢', 'kÄ±zgÄ±n': 'ğŸ˜ ', 
                    'sakin': 'ğŸ˜Œ', 'heyecanlÄ±': 'ğŸ¤©', 'stresli': 'ğŸ˜°',
                    'ÅŸaÅŸkÄ±n': 'ğŸ˜®', 'korku': 'ğŸ˜¨'}
            icon = icons.get(emotion, 'ğŸ˜')
            
            rank_icon = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            
            self.emotion_text.insert(tk.END, 
                f"{rank_icon} {icon} {emotion.capitalize()}: {percentage:.1f}% {bar}\n")
        
        # Dominant duygu
        dominant_emotion, dominant_score = sorted_emotions[0]
        self.emotion_text.insert(tk.END, f"\nğŸ¯ BaskÄ±n Duygu: {dominant_emotion.capitalize()} ({dominant_score*100:.1f}%)\n")
        
        # DetaylÄ± ses Ã¶zellikleri
        self.emotion_text.insert(tk.END, f"\nğŸ“Š DetaylÄ± Ses Ã–zellikleri:\n")
        self.emotion_text.insert(tk.END, f"âš¡ Enerji Seviyesi: {features['energy']:.4f}\n")
        self.emotion_text.insert(tk.END, f"ğŸµ Ortalama Perde: {features['pitch_mean']:.1f} Hz\n")
        self.emotion_text.insert(tk.END, f"ğŸ“ˆ Perde DeÄŸiÅŸkenliÄŸi: {features['pitch_std']:.1f} Hz\n")
        self.emotion_text.insert(tk.END, f"ğŸ—£ï¸ KonuÅŸma HÄ±zÄ±: {features['speaking_rate']:.1f}\n")
        self.emotion_text.insert(tk.END, f"ğŸ¤« Sessizlik OranÄ±: {features['silence_ratio']*100:.1f}%\n")
        self.emotion_text.insert(tk.END, f"ğŸŒŠ ZCR Ortalama: {features['zcr_mean']:.3f}\n")
        self.emotion_text.insert(tk.END, f"ğŸ¼ Spektral Merkez: {features['spectral_centroid_mean']:.1f} Hz\n")
        self.emotion_text.insert(tk.END, f"ğŸ›ï¸ Enerji VaryansÄ±: {features['energy_variance']:.6f}\n")
        
        # GÃ¼ven seviyesi yorumu
        if confidence > 0.8:
            conf_text = "Ã‡ok YÃ¼ksek âœ¨"
        elif confidence > 0.6:
            conf_text = "YÃ¼ksek âœ…"
        elif confidence > 0.4:
            conf_text = "Orta âš ï¸"
        else:
            conf_text = "DÃ¼ÅŸÃ¼k âŒ"
            
        self.emotion_text.insert(tk.END, f"\nğŸ¯ Analiz GÃ¼venilirliÄŸi: {conf_text}\n")
    
    def use_pretrained_emotion_model(self, feature_vector):
        """Ã–nceden eÄŸitilmiÅŸ duygu modeli kullan"""
        try:
            # Hugging Face Transformers ile ses duygu tanÄ±ma
            from transformers import pipeline
            
            # Audio classification pipeline
            emotion_classifier = pipeline(
                "audio-classification",
                model="ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
            )
            
            # Alternatif: TensorFlow/Keras modeli
            # model = tf.keras.models.load_model('emotion_model.h5')
            # predictions = model.predict(feature_vector)
            
            self.add_log("ğŸ¤– Pretrained model kullanÄ±lÄ±yor...")
            return self.mock_pretrained_results()  # Åimdilik mock data
            
        except ImportError:
            self.add_log("âš ï¸ Transformers kÃ¼tÃ¼phanesi yok, alternatif yÃ¶ntem kullanÄ±lÄ±yor")
            return self.use_sklearn_emotion_model(feature_vector)
        except Exception as e:
            self.add_log(f"âŒ Pretrained model hatasÄ±: {e}")
            raise e
    
    def use_sklearn_emotion_model(self, feature_vector):
        """Scikit-learn tabanlÄ± duygu modeli"""
        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.svm import SVC
            from sklearn.naive_bayes import GaussianNB
            
            # Mock eÄŸitilmiÅŸ model (gerÃ§ek projede Ã¶nceden eÄŸitilmiÅŸ model yÃ¼klenir)
            # model = joblib.load('emotion_rf_model.pkl')
            
            # Åimdilik geliÅŸmiÅŸ kural tabanlÄ± tahmin dÃ¶ndÃ¼r
            return self.mock_sklearn_results()
            
        except Exception as e:
            self.add_log(f"âŒ Sklearn model hatasÄ±: {e}")
            raise e
    
    def mock_pretrained_results(self):
        """Mock pretrained model sonuÃ§larÄ±"""
        return {
            'mutlu': 0.15, 'Ã¼zgÃ¼n': 0.10, 'kÄ±zgÄ±n': 0.08,
            'sakin': 0.25, 'heyecanlÄ±': 0.20, 'stresli': 0.12,
            'ÅŸaÅŸkÄ±n': 0.06, 'korku': 0.04
        }
    
    def mock_sklearn_results(self):
        """Mock sklearn model sonuÃ§larÄ±"""
        return {
            'mutlu': 0.18, 'Ã¼zgÃ¼n': 0.12, 'kÄ±zgÄ±n': 0.10,
            'sakin': 0.22, 'heyecanlÄ±': 0.18, 'stresli': 0.15,
            'ÅŸaÅŸkÄ±n': 0.03, 'korku': 0.02
        }
    
    def plot_temporal_emotion_analysis(self):
        """Zamansal duygu analizi gÃ¶rselleÅŸtirmesi"""
        try:
            if not hasattr(self, 'temporal_emotion_history') or not self.temporal_emotion_history:
                return
                
            self.ax_emotion.clear()
            
            # Zaman ekseni oluÅŸtur
            times = [te['time'] for te in self.temporal_emotion_history]
            
            # Her duygu iÃ§in zaman serisi Ã§iz
            emotion_colors = {
                'mutlu': '#2ECC71', 'Ã¼zgÃ¼n': '#3498DB', 'kÄ±zgÄ±n': '#E74C3C',
                'sakin': '#95A5A6', 'heyecanlÄ±': '#F39C12', 'stresli': '#E67E22',
                'ÅŸaÅŸkÄ±n': '#9B59B6', 'korku': '#34495E'
            }
            
            for emotion in ['mutlu', 'Ã¼zgÃ¼n', 'kÄ±zgÄ±n', 'sakin', 'heyecanlÄ±']:
                scores = [te['emotions'].get(emotion, 0) * 100 for te in self.temporal_emotion_history]
                color = emotion_colors.get(emotion, '#95A5A6')
                
                self.ax_emotion.plot(times, scores, label=emotion.capitalize(), 
                                   color=color, linewidth=2, marker='o', markersize=4)
            
            self.ax_emotion.set_xlabel('Zaman (saniye)')
            self.ax_emotion.set_ylabel('Duygu Skoru (%)')
            self.ax_emotion.set_title('â° Zamansal Duygu DeÄŸiÅŸimi')
            self.ax_emotion.legend(loc='upper right', fontsize=9)
            self.ax_emotion.grid(True, alpha=0.3)
            self.ax_emotion.set_ylim(0, 100)
            
            # Dominant duygu bÃ¶lgelerini vurgula
            for i in range(len(self.temporal_emotion_history) - 1):
                current_emotions = self.temporal_emotion_history[i]['emotions']
                dominant_emotion = max(current_emotions.keys(), key=current_emotions.get)
                dominant_color = emotion_colors.get(dominant_emotion, '#95A5A6')
                
                start_time = self.temporal_emotion_history[i]['time']
                end_time = self.temporal_emotion_history[i + 1]['time']
                
                self.ax_emotion.axvspan(start_time, end_time, alpha=0.1, color=dominant_color)
            
            self.canvas3.draw()
            
        except Exception as e:
            self.add_log(f"âŒ Zamansal gÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def assess_audio_quality(self, audio_data, sample_rate):
        """Ses kalitesini deÄŸerlendir"""
        try:
            quality_score = 0.0
            quality_factors = {}
            
            # 1. Sinyal-GÃ¼rÃ¼ltÃ¼ OranÄ± (SNR) tahmini
            signal_power = np.mean(audio_data ** 2)
            noise_threshold = 0.001
            noise_power = np.mean(audio_data[np.abs(audio_data) < noise_threshold] ** 2) if np.any(np.abs(audio_data) < noise_threshold) else 0.0001
            snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else 40
            quality_factors['snr'] = min(snr / 40, 1.0)  # 40 dB'yi maksimum kabul et
            
            # 2. Dinamik AralÄ±k
            dynamic_range = np.max(audio_data) - np.min(audio_data)
            quality_factors['dynamic_range'] = min(dynamic_range / 2.0, 1.0)  # -1 ile +1 arasÄ± max
            
            # 3. Kliping (Kesik) Tespiti
            clipping_threshold = 0.95
            clipped_samples = np.sum(np.abs(audio_data) > clipping_threshold)
            clipping_ratio = clipped_samples / len(audio_data)
            quality_factors['no_clipping'] = max(1.0 - clipping_ratio * 10, 0.0)
            
            # 4. Frekans DaÄŸÄ±lÄ±mÄ±
            freqs = np.fft.fftfreq(len(audio_data), 1/sample_rate)
            fft = np.abs(np.fft.fft(audio_data))
            
            # Ä°nsan sesi frekans aralÄ±ÄŸÄ± (80-8000 Hz)
            voice_freq_mask = (np.abs(freqs) >= 80) & (np.abs(freqs) <= 8000)
            voice_energy = np.sum(fft[voice_freq_mask])
            total_energy = np.sum(fft)
            voice_ratio = voice_energy / total_energy if total_energy > 0 else 0
            quality_factors['voice_frequency'] = voice_ratio
            
            # 5. Ses SÃ¼rekliliÄŸi (Sessizlik analizine gÃ¶re)
            silence_threshold = 0.01
            voice_frames = np.abs(audio_data) > silence_threshold
            continuity = np.sum(voice_frames) / len(audio_data)
            quality_factors['continuity'] = continuity
            
            # 6. Spektral DÃ¼zlÃ¼k (GÃ¼rÃ¼ltÃ¼ gÃ¶stergesi)
            spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=audio_data))
            quality_factors['spectral_clarity'] = max(1.0 - spectral_flatness, 0.0)
            
            # Genel kalite skoru hesapla
            weights = {
                'snr': 0.25,
                'dynamic_range': 0.15,
                'no_clipping': 0.20,
                'voice_frequency': 0.15,
                'continuity': 0.15,
                'spectral_clarity': 0.10
            }
            
            quality_score = sum(quality_factors[factor] * weights[factor] for factor in quality_factors)
            
            return {
                'overall_score': quality_score,
                'factors': quality_factors,
                'details': {
                    'snr_db': snr,
                    'dynamic_range': dynamic_range,
                    'clipping_percentage': clipping_ratio * 100,
                    'voice_frequency_ratio': voice_ratio * 100,
                    'voice_continuity': continuity * 100,
                    'spectral_flatness': spectral_flatness
                }
            }
            
        except Exception as e:
            self.add_log(f"âŒ Ses kalitesi analizi hatasÄ±: {e}")
            return {'overall_score': 0.5, 'factors': {}, 'details': {}}
    
    def debug_emotion_analysis(self, features):
        """Duygu analizi debug bilgileri"""
        debug_info = []
        
        debug_info.append(f"ğŸ” DUYGU ANALÄ°ZÄ° DEBUG BÄ°LGÄ°LERÄ°:")
        debug_info.append(f"{'='*40}")
        
        # Temel Ã¶zellikler
        debug_info.append(f"ğŸ“Š Temel Ã–zellikler:")
        debug_info.append(f"  âš¡ Enerji: {features['energy']:.6f}")
        debug_info.append(f"  ğŸµ Perde: {features['pitch_mean']:.1f} Hz")
        debug_info.append(f"  ğŸ“ˆ Perde Std: {features['pitch_std']:.1f} Hz")
        debug_info.append(f"  ğŸŒŠ ZCR: {features['zcr_mean']:.4f}")
        debug_info.append(f"  ğŸ“Š Spektral Bant: {features['spectral_bandwidth_mean']:.1f} Hz")
        debug_info.append(f"  ğŸ›ï¸ Enerji Varyans: {features['energy_variance']:.8f}")
        debug_info.append("")
        
        # MUTLULUK kontrolleri
        debug_info.append(f"ğŸ˜Š MUTLULUK KONTROL:")
        mutlu_tests = []
        if features['pitch_mean'] > 150:
            mutlu_tests.append("âœ… Perde > 150 Hz")
        else:
            mutlu_tests.append("âŒ Perde Ã§ok dÃ¼ÅŸÃ¼k")
            
        if features['energy'] > 0.0005:
            mutlu_tests.append("âœ… Enerji > 0.0005")
        else:
            mutlu_tests.append("âŒ Enerji Ã§ok dÃ¼ÅŸÃ¼k")
            
        if features['zcr_mean'] > 0.05:
            mutlu_tests.append("âœ… ZCR > 0.05")
        else:
            mutlu_tests.append("âŒ ZCR dÃ¼ÅŸÃ¼k")
            
        # Kahkaha testi
        laughter_detected = self.detect_laughter_patterns(features)
        if laughter_detected:
            mutlu_tests.append("ğŸ‰ KAHKAHA TESPÄ°T EDÄ°LDÄ°!")
        else:
            mutlu_tests.append("âŒ Kahkaha tespit edilmedi")
            
        for test in mutlu_tests:
            debug_info.append(f"  {test}")
        debug_info.append("")
        
        # SAKÄ°NLÄ°K kontrolleri
        debug_info.append(f"ğŸ˜Œ SAKÄ°NLÄ°K KONTROL:")
        sakin_tests = []
        if features['energy'] < 0.0002:
            sakin_tests.append("âœ… Ã‡ok dÃ¼ÅŸÃ¼k enerji")
        else:
            sakin_tests.append("âŒ Enerji yeterince dÃ¼ÅŸÃ¼k deÄŸil")
            
        if features['zcr_mean'] < 0.03:
            sakin_tests.append("âœ… Ã‡ok dÃ¼ÅŸÃ¼k ZCR")
        else:
            sakin_tests.append("âŒ ZCR yeterince dÃ¼ÅŸÃ¼k deÄŸil")
            
        if features['pitch_std'] < 20:
            sakin_tests.append("âœ… Stabil perde")
        else:
            sakin_tests.append("âŒ Perde deÄŸiÅŸken")
            
        for test in sakin_tests:
            debug_info.append(f"  {test}")
        debug_info.append("")
        
        return "\n".join(debug_info)
    
    def clean_audio_buffer(self, audio_data):
        """Ses verisini temizle - NaN ve Infinity deÄŸerleri kaldÄ±r"""
        try:
            self.add_log("ğŸ§¹ Ses verisi temizleniyor...")
            
            # NaN ve Infinity kontrolÃ¼
            if not np.isfinite(audio_data).all():
                self.add_log("âš ï¸ Ses verisinde NaN/Infinity deÄŸerleri tespit edildi, temizleniyor...")
                
                # NaN deÄŸerleri sÄ±fÄ±r ile deÄŸiÅŸtir
                audio_data = np.nan_to_num(audio_data, nan=0.0, posinf=0.0, neginf=0.0)
                
                # AÅŸÄ±rÄ± bÃ¼yÃ¼k deÄŸerleri kÄ±rp (-1, +1 aralÄ±ÄŸÄ±na)
                audio_data = np.clip(audio_data, -1.0, 1.0)
                
                # EÄŸer tÃ¼m veriler sÄ±fÄ±r olduysa, kÃ¼Ã§Ã¼k bir gÃ¼rÃ¼ltÃ¼ ekle
                if np.all(audio_data == 0):
                    self.add_log("âš ï¸ TÃ¼m ses verisi sÄ±fÄ±r, kÃ¼Ã§Ã¼k gÃ¼rÃ¼ltÃ¼ ekleniyor...")
                    audio_data = np.random.normal(0, 0.001, len(audio_data))
                
                self.add_log("âœ… Ses verisi temizlendi")
            
            return audio_data.astype(np.float32)
            
        except Exception as e:
            self.add_log(f"âŒ Ses temizleme hatasÄ±: {e}")
            # Son Ã§are: sessizlik dÃ¶ndÃ¼r
            return np.zeros(len(audio_data), dtype=np.float32)
    
    def get_default_features(self):
        """Hata durumunda kullanÄ±lacak varsayÄ±lan Ã¶zellikler"""
        return {
            # Temel Ã¶zellikler
            'mfcc_mean': 0.0,
            'mfcc_std': 1.0,
            'spectral_centroid_mean': 1000.0,
            'spectral_centroid_std': 500.0,
            'zcr_mean': 0.05,
            'zcr_std': 0.02,
            
            # GeliÅŸmiÅŸ spektral Ã¶zellikler
            'chroma_mean': 0.1,
            'chroma_std': 0.05,
            'mel_spectrogram_mean': 0.01,
            'tonnetz_mean': 0.0,
            'spectral_contrast_mean': 10.0,
            'spectral_bandwidth_mean': 1500.0,
            'spectral_flatness_mean': 0.1,
            'spectral_rolloff_mean': 2000.0,
            
            # Prozodik Ã¶zellikler
            'tempo': 100.0,
            'speaking_rate': 2.0,
            'pitch_mean': 200.0,
            'pitch_std': 50.0,
            'pitch_range': 0.25,
            
            # Enerji Ã¶zellikleri
            'energy': 0.001,
            'rms_energy': 0.01,
            'energy_variance': 0.0001,
            'energy_mean': 0.001,
            'energy_dynamic_range': 0.005,
            
            # Sessizlik ve duraklama
            'silence_ratio': 0.3,
            'voice_activity_ratio': 0.7,
            
            # Harmonik Ã¶zellikler
            'harmonic_mean': 0.001,
            'percussive_mean': 0.001,
        }
    
    def get_default_emotion_scores(self):
        """Hata durumunda kullanÄ±lacak varsayÄ±lan duygu skorlarÄ±"""
        return {
            'mutlu': 0.25,
            'Ã¼zgÃ¼n': 0.15,
            'kÄ±zgÄ±n': 0.10,
            'sakin': 0.30,
            'heyecanlÄ±': 0.15,
            'stresli': 0.05
        }
    
    def update_live_waveform(self):
        """CanlÄ± kayÄ±t sÄ±rasÄ±nda dalga formunu gÃ¼ncelle"""
        try:
            if len(self.recorded_audio) > 0:
                audio_array = np.array(self.recorded_audio[-SAMPLE_RATE:])  # Son 1 saniye
                if len(audio_array) > 100:  # Yeterli veri varsa
                    time_axis = np.arange(len(audio_array)) / SAMPLE_RATE
                    
                    # Ana dalga formu - sadece gÃ¼ncelle, layout deÄŸiÅŸikliÄŸi yapma
                    self.ax_waveform.clear()
                    self.ax_waveform.plot(time_axis, audio_array, color='#2E86AB', linewidth=0.8)
                    self.ax_waveform.set_ylim(-1, 1)
                    self.ax_waveform.set_title("ğŸ™ï¸ CanlÄ± KayÄ±t - Dalga Formu")
                    self.ax_waveform.grid(True, alpha=0.3)
                    
                    # Canvas'Ä± gÃ¼ncelle - ancak layout hesaplamasÄ± yapma
                    self.canvas.draw_idle()
                    
        except Exception as e:
            # CanlÄ± gÃ¼ncelleme hatalarÄ±nÄ± sessizce geÃ§
            pass
    
    def optimize_audio_device(self):
        """En uygun ses cihazÄ±nÄ± seÃ§ ve optimize et"""
        try:
            self.add_log("ğŸ§ Ses cihazlarÄ± taranÄ±yor...")
            
            # Mevcut cihazlarÄ± listele
            devices = sd.query_devices()
            
            # En iyi giriÅŸ cihazÄ±nÄ± bul
            best_input_device = None
            best_score = 0
            
            for i, device in enumerate(devices):
                if device['max_input_channels'] > 0:  # GiriÅŸ cihazÄ±
                    score = 0
                    
                    # Samplerate desteÄŸi
                    try:
                        sd.check_device(i, samplerate=SAMPLE_RATE)
                        score += 10
                    except:
                        continue
                    
                    # Kanal sayÄ±sÄ±
                    score += device['max_input_channels']
                    
                    # VarsayÄ±lan cihaz bonusu
                    if device == sd.query_devices(kind='input'):
                        score += 5
                    
                    # macOS iÃ§in Core Audio tercih et
                    if 'Core Audio' in device['hostapi_name']:
                        score += 3
                    
                    # USB/Harici cihaz bonusu
                    if 'USB' in device['name'] or 'External' in device['name']:
                        score += 2
                    
                    if score > best_score:
                        best_score = score
                        best_input_device = i
            
            if best_input_device is not None:
                sd.default.device[0] = best_input_device  # Input device
                device_info = devices[best_input_device]
                self.add_log(f"ğŸ¤ SeÃ§ilen cihaz: {device_info['name']}")
                self.add_log(f"ğŸ“Š API: {device_info['hostapi_name']}")
                self.add_log(f"ğŸ”Š Max kanallar: {device_info['max_input_channels']}")
                self.add_log(f"âš¡ Gecikme: {device_info['default_low_input_latency']*1000:.1f}ms")
            else:
                self.add_log("âš ï¸ Uygun ses cihazÄ± bulunamadÄ±, varsayÄ±lan kullanÄ±lacak")
                
            # Buffer ayarlarÄ±nÄ± optimize et
            latency = sd.query_devices(sd.default.device[0])['default_low_input_latency']
            optimal_blocksize = int(SAMPLE_RATE * latency)
            
            self.add_log(f"ğŸ”§ Optimal buffer boyutu: {optimal_blocksize} sample")
            return optimal_blocksize
            
        except Exception as e:
            self.add_log(f"âŒ Cihaz optimizasyonu hatasÄ±: {e}")
            return BUFFER_SIZE
    
    def update_recording_progress(self, progress, elapsed, total_duration):
        """KayÄ±t ilerlemesini gÃ¼ncelle"""
        try:
            remaining = total_duration - elapsed
            
            # Durum etiketi gÃ¼ncelle
            self.status_label.config(
                text=f"ğŸ™ï¸ KayÄ±t: {elapsed:.1f}s / {total_duration:.1f}s (%{progress:.1f})"
            )
            
            # Her 2 saniyede bir ilerleme logu
            if int(elapsed) % 2 == 0 and elapsed > 0:
                self.add_log(f"ğŸ“Š Ä°lerleme: %{progress:.1f} - {elapsed:.1f}s / {total_duration:.1f}s")
            
        except Exception as e:
            # Progress gÃ¼ncelleme hatalarÄ±nÄ± sessizce geÃ§
            pass
    
    def record_audio_simple(self, duration):
        """En basit kayÄ±t yÃ¶ntemi - fallback"""
        try:
            self.add_log("ğŸ™ï¸ FALLBACK: En basit kayÄ±t yÃ¶ntemi")
            
            # Ses cihazÄ±nÄ± optimize et
            self.optimize_audio_device()
            
            # Tek seferde tÃ¼m kaydÄ± al - eski yÃ¶ntem ama gÃ¼venilir
            total_samples = int(SAMPLE_RATE * duration)
            self.add_log(f"ğŸ“Š Tek seferde {total_samples} sample alÄ±nacak")
            
            # KayÄ±t baÅŸlat
            self.add_log("ğŸ”´ KayÄ±t baÅŸlÄ±yor...")
            start_time = time.time()
            
            # SoundDevice'Ä±n basit rec fonksiyonu
            audio_data = sd.rec(
                frames=total_samples,
                samplerate=SAMPLE_RATE,
                channels=CHANNELS,
                dtype=DTYPE,
                blocking=False  # Non-blocking kayÄ±t
            )
            
            # KayÄ±t tamamlanana kadar bekle ve progress gÃ¶ster
            while sd.get_stream().active and self.is_recording:
                elapsed = time.time() - start_time
                progress = min((elapsed / duration) * 100, 100)
                
                self.root.after(1, lambda p=progress, e=elapsed: 
                    self.status_label.config(text=f"ğŸ™ï¸ Basit KayÄ±t: {e:.1f}s / {duration}s (%{p:.1f})"))
                
                if int(elapsed) % 2 == 0:  # Her 2 saniyede log
                    self.add_log(f"ğŸ“Š KayÄ±t sÃ¼rÃ¼yor: {elapsed:.1f}s")
                
                time.sleep(0.1)
                
                # Timeout kontrolÃ¼
                if elapsed > duration + 2:
                    break
            
            # KayÄ±t verisini al
            sd.wait()  # KayÄ±t tamamlanana kadar bekle
            
            # SonuÃ§larÄ± kontrol et
            self.recorded_audio = audio_data[:, 0] if audio_data.ndim > 1 else audio_data
            self.sample_rate = SAMPLE_RATE
            
            actual_duration = len(self.recorded_audio) / SAMPLE_RATE
            self.add_log(f"âœ… Basit kayÄ±t tamamlandÄ±!")
            self.add_log(f"ğŸ“Š Hedef: {duration}s, GerÃ§ek: {actual_duration:.2f}s")
            self.add_log(f"ğŸ”¢ Sample sayÄ±sÄ±: {len(self.recorded_audio)}")
            
            # Ses analizi
            if len(self.recorded_audio) > 0:
                max_amplitude = np.max(np.abs(self.recorded_audio))
                avg_amplitude = np.mean(np.abs(self.recorded_audio))
                
                self.add_log(f"ğŸ”Š Max seviye: {max_amplitude:.4f}")
                self.add_log(f"ğŸ“ˆ Ortalama seviye: {avg_amplitude:.4f}")
                
                # Normalize et (gerekirse)
                if max_amplitude > 0.95:
                    self.recorded_audio = self.recorded_audio / max_amplitude * 0.9
                    self.add_log("ğŸ”§ Ses seviyesi normalize edildi")
                
                # UI gÃ¼ncelle
                self.root.after(1, lambda: self.update_comprehensive_plots(self.recorded_audio))
                self.root.after(1, lambda: self.status_label.config(text="âœ… Basit kayÄ±t tamamlandÄ±"))
                
                # Dosyaya kaydet
                self.save_recording()
                
                # DÃ¼ÄŸmeleri etkinleÅŸtir
                self.root.after(1, lambda: self.analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.quick_analyze_button.config(state=tk.NORMAL))
                self.root.after(1, lambda: self.report_button.config(state=tk.NORMAL))
                
                return True
            else:
                self.add_log("âŒ KayÄ±t verisi alÄ±namadÄ±!")
                return False
                
        except Exception as e:
            self.add_log(f"âŒ Basit kayÄ±t hatasÄ±: {e}")
            return False
        finally:
            self.is_recording = False
            self.root.after(1, lambda: self.record_button.config(text="ğŸ”´ KaydÄ± BaÅŸlat"))

def main():
    """Ana fonksiyon"""
    # macOS uyumluluÄŸu iÃ§in
    import sys
    
    # Root pencere oluÅŸtur
    root = tk.Tk()
    
    # macOS'ta Tkinter'Ä± Ã¶n plana getir
    if sys.platform == "darwin":  # macOS
        try:
            # macOS'ta Tkinter uygulamasÄ±nÄ± aktifleÅŸtir
            from subprocess import call
            call(['osascript', '-e', 'tell application "Python" to activate'])
        except:
            pass
    
    # UygulamayÄ± baÅŸlat
    app = SesKayitAnaliz(root)
    
    # Pencereyi merkeze getir ve gÃ¶rÃ¼nÃ¼r yap
    root.update()
    root.after(100, lambda: root.focus_force())
    
    try:
        root.mainloop()
    except KeyboardInterrupt:
        print("\nUygulama kapatÄ±lÄ±yor...")
        root.quit()

if __name__ == "__main__":
    main() 